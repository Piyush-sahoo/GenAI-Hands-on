{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3176778",
   "metadata": {},
   "source": [
    "IMPORTING ALL LIBRARIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b5d1b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aditya CS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed, GPT2Tokenizer\n",
    "import nltk , os , torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b6ea2",
   "metadata": {},
   "source": [
    "LOADING THE TEXT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e4230b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path =\"unit 1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f172e4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File successfully loaded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(file_path,'r', encoding=\"utf-8\") as f:\n",
    "        text=f.read()\n",
    "    print(\"File successfully loaded\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error:{file_path} not found\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fbde99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets see our data\n",
      "Generative AI and Its Applications: A Foundational Briefing\n",
      "\n",
      "Executive Summary\n",
      "\n",
      "This document provides a comprehensive overview of Generative AI, synthesizing foundational concepts, technological underpinnings, and practical applications as outlined in the course materials from PES University. Generative AI represents a transformative subset of Artificial Intelligence focused on creating novel con\n"
     ]
    }
   ],
   "source": [
    "print(\"Lets see our data\")\n",
    "print(text[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9afe11",
   "metadata": {},
   "source": [
    "EXPERIMENT 1 - TEXT GENERATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2f2efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(69)\n",
    "\n",
    "prompt=\"The future of Artificial Intelligence is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198eea9d",
   "metadata": {},
   "source": [
    "1) BERT MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93b08c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is................................................................................................................................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "bert_model = pipeline('text-generation',model=\"bert-base-uncased\")\n",
    "bert_output = bert_model(prompt, max_length=50, num_return_sequences=1)\n",
    "print(bert_output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd31822c",
   "metadata": {},
   "source": [
    "2. ROBERTA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bba03be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is\n"
     ]
    }
   ],
   "source": [
    "roberta_model = pipeline(\"text-generation\",model=\"roberta-base\")\n",
    "roberta_output = roberta_model(prompt,max_length=50,num_return_sequences=1)\n",
    "print(roberta_output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dbb110",
   "metadata": {},
   "source": [
    "3. BART MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "851af7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is govern 215 215 215 Ripple51 globally Bitcoinasuredudud Tankudud Severalud aeros Tank Tank dest summons Tank dest Diseintensiveintensiverunner Stupid 215 Stupid Stupid 215 Collect Tank aeros resemblance aeros aeros aeros Bitcoin aeros aeros Mann aeros aeros ASAP aeros Several Meet Americans Dise Stupid aeros aeros Meet aeros Dise ca Dise adam Meet resemblance aeros dest dest adam Stupid5 Stupid Vaughn aeros dest aeros Meet SeveralComplete ASAP aeros resemblance Tank Tank Stupid aeros ASAP Federal Sick aeros Shelby5 aeros aeros dest Shelby aeros Vaughn aerosfred aeros aeros Vaughn dest adam aeros aeroskward aeros aeros motivate aeros5 ASAP aeros ASAP5 Vaughniciary aeros aerosiciary dest aeros aerosprison aeros destprisonkward omn should adam55 aeros multif aeros aeros rodent aeros aeros adam aeros ASAP guid aeros dest Vaughn aerosiciary ooz aerosiciary Vaughn aeros aeros Federal aeros ASAPiciary aeros dest ASAP adam Vaughn ASAP should aeros aerosGoldMagikarp aerosiciary aeros ASAP Clearly legitim aeros VaughnacketInv aeros ASAP waking Sick ASAP scrib dest dest aeros Vaughn Vaughnsq aerosiaturesiaturessq compel aeros Vaughn ASAP Vaughn ASAPiciary Vaughn Vaughn Holidayprisoniatures aeros should aeros Clearlyprisonprison aeros loud Clearly Clearly contrastingiatures should aerosprisoniatures loudiatures loud epidem loud scrib aeros guid loud loudiaturesAge loud shouldoresc loud should governmental noble loud loud almonds loud forcibly multif Genesis Genesis loudiatures governmental noble noble loud\n"
     ]
    }
   ],
   "source": [
    "bart_model = pipeline(\"text-generation\",model=\"facebook/bart-base\")\n",
    "bart_output = bart_model(prompt,max_length=50,num_return_sequences=1)\n",
    "print(bart_output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc08780a",
   "metadata": {},
   "source": [
    "CONCLUSION FROM TEXT GENERATION \n",
    "\n",
    "Bert and Roberta are only encoder models so they can only read text and cannot generate text as we want , the the outputs for both them are none(no words generated)\n",
    "But Bart even though its a encoder and decoder model which is capable of generating text , the output it gave was gibberish and not proper because it is not a decoder oly model like gpt , so bart can be used only for sequence to sequence sentences like summarisation or transalation and not suitable for text generation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9337cde8",
   "metadata": {},
   "source": [
    "EXPERIMENT 2 - MASKED LANGUAGE MODELLING (MISSING WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd1f6e",
   "metadata": {},
   "source": [
    "1.BERT MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95d963f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create: 0.54\n",
      "generate: 0.16\n",
      "produce: 0.05\n",
      "develop: 0.04\n",
      "add: 0.02\n"
     ]
    }
   ],
   "source": [
    "mask_filler= pipeline(\"fill-mask\",model=\"bert-base-uncased\")\n",
    "masked_sentence = \"The goal of Generative AI is to [MASK] new content.\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb740d9",
   "metadata": {},
   "source": [
    "2.ROBERTA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc4e6b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " generate: 0.37\n",
      " create: 0.37\n",
      " discover: 0.08\n",
      " find: 0.02\n",
      " provide: 0.02\n"
     ]
    }
   ],
   "source": [
    "mask_filler= pipeline(\"fill-mask\",model=\"roberta-base\")\n",
    "masked_sentence = \"The goal of Generative AI is to <mask> new content.\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93480a0d",
   "metadata": {},
   "source": [
    "3.BART MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "909c44e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create:0.07\n",
      " help:0.07\n",
      " provide:0.06\n",
      " enable:0.04\n",
      " improve:0.03\n"
     ]
    }
   ],
   "source": [
    "mask_filler=pipeline(\"fill-mask\",model=\"facebook/bart-base\")\n",
    "masked_sentence = f\"The goal of Generative AI is to {mask_filler.tokenizer.mask_token} new content.\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"{p['token_str']}:{p['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08218cf6",
   "metadata": {},
   "source": [
    "CONCLUSION OF MASKED LANGUAGE MODELLING \n",
    "\n",
    "Bert and Roberta are extremely good at predicting masked words with high score/probability but in the case of Bart it predicted the masked word but with less confidence and less score , at this point i dont know in what Bart is good , didnt perfrom well in text generation nor predicting missed words ,lets see what it does in the next one \n",
    "Also i was copy pasting the code of bert for roberta and it was giving me error everytime , then i got to know its not [MASK] in roberta its <mask> so had to make that change and also its diffrent for each model so you can also use a general method which i used in Bart that is mask_filler.tokeniser.mask_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f537561",
   "metadata": {},
   "source": [
    "EXPERIMENT 3 QUESTION ANSWERING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc229e9f",
   "metadata": {},
   "source": [
    "1.BERT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd9e72fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What are the risks?\n",
      "A: risks such as hallucinations\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n",
    "question = \"What are the risks?\"\n",
    "context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "res = qa_pipeline(question=question,context=context)\n",
    "print(f\"\\nQ: {question}\")\n",
    "print(f\"A: {res['answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c82fef",
   "metadata": {},
   "source": [
    "2.ROBERATA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3dc58dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What are the risks?\n",
      "A: deepfakes.\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\",model=\"roberta-base\")\n",
    "question = \"What are the risks?\"\n",
    "context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "res = qa_pipeline(question=question,context=context)\n",
    "print(f\"\\nQ: {question}\")\n",
    "print(f\"A: {res['answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d194f9e",
   "metadata": {},
   "source": [
    "3.BART MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c7a8e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What are the risks?\n",
      "A: risks such as hallucinations, bias, and deepfakes\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\",model=\"facebook/bart-base\")\n",
    "question = \"What are the risks?\"\n",
    "context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "res = qa_pipeline(question=question,context=context)\n",
    "print(f\"\\nQ: {question}\")\n",
    "print(f\"A: {res['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c5c8c",
   "metadata": {},
   "source": [
    "CONCLUSION FROM QUESTION ANSWERING \n",
    "\n",
    "as we can see bert and roberta both gave answers to the question unlike experiment 1 text generation where they didnt give any answer but here also the answer the gave was not full , it was just a partial form only reason is its only encoder model \n",
    "if we talk about Bart this where it shines , because since its a decoder and encoder model and also we give it a proper context and input the answer it gave was complete and correct , finally i am happy for Bart it this something successfully in life "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fec7c8",
   "metadata": {},
   "source": [
    "\n",
    "### Deliverable: Observation Table\n",
    "\n",
    "### Deliverable: Observation Table\n",
    "\n",
    "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Generation** | BERT | Failure | Failed to generate coherent text and returned minimal or noisy output . | BERT is an encoder-only model and is not trained for autoregressive text generation. |\n",
    "|  | RoBERTa | Failure | Failed to generate coherent text and returned minimal or noisy output similar to BERT. | RoBERTa is also an encoder-only model optimized for understanding rather than generation. |\n",
    "|  | BART |  Somewhat Succuess(generated words but wrong) | Generated incoherent or unstable text when used with the text-generation pipeline. | BART requires task-specific, conditional generation and is not trained for free-form continuation. |\n",
    "| **Fill-Mask** | BERT | Success | Correctly predicted suitable words such as “generate” and “create” for the masked token. | BERT is explicitly trained using Masked Language Modeling (MLM). |\n",
    "|  | RoBERTa | Success | Produced highly relevant and context-aware predictions with strong confidence scores. | RoBERTa improves MLM performance through larger training data and dynamic masking. |\n",
    "|  | BART | Partial Success | Predicted reasonable masked tokens but with lower confidence than BERT and RoBERTa. | BART is trained as a denoising autoencoder rather than a token-level MLM model. |\n",
    "| **QA** | BERT | Success(not full answer) | Returned short and precise answers extracted directly from the context. | BERT is trained for extractive question answering by predicting answer spans. |\n",
    "|  | RoBERTa | Success(not full answer) | Provided concise and accurate span-based answers similar to BERT. | RoBERTa’s optimized encoder architecture supports effective extractive QA. |\n",
    "|  | BART | Success(full answer) | Generated complete, fluent, and human-readable answers. | BART’s encoder–decoder architecture enables conditional generative question answering. |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
