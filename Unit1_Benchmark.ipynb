{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Jx_dk9FlFDrn"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "models = [\n",
        "    \"bert-base-uncased\",\n",
        "    \"roberta-base\",\n",
        "    \"facebook/bart-base\"\n",
        "]\n",
        "\n",
        "for m in models:\n",
        "    print(f\"\\n=== {m} ===\")\n",
        "    try:\n",
        "        pipe = pipeline(\"text-generation\", model=m)\n",
        "        print(pipe(prompt, max_length=30))\n",
        "    except Exception as e:\n",
        "        print(\"ERROR:\", e)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXbyurniGIae",
        "outputId": "03ab57d0-94f6-4bab-f0d3-b25929f992b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== bert-base-uncased ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n",
            "\n",
            "=== roberta-base ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'The future of Artificial Intelligence is'}]\n",
            "\n",
            "=== facebook/bart-base ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'The future of Artificial Intelligence isuddenly Ital Ital Ital Madd Madd Madd Medicare Medicare Ital maritalaliasalias Madd Madd marital irregular Italastery Charlie�� Salmon voc irregular Madd Charlie Ital Ital populatedischer seating Madd sticky Stuff reproduce Stuff Stuff sticky Stuff exclaim Stuff Stuff Stuff proclaiming epic reproduceED Stuff Galileo Stuff Stuffopolis Stuff Stuff Lex Stuff facilitated Stuff Stuff backups Stuff Stuff epic Stuff Stuff Baby scramble Stuff Stuff statistic Baby Traps Stuff Stuff Armenian Stuff Stuff scramble Stuff Baby Stuff statistic proclaiming Stuff Stuff facilitated Baby Baby statistic statistic circumvent Stuff Stuff too Stuff StuffED High reproduce Stuff Traps Stuff Beat facilitated Padres Stuff Stuffhabi Stuff Stuff Immortal scramble Stuff kidding Baby Stuff Mac Stuffopolis Traps� Stuff Stuff scores statistic simulac Mac Stuff Stuff Mac proclaiming Stuff tooovic Stuff irregular Stuff too Baby Stuff Stuff journalists Stuff Inspection scramble Stuff Craw Stuff Lex Padres scramble Baby circumvent Stuff Baby Baby stuffed Stuff statistic statistic Immortal too too epic Stuff scramble scramble embryos Stuff Lexzinskieless Baby Stuff kidding RHP Stuff Baby kidding Stuff Padres Stuff Lex Baby Traps simulac Stuff kidding Jerome Stuff Immortalovicovicovic Stuff journalistsvertising Stuff Lex Madd Stuff RHP Padres Stuff Immortal Padresgencies simulac Traps Baby Stuffovicovic kidding journalists journalists Stuffovic Traps Stuff Baby Padres kidding kidding kidding Padreseless Traps Baby Padres Stuff Dw Stuff Beat Traps Padres Traps Babygencies Babyeless Traps Padres too Stuff kidding PadresovicInsp journalists kidding Traps Traps Traps Mechan'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_masks = {\n",
        "    \"bert-base-uncased\": \"[MASK]\",\n",
        "    \"roberta-base\": \"<mask>\",\n",
        "    \"facebook/bart-base\": \"<mask>\"\n",
        "}\n",
        "\n",
        "base = \"The goal of Generative AI is to {} new content.\"\n",
        "\n",
        "for model_name, mask_token in model_masks.items():\n",
        "    print(f\"\\n=== {model_name} ===\")\n",
        "\n",
        "    # build sentence for the model\n",
        "    sentence = base.format(mask_token)\n",
        "\n",
        "    try:\n",
        "        pipe = pipeline(\"fill-mask\", model=model_name)\n",
        "        print(\"Input:\", sentence)\n",
        "        result = pipe(sentence)\n",
        "        print(result)\n",
        "    except Exception as e:\n",
        "        print(\"ERROR:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC9RQTTaGO-2",
        "outputId": "df976cb1-4519-41a2-a135-714450fda8a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== bert-base-uncased ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: The goal of Generative AI is to [MASK] new content.\n",
            "[{'score': 0.5396888852119446, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to create new content.'}, {'score': 0.15575668215751648, 'token': 9699, 'token_str': 'generate', 'sequence': 'the goal of generative ai is to generate new content.'}, {'score': 0.054054468870162964, 'token': 3965, 'token_str': 'produce', 'sequence': 'the goal of generative ai is to produce new content.'}, {'score': 0.04451529309153557, 'token': 4503, 'token_str': 'develop', 'sequence': 'the goal of generative ai is to develop new content.'}, {'score': 0.01757732406258583, 'token': 5587, 'token_str': 'add', 'sequence': 'the goal of generative ai is to add new content.'}]\n",
            "\n",
            "=== roberta-base ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: The goal of Generative AI is to <mask> new content.\n",
            "[{'score': 0.3711293935775757, 'token': 5368, 'token_str': ' generate', 'sequence': 'The goal of Generative AI is to generate new content.'}, {'score': 0.36771273612976074, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.08351442217826843, 'token': 8286, 'token_str': ' discover', 'sequence': 'The goal of Generative AI is to discover new content.'}, {'score': 0.021335095167160034, 'token': 465, 'token_str': ' find', 'sequence': 'The goal of Generative AI is to find new content.'}, {'score': 0.016521504148840904, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}]\n",
            "\n",
            "=== facebook/bart-base ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: The goal of Generative AI is to <mask> new content.\n",
            "[{'score': 0.0746147632598877, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.06571780890226364, 'token': 244, 'token_str': ' help', 'sequence': 'The goal of Generative AI is to help new content.'}, {'score': 0.060879286378622055, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}, {'score': 0.03593532741069794, 'token': 3155, 'token_str': ' enable', 'sequence': 'The goal of Generative AI is to enable new content.'}, {'score': 0.03319435939192772, 'token': 1477, 'token_str': ' improve', 'sequence': 'The goal of Generative AI is to improve new content.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are the risks?\"\n",
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "\n",
        "models = [\n",
        "    \"bert-base-uncased\",\n",
        "    \"roberta-base\",\n",
        "    \"facebook/bart-base\"\n",
        "]\n",
        "\n",
        "for m in models:\n",
        "    print(f\"\\n=== {m} ===\")\n",
        "    try:\n",
        "        qa = pipeline(\"question-answering\", model=m)\n",
        "        print(qa(question=question, context=context))\n",
        "    except Exception as e:\n",
        "        print(\"ERROR:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AZQaEERGVAK",
        "outputId": "dd6ce765-a4d5-4d4a-91ef-24c029157703"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== bert-base-uncased ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.01443921448662877, 'start': 14, 'end': 60, 'answer': 'poses significant risks such as hallucinations'}\n",
            "\n",
            "=== roberta-base ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.008587918244302273, 'start': 0, 'end': 42, 'answer': 'Generative AI poses significant risks such'}\n",
            "\n",
            "=== facebook/bart-base ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.11816955544054508, 'start': 11, 'end': 81, 'answer': 'AI poses significant risks such as hallucinations, bias, and deepfakes'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **Generation** | BERT | *Failure* | *Generated symbols.* | *BERT is an Encoder; it isn't trained to predict the next word.* |\n",
        "| | RoBERTa | *Failure* | *Generated nothing* | *RoBERTa is an encoder* |\n",
        "| | BART | *Success* | *Generated text but was not meaningful* | *BART is an encoder-decoder* |\n",
        "| **Fill-Mask** | BERT | *Success* | *Predicted 'create'.* | *BERT is trained on Masked Language Modeling (MLM).* |\n",
        "| | RoBERTa | *Success* | *Predicted 'generate'.*| RoBERTa is trained on Masked Language Modelling (MLM).* |\n",
        "| | BART | * Partial Success* | *Predicted 'create'.* | Lower quality because MLM is only a pretrain objective |\n",
        "| **QA** | BERT | *Partial Success* | *selects a chunk that partially answers the question.* | *Encoder-only models can extract spans* |\n",
        "| | RoBERTa | *Partial Success* | *similar span selection, slightly less precise.* | *Optimized encoder gives good contextual understanding but lacks QA head* |\n",
        "| | BART | *Partial Success* | *the closest to a full correct span, even though BART isn’t designed for extractive QA.* | *Encoder–decoder can reconstruct answers but is not extractive QA oriented* |"
      ],
      "metadata": {
        "id": "u7zJ-0rPID8g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9mRH03UfIxY-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}