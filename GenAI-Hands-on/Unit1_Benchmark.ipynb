{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 1 Assignment – Model Benchmark Challenge\n",
        "\n",
        "Name: AMAR FAROQUI\n",
        "SRN: PES2UG24CS805\n"
      ],
      "metadata": {
        "id": "tizqS02PoMsF"
      },
      "id": "tizqS02PoMsF"
    },
    {
      "cell_type": "markdown",
      "id": "6d1dbdb3",
      "metadata": {
        "id": "6d1dbdb3"
      },
      "source": [
        "## Objective\n",
        "\n",
        "The objective of this assignment is to compare BERT, RoBERTa, and BART models across different NLP tasks and understand how model architecture affects performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f57f99f",
      "metadata": {
        "id": "0f57f99f"
      },
      "source": [
        "DISABLE VS CODE WIDGET ERRORS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "74cb1b6f",
      "metadata": {
        "id": "74cb1b6f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7b7ff67",
      "metadata": {
        "id": "a7b7ff67"
      },
      "source": [
        "INSTALL REQUIRED LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cce30431",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cce30431",
        "outputId": "c175da9b-37f8-4e20-bb13-9c79406d3155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3abbd400",
      "metadata": {
        "id": "3abbd400"
      },
      "source": [
        "IMPORT PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c6bcee3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6bcee3b",
        "outputId": "0e873cc1-5cdf-44ca-9ada-05f325be9cbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d10815f8",
      "metadata": {
        "id": "d10815f8"
      },
      "source": [
        "DEFINE MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "92b0e703",
      "metadata": {
        "id": "92b0e703"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f796b22",
      "metadata": {
        "id": "1f796b22"
      },
      "source": [
        "EXPERIMENT 1 — TEXT GENERATION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77d7e4f3",
      "metadata": {
        "id": "77d7e4f3"
      },
      "source": [
        "## Experiment 1: Text Generation\n",
        "\n",
        "Prompt: \"The future of Artificial Intelligence is\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "58172794",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58172794",
        "outputId": "850c6c7f-6bfb-41dd-efad-0fbb0b289971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERT Output:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n",
            "\n",
            "RoBERTa Output:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'The future of Artificial Intelligence is'}]\n",
            "\n",
            "BART Output:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'The future of Artificial Intelligence is wallet southwestern southwestern paninki MotionMainMainMaininkiinki070 peppinkiinki pepp Croinkiinki depended peppinki distributorsinkiilateralilateralinkiinkiinkiCPinkiinkiilateralinkiilateral distributorsinki DortmundilateralilateralMainMain clasp070 WalkinginkiンジClub depended depended depended contradicts spray depended southwestern distributors distributors Poison dependedilateral Poison Roughilateralcreationilateralcreation Rough Jungle depended dependedablilateralcreation depended Rough Rough dependedClub hallucinationsilateralilateralilateralcreationBroad dependedClubilateral Roughcreationcreationger Rough Rough Conference Rough deeds RoughilateralilateralCatholic disagrees depended depended PoisonClubSpawn Rough Dortmundilateral 340 Rough RoughcreationMainilateralcreationSpawn Rough Circle depended Dortmund Rough Rough Rough n dependedClubsettingsMain disagrees Jungle dependedcreation RoughBroad dependedilateralChuckMain Navigation Roughsolete Conference Rough dependedcreation depended DortmundilateralActually Rough Poison disagrees depended Rough depended Rough Dortmund JungleMainMain prayed Rough DortmundClub RoughMain Roughuman Dortmund disagreesMain disagrees Rough Rough befriend Jungle repaid Rough Roughishing Rough RoughMainger Rough disagreesActually disagrees Roughcreation numericalishingParameterscreationilateral Roughilateral profit Rough Rough FallsClub Rough Rough sequels disagrees disagreesClubilateral experiencingcreation Roughzen Rough Rough disagreesClubClub Rough Nebraska befriend disagrees Nebraska Rough profit disagrees prayed Dortmundcreation RoughSpawn Rough Jungle profit Rough Dortmund befriend profit Nebraska befriendClubClub befriend Falls Roughazard RoughClub Rough dependedMainMaincreationMain'}]\n"
          ]
        }
      ],
      "source": [
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name} Output:\")\n",
        "    try:\n",
        "        generator = pipeline(\"text-generation\", model=model)\n",
        "        print(generator(prompt, max_length=30))\n",
        "    except Exception as e:\n",
        "        print(\"Failed:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a07a8d3f",
      "metadata": {
        "id": "a07a8d3f"
      },
      "source": [
        "### Observation – Experiment 1\n",
        "\n",
        "BERT and RoBERTa were unable to generate meaningful text and mostly repeated or produced irrelevant output because they are encoder-only models not designed for text generation.\n",
        "BART was able to generate some continuation since it follows an encoder–decoder architecture.\n",
        "However, the generated text was not very coherent because the base BART model is not fine-tuned for open-ended text generation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f7b8757",
      "metadata": {
        "id": "1f7b8757"
      },
      "source": [
        "## Experiment 2: Masked Language Modeling (Fill Mask)\n",
        "\n",
        "Sentence: \"The goal of Generative AI is to [MASK] new content.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "29e40d1d",
      "metadata": {
        "id": "29e40d1d"
      },
      "outputs": [],
      "source": [
        "sentences = {\n",
        "    \"BERT\": \"The goal of Generative AI is to [MASK] new content.\",\n",
        "    \"RoBERTa\": \"The goal of Generative AI is to <mask> new content.\",\n",
        "    \"BART\": \"The goal of Generative AI is to <mask> new content.\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "37cbec2e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37cbec2e",
        "outputId": "837c864c-2310-4b2e-9451-467efefb7311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERT Output:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.5396932363510132, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to create new content.'}, {'score': 0.15575720369815826, 'token': 9699, 'token_str': 'generate', 'sequence': 'the goal of generative ai is to generate new content.'}]\n",
            "\n",
            "RoBERTa Output:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.3711312413215637, 'token': 5368, 'token_str': ' generate', 'sequence': 'The goal of Generative AI is to generate new content.'}, {'score': 0.3677145540714264, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}]\n",
            "\n",
            "BART Output:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.07461541891098022, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.06571870297193527, 'token': 244, 'token_str': ' help', 'sequence': 'The goal of Generative AI is to help new content.'}]\n"
          ]
        }
      ],
      "source": [
        "for name, model in models.items():\n",
        "    print(f\"\\n{name} Output:\")\n",
        "    try:\n",
        "        masker = pipeline(\"fill-mask\", model=model)\n",
        "        result = masker(sentences[name])\n",
        "        print(result[:2])   # show top 2 predictions\n",
        "    except Exception as e:\n",
        "        print(\"Failed:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82e7aacf",
      "metadata": {
        "id": "82e7aacf"
      },
      "source": [
        "## Observation Experiment 2\n",
        "BERT successfully predicted meaningful words like “create” and “generate” because it is trained using masked language modeling.\n",
        "RoBERTa also performed well after using the correct <mask> token, showing similar behavior to BERT.\n",
        "BART showed partial success, as masked language modeling is not its primary training objective.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0d57247",
      "metadata": {
        "id": "a0d57247"
      },
      "source": [
        "## Experiment 3: Question Answering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0d7f3f87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d7f3f87",
        "outputId": "135857fe-4161-4ba2-9832-232d0f8c2f55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERT Output:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.010261185467243195, 'start': 0, 'end': 45, 'answer': 'Generative AI poses significant risks such as'}\n",
            "\n",
            "RoBERTa Output:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.004142744466662407, 'start': 72, 'end': 81, 'answer': 'deepfakes'}\n",
            "\n",
            "BART Output:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.07378441467881203, 'start': 72, 'end': 82, 'answer': 'deepfakes.'}\n"
          ]
        }
      ],
      "source": [
        "question = \"What are the risks?\"\n",
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name} Output:\")\n",
        "    try:\n",
        "        qa = pipeline(\"question-answering\", model=model)\n",
        "        print(qa(question=question, context=context))\n",
        "    except Exception as e:\n",
        "        print(\"Failed:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c4c0558",
      "metadata": {
        "id": "5c4c0558"
      },
      "source": [
        "### Observation for Expermient 3\n",
        "\n",
        "The models produced inconsistent or partially correct answers when answering the given question.\n",
        "This is because the models used were base versions and not fine-tuned specifically for question answering tasks.\n",
        "The results highlight the importance of task-specific fine-tuning for accurate QA performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18517175",
      "metadata": {
        "id": "18517175"
      },
      "source": [
        "## Observation Table\n",
        "\n",
        "| Task | Model | Success/Failure | Observation | Architectural Reason |\n",
        "|------|------|----------------|------------|----------------------|\n",
        "| Generation | BERT | Failure | Could not generate text | Encoder-only |\n",
        "| Generation | RoBERTa | Failure | Similar to BERT | Encoder-only |\n",
        "| Generation | BART | Success | Generated coherent text | Encoder-Decoder |\n",
        "| Fill-Mask | BERT | Success | Correct predictions | MLM training |\n",
        "| Fill-Mask | RoBERTa | Success | Accurate predictions | Optimized MLM |\n",
        "| Fill-Mask | BART | Partial | Less accurate | Not MLM-based |\n",
        "| QA | BERT | Partial | Inconsistent answers | Not QA fine-tuned |\n",
        "| QA | RoBERTa | Partial | Similar behavior | Base model |\n",
        "| QA | BART | Partial | Random answers | Not QA fine-tuned |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d82d298",
      "metadata": {
        "id": "8d82d298"
      },
      "source": [
        "## Final Understanding\n",
        "\n",
        "Through this hands-on, I understood that the architecture of a transformer model plays a crucial role in deciding which NLP tasks it can perform effectively.\n",
        "Encoder-only models like BERT and RoBERTa are better suited for understanding tasks, while encoder–decoder models like BART support text generation.\n",
        "I also learned that tokenizer design and task-specific fine-tuning significantly impact model performance.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}