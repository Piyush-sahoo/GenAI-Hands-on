{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eec9861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\keeru\\appdata\\roaming\\python\\python312\\site-packages (4.47.0)\n",
      "Requirement already satisfied: torch in c:\\python312\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in c:\\python312\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\keeru\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\keeru\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\keeru\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\keeru\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\keeru\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\keeru\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\keeru\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\keeru\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: sympy in c:\\python312\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\keeru\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\keeru\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\keeru\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\keeru\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\keeru\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be15b838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5cd6cef51a47b89141ce34063806b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keeru\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\keeru\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\keeru\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6749c33e57ba4de6a1b3748ddb40bd4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caec7311576247ebb797769a7843c253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b75b5460ce40fc9a002a6cd6b6c59e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f70e4780d54d358290327ad6033b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477c46ae13df41bd8e86e6f6fe38b8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keeru\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\keeru\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77afabb6255c4127af98c33e85a33d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e906e77f656d442097535a15b5a01ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fc35ee32ed4dd1aec137eaaa3d21d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6685e87d818242fc8b52efe352aeea9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c212ef1ae14616bc98157545be6cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b115875653d4f23886776b04c137b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keeru\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\keeru\\.cache\\huggingface\\hub\\models--facebook--bart-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0be301a664844fc8682fd7dc3075087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4732cb7bde4879a5ceeeba001eeff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf387aaf2d148a9b6b8ac6c6a95979e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f66b41f38f241f7a2f2e98457c99791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "bert_gen = pipeline(\"text-generation\", model=\"bert-base-uncased\")\n",
    "roberta_gen = pipeline(\"text-generation\", model=\"roberta-base\")\n",
    "bart_gen = pipeline(\"text-generation\", model=\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8240dd43",
   "metadata": {},
   "source": [
    "Experiment 1: Text Generation\n",
    "Task: Try to generate text using the prompt: \"The future of Artificial Intelligence is\"\n",
    "\n",
    "- Code Hint: pipeline('text-generation', model='...')\n",
    "- Hypothesis: Which models will fail? Why? (Hint: Can an Encoder generate new tokens easily?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f874b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The future of Artificial Intelligence is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb26e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence is......................'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_gen(prompt, max_length=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fea688",
   "metadata": {},
   "source": [
    "- Generated only repeated punctuation (................) instead of meaningful text.\n",
    "- BERT is an encoder-only model trained for understanding, not autoregressive text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25f88e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence is'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_gen(prompt, max_length=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290ee040",
   "metadata": {},
   "source": [
    "- Similar to BERT\n",
    "\n",
    "- Returned only the input prompt without generating new tokens.\n",
    "- RoBERTa is an optimized encoder-only model and lacks a decoder for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24f9290d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence is stopping fantassinaku ability ability ability HIGH ability ability teamed teamedassin ability abilityulators stopping Buddhist Buddhist Buddhist fant'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_gen(prompt, max_length=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95d934f",
   "metadata": {},
   "source": [
    "- Generated new tokens beyond the prompt, but the text was partially incoherent and repetitive.\n",
    "\n",
    "- BART has an encoder‚Äìdecoder architecture, allowing text generation, but the base model is not fine-tuned for fluent open-ended generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed384cee",
   "metadata": {},
   "source": [
    "Experiment 2: Masked Language Modeling (Missing Word)\n",
    "Task: Predict the missing word in: \"The goal of Generative AI is to [MASK] new content.\"\n",
    "\n",
    "- Code Hint: pipeline('fill-mask', model='...')\n",
    "- Hypothesis: This is how BERT/RoBERTa were trained. They should perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c7b18fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "bert_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "roberta_mask = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
    "bart_mask = pipeline(\"fill-mask\", model=\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1227c600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.539692759513855,\n",
       "  'token': 3443,\n",
       "  'token_str': 'create',\n",
       "  'sequence': 'the goal of generative ai is to create new content.'},\n",
       " {'score': 0.15575766563415527,\n",
       "  'token': 9699,\n",
       "  'token_str': 'generate',\n",
       "  'sequence': 'the goal of generative ai is to generate new content.'},\n",
       " {'score': 0.05405496060848236,\n",
       "  'token': 3965,\n",
       "  'token_str': 'produce',\n",
       "  'sequence': 'the goal of generative ai is to produce new content.'},\n",
       " {'score': 0.044515229761600494,\n",
       "  'token': 4503,\n",
       "  'token_str': 'develop',\n",
       "  'sequence': 'the goal of generative ai is to develop new content.'},\n",
       " {'score': 0.017577484250068665,\n",
       "  'token': 5587,\n",
       "  'token_str': 'add',\n",
       "  'sequence': 'the goal of generative ai is to add new content.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_mask(\"The goal of Generative AI is to [MASK] new content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9e776",
   "metadata": {},
   "source": [
    "- Predicted highly relevant masked words like create and generate with high confidence.\n",
    "- BERT is trained using Masked Language Modeling (MLM), making it well-suited for fill-mask tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27996b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.37159380316734314,\n",
       "  'token': 1045,\n",
       "  'token_str': ' create',\n",
       "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
       " {'score': 0.37126225233078003,\n",
       "  'token': 5368,\n",
       "  'token_str': ' generate',\n",
       "  'sequence': 'The goal of Generative AI is to generate new content.'},\n",
       " {'score': 0.08263008296489716,\n",
       "  'token': 8286,\n",
       "  'token_str': ' discover',\n",
       "  'sequence': 'The goal of Generative AI is to discover new content.'},\n",
       " {'score': 0.01957070454955101,\n",
       "  'token': 465,\n",
       "  'token_str': ' find',\n",
       "  'sequence': 'The goal of Generative AI is to find new content.'},\n",
       " {'score': 0.01648854836821556,\n",
       "  'token': 694,\n",
       "  'token_str': ' provide',\n",
       "  'sequence': 'The goal of Generative AI is to provide new content.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_mask(\"The goal of Generative AI is to <mask> new content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d26f1",
   "metadata": {},
   "source": [
    "- Accurately predicted appropriate masked words with strong confidence scores.\n",
    "- RoBERTa is an optimized encoder-only model with improved MLM training on larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6557dfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.07461544126272202,\n",
       "  'token': 1045,\n",
       "  'token_str': ' create',\n",
       "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
       " {'score': 0.06571853160858154,\n",
       "  'token': 244,\n",
       "  'token_str': ' help',\n",
       "  'sequence': 'The goal of Generative AI is to help new content.'},\n",
       " {'score': 0.060880184173583984,\n",
       "  'token': 694,\n",
       "  'token_str': ' provide',\n",
       "  'sequence': 'The goal of Generative AI is to provide new content.'},\n",
       " {'score': 0.035935722291469574,\n",
       "  'token': 3155,\n",
       "  'token_str': ' enable',\n",
       "  'sequence': 'The goal of Generative AI is to enable new content.'},\n",
       " {'score': 0.03319481760263443,\n",
       "  'token': 1477,\n",
       "  'token_str': ' improve',\n",
       "  'sequence': 'The goal of Generative AI is to improve new content.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_mask(\"The goal of Generative AI is to <mask> new content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f2e322",
   "metadata": {},
   "source": [
    "- Generated reasonable masked word predictions but with lower confidence than BERT and RoBERTa.\n",
    "\n",
    "- BART is primarily designed for sequence-to-sequence tasks, not masked language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aaef71",
   "metadata": {},
   "source": [
    "Experiment 3: Question Answering\n",
    "Task: Answer the question \"What are the risks?\" based on the context: \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "\n",
    "- Code Hint: pipeline('question-answering', model='...')\n",
    "- Note: Using a \"base\" model (not fine-tuned for SQuAD) might yield random or poor results. Observe this behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47a82710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "bert_qa = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n",
    "roberta_qa = pipeline(\"question-answering\", model=\"roberta-base\")\n",
    "bart_qa = pipeline(\"question-answering\", model=\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3b8374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question = \"What are the risks?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ead778a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.004292937461286783,\n",
       " 'start': 66,\n",
       " 'end': 81,\n",
       " 'answer': ', and deepfakes'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_qa(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b844ef7",
   "metadata": {},
   "source": [
    "- Returned only a partial answer (‚Äúand deepfakes‚Äù) instead of the full list of risks.\n",
    "\n",
    "- Base BERT is not fine-tuned for question answering and struggles to extract complete spans from context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5716df15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.003996011335402727, 'start': 66, 'end': 71, 'answer': ', and'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_qa(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc5640d",
   "metadata": {},
   "source": [
    "- Produced an incomplete and vague answer fragment (‚Äúand‚Äù).\n",
    "\n",
    "- Although a strong encoder, RoBERTa requires QA-specific fine-tuning to accurately locate answer spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e2a8d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.04243619367480278,\n",
       " 'start': 0,\n",
       " 'end': 71,\n",
       " 'answer': 'Generative AI poses significant risks such as hallucinations, bias, and'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_qa(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e4420",
   "metadata": {},
   "source": [
    "- Returned a longer and more fluent answer but included extra context beyond the exact answer.\n",
    "\n",
    "- As an encoder‚Äìdecoder model, BART tends to generate or paraphrase text rather than precisely extract answer spans when not fine-tuned for QA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ff332",
   "metadata": {},
   "source": [
    "**This experiment demonstrates that model architecture strongly influences task performance, and models fail when forced outside their training objectives.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8784de01",
   "metadata": {},
   "source": [
    "| Task       | Model   | Classification  | Observation                                          | Architectural Reason                                                   |\n",
    "| ---------- | ------- | --------------- | ---------------------------------------------------- | ---------------------------------------------------------------------- |\n",
    "| Generation | BERT    | Failure         | Generated only repeated punctuation (................) instead of meaningful text. | BERT is an encoder-only model trained for understanding, not autoregressive text generation. |\n",
    "| Generation | RoBERTa | Failure         | Returned only the input prompt without generating new tokens. | RoBERTa is an optimized encoder-only model and lacks a decoder for text generation. |\n",
    "| Generation | BART    | Success         | Generated new tokens beyond the prompt, but the text was partially incoherent and repetitive.  | BART has an encoder‚Äìdecoder architecture, allowing text generation, but the base model is not fine-tuned for fluent open-ended generation. |\n",
    "| Fill-Mask  | BERT    | Success         | Predicted highly relevant masked words like create and generate with high confidence. | BERT is trained using Masked Language Modeling (MLM), making it well-suited for fill-mask tasks.  |\n",
    "| Fill-Mask  | RoBERTa | Success         | Accurately predicted appropriate masked words with strong confidence scores.  | RoBERTa is an optimized encoder-only model with improved MLM training on larger datasets.  |\n",
    "| Fill-Mask  | BART    | Partial Success | Generated reasonable masked word predictions but with lower confidence than BERT and RoBERTa.  | BART is primarily designed for sequence-to-sequence tasks, not masked language modeling.   |\n",
    "| QA         | BERT    | Partial Success | Returned only a partial answer (‚Äúand deepfakes‚Äù) instead of the full list of risks.    | Base BERT is not fine-tuned for question answering and struggles to extract complete spans from context.   |\n",
    "| QA         | RoBERTa | Partial Success | Produced an incomplete and vague answer fragment (‚Äúand‚Äù).   | Although a strong encoder, RoBERTa requires QA-specific fine-tuning to accurately locate answer spans.   |\n",
    "| QA         | BART    | Partial Success | Returned a longer and more fluent answer but included extra context beyond the exact answer.    | As an encoder‚Äìdecoder model, BART tends to generate or paraphrase text rather than precisely extract answer spans when not fine-tuned for QA.    |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
