{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37307c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.8.0+cpu\n",
      "Transformers imported successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Transformers imported successfully!\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19cf28f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b277c62cdc3e4f0bac63e71ab7217b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5b1d8ba5a94de5a3c97d392b18804d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8475dc93b59640839ec3a9e8c168c461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfdce57554dc4bbc931504d8673dab91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e88f2ad276a45d49346103421d7cd32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1137e398a1eb4f20aa499b43ea3c3e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11c0a6fcf70476ea13b88d4dd40a5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa740419d0e407cafc603575d84fe2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ddda3474744686a70626e921c8d315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5f629ed6634cc1a542caa62b06e279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffe4e4dba17476e9e68dd9a329858d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906a4466c2a344e698115c5fb057344c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6840762d04524ce8adc57a401f412188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b997484f72b4d1f993e7da295bbb4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generation pipelines loaded!\n"
     ]
    }
   ],
   "source": [
    "# Standard quality model\n",
    "gpt2_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2\"\n",
    ")\n",
    "\n",
    "# Fast lightweight model\n",
    "distil_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"distilgpt2\"\n",
    ")\n",
    "\n",
    "print(\"Text generation pipelines loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "682279a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI Storyteller Output:\n",
      "\n",
      "The knight entered the dark cave with a smile on his face.\n",
      "\n",
      "\"Well, that's what I was talking about, it's really not as bad as I thought…\"\n",
      "\n",
      "「I see. I'll go with you.」\n",
      "\n",
      "「…You'll be fine after taking this place.」\n",
      "\n",
      "I went back to the man with the red-haired witch, who had been making a speech.\n",
      "\n",
      "「So, I won't let you take any other place, don't worry, after all you're an adventurer, you will always be a knight.」\n",
      "\n",
      "「Oi, I didn't say it like that. And how about you……I'll take the place you gave me.」\n",
      "\n",
      "After a couple of tense words, the knight replied.\n",
      "\n",
      "「I won't let you take other places, this place is my residence.」\n",
      "\n",
      "And with that, the knight disappeared.\n",
      "\n",
      "The next moment the woman in red-haired witch entered the dungeon.\n",
      "\n",
      "「What the heck?」\n",
      "\n",
      "「If this woman was someone who could kill me, the knight would be too weak to protect you.」\n",
      "\n",
      "She was a knight of the Knights of the White Knight.\n",
      "\n",
      "The knight that had been knighted by the woman with red hair and white\n"
     ]
    }
   ],
   "source": [
    "story_prompt = \"The knight entered the dark cave\"\n",
    "\n",
    "story_output = gpt2_generator(\n",
    "    story_prompt,\n",
    "    max_length=120,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    top_k=50,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "print(\" AI Storyteller Output:\\n\")\n",
    "print(story_output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4273ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Email Draft:\n",
      "\n",
      "\n",
      "Write a polite and professional email using the following points:\n",
      "\n",
      "- Sick leave\n",
      "- Monday\n",
      "- Back Tuesday\n",
      "\n",
      "\n",
      "Email:\n",
      "\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send a friend to work\n",
      "- Email:\n",
      "- Send\n"
     ]
    }
   ],
   "source": [
    "email_points = \"\"\"\n",
    "- Sick leave\n",
    "- Monday\n",
    "- Back Tuesday\n",
    "\"\"\"\n",
    "\n",
    "email_prompt = f\"\"\"\n",
    "Write a polite and professional email using the following points:\n",
    "{email_points}\n",
    "\n",
    "Email:\n",
    "\"\"\"\n",
    "\n",
    "email_output = distil_generator(\n",
    "    email_prompt,\n",
    "    max_length=120,\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\" Email Draft:\\n\")\n",
    "print(email_output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "664bcd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem:\n",
      "\n",
      "\n",
      "Write a short poem with rhyming lines about Coding.\n",
      "\n",
      "Write the poem you want to write.\n",
      "\n",
      "Include a complete URL if possible, or include a link to a link on the page.\n",
      "\n",
      "You can also submit one short poem a week, which will be a lot easier if you write it in a short URL format. You can submit all your submissions for the same length of time.\n",
      "\n",
      "How to Send a Medium\n",
      "\n",
      "In general, send a single post a week for about five months. A very simple method would be to send a message once a week at 12am and send it to your Facebook friends as soon as you get to their Facebook page.\n",
      "\n",
      "For the next couple weeks, your message will be sent to Facebook for review. You can choose to use a social media manager like Instagram, Twitter or Facebook. This is usually a great time to check out your Facebook followers and share your story or other important posts with your friends on Instagram and Facebook.\n",
      "\n",
      "As of August 15th, all Instagram followers have to be added to the Instagram Account on their accounts. For a list of what this means, scroll to the bottom of your account. If you have a user account at Facebook then you will have the option of adding your follower to the account. To change your profile picture and\n"
     ]
    }
   ],
   "source": [
    "theme = \"Coding\"\n",
    "\n",
    "poem_prompt = f\"\"\"\n",
    "Write a short poem with rhyming lines about {theme}.\n",
    "\"\"\"\n",
    "\n",
    "poem_output = gpt2_generator(\n",
    "    poem_prompt,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(\"Generated Poem:\\n\")\n",
    "print(poem_output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c5fe7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generated Recipe:\n",
      "\n",
      "\n",
      "Create a simple cooking recipe using only these ingredients:\n",
      "Eggs, Tomato, Cheese\n",
      "\n",
      "Recipe:\n",
      "\n",
      "1. Mix all ingredients in a pot.\n",
      "\n",
      "2. Cook on low for 8 hours.\n",
      "\n",
      "3. Add 2 cups of cooked salmon (I used Salmon, but the fish is from the Philippines) to the pot.\n",
      "\n",
      "4. Add the fish and fish sauce, to taste.\n",
      "\n",
      "5. Add salt.\n",
      "\n",
      "6. Stir it up perfectly.\n",
      "\n",
      "7. Take the salmon and add it to the pot.\n",
      "\n",
      "8. When the salmon is still steaming, cook it in the sauce, which is very flavorful.\n",
      "\n",
      "8. Then add the fish and fish sauce, to taste.\n",
      "\n",
      "9. Stir it up perfectly.\n",
      "\n",
      "10. When the salmon is still steaming, remove the pan from the pot.\n",
      "\n",
      "11. Remove the fish and fish sauce from the pot.\n",
      "\n",
      "12. Stir it up perfectly.\n",
      "\n",
      "13. Stir it up perfectly.\n",
      "\n",
      "14. Add the rest of the ingredients to the pot.\n",
      "\n",
      "15. Bring the pot on high to boil for 30 seconds.\n",
      "\n",
      "16. Remove from heat and let cool.\n",
      "\n",
      "17. Remove from heat and let cool.\n",
      "\n",
      "18. Set aside.\n",
      "\n",
      "19. Once cooled, serve\n"
     ]
    }
   ],
   "source": [
    "ingredients = \"Eggs, Tomato, Cheese\"\n",
    "\n",
    "recipe_prompt = f\"\"\"\n",
    "Create a simple cooking recipe using only these ingredients:\n",
    "{ingredients}\n",
    "\n",
    "Recipe:\n",
    "\"\"\"\n",
    "\n",
    "recipe_output = gpt2_generator(\n",
    "    recipe_prompt,\n",
    "    max_length=120,\n",
    "    do_sample=True,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "print(\" Generated Recipe:\\n\")\n",
    "print(recipe_output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b026102a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YouTube Video Ideas:\n",
      "\n",
      "\n",
      "List of 5 catchy and viral YouTube video titles for the niche: Tech Review\n",
      "1.\n",
      "\n",
      "2.\n",
      "\n",
      "3.\n",
      "\n",
      "4.\n",
      "\n",
      "5.\n",
      "\n",
      "6.\n"
     ]
    }
   ],
   "source": [
    "niche = \"Tech Review\"\n",
    "\n",
    "idea_prompt = f\"\"\"\n",
    "List of 5 catchy and viral YouTube video titles for the niche: {niche}\n",
    "1.\n",
    "\"\"\"\n",
    "\n",
    "idea_output = gpt2_generator(\n",
    "    idea_prompt,\n",
    "    max_length=80,\n",
    "    do_sample=True,\n",
    "    temperature=0.85\n",
    ")\n",
    "\n",
    "print(\"YouTube Video Ideas:\\n\")\n",
    "print(idea_output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27416fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " distilgpt2 Output:\n",
      "\n",
      "Artificial Intelligence will transform the future by moving information from the Internet to the Web. The technology could transform this field and help other sectors in the field, such as agriculture, education, education, transportation, and health care.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " gpt2 Output:\n",
      "\n",
      "Artificial Intelligence will transform the future by making the machines smarter, better, and more efficient, and by making them more powerful.\n",
      "\n",
      "In the next few years, artificial intelligence will use the human brain's ability to better understand what we are doing, and to more accurately understand how to solve problems. By 2016, we will be able to better manage our lives, but we'll be able to better make decisions and manage our relationships. It will be possible to better understand how our lives work and make decisions based on our brains and intuition.\n",
      "\n",
      "The future of artificial intelligence will not only be more efficient but will also lead to great changes in society. There will be a huge shift to education, healthcare, and social services. We will be able to better understand the human brain and its interactions with our environment, and it will also lead to a wealth of new experiences and insights that will be better for humanity.\n",
      "\n",
      "We will be able to better understand and understand the human brain's interaction with the environment. We will be able to better understand and understand the human brain's interaction with the environment.\n",
      "\n",
      "We will be able to better understand and understand the human brain's interaction with the environment. We will be able to better understand and understand the human brain's interaction with the environment. And we will be\n"
     ]
    }
   ],
   "source": [
    "comparison_prompt = \"Artificial Intelligence will transform the future by\"\n",
    "\n",
    "fast_output = distil_generator(\n",
    "    comparison_prompt,\n",
    "    max_length=60,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "smart_output = gpt2_generator(\n",
    "    comparison_prompt,\n",
    "    max_length=60,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(\" distilgpt2 Output:\\n\")\n",
    "print(fast_output[0][\"generated_text\"])\n",
    "\n",
    "print(\"\\n gpt2 Output:\\n\")\n",
    "print(smart_output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f3650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
