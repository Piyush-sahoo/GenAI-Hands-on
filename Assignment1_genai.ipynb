{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7OHmDDULuF2B"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuBIkLjRuJb3",
        "outputId": "af93463b-044a-45c1-d3db-8b6f05e8c8fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "NNNYd7J3uPtW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text generation - exp 1\n",
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "for name, model_id in models.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    try:\n",
        "        generator = pipeline(\"text-generation\", model=model_id)\n",
        "        output = generator(prompt, max_new_tokens=25)\n",
        "        print(output[0][\"generated_text\"])\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQEySwVCuS85",
        "outputId": "28a3c771-1755-43c0-ce04-5414b90fff8b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: BERT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is.........................\n",
            "\n",
            "Model: RoBERTa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is\n",
            "\n",
            "Model: BART\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is Chou apex TC Reid Reid TC TC slaughtered slaughtered addon II Reid Reid Reid WINroup Reid Reid II taxes Reid Reidrusroup\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ATsW_QSZ4trt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#missing word - exp 2\n",
        "sentences = {\n",
        "    \"BERT\": \"The goal of Generative AI is to [MASK] new content.\",\n",
        "    \"RoBERTa\": \"The goal of Generative AI is to <mask> new content.\",\n",
        "    \"BART\": \"The goal of Generative AI is to <mask> new content.\"\n",
        "}\n",
        "\n",
        "for name, model_id in models.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    try:\n",
        "        fill = pipeline(\"fill-mask\", model=model_id)\n",
        "        results = fill(sentences[name])\n",
        "        for r in results[:3]:\n",
        "            print(r[\"token_str\"], \"-\", round(r[\"score\"], 3))\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8ocsrOvu6D1",
        "outputId": "6858b5df-5dfe-4272-b64c-d18f1f9437e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: BERT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "create - 0.54\n",
            "generate - 0.156\n",
            "produce - 0.054\n",
            "\n",
            "Model: RoBERTa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " generate - 0.371\n",
            " create - 0.368\n",
            " discover - 0.084\n",
            "\n",
            "Model: BART\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " create - 0.075\n",
            " help - 0.066\n",
            " provide - 0.061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#qna - exp 3\n",
        "question = \"What are the risks?\"\n",
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "\n",
        "for name, model_id in models.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    try:\n",
        "        qa = pipeline(\"question-answering\", model=model_id)\n",
        "        answer = qa(question=question, context=context)\n",
        "        print(answer)\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy196HSdvP5p",
        "outputId": "04fa1b14-9bb9-417c-9c04-c29c20dc1976"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: BERT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.015690370462834835, 'start': 66, 'end': 81, 'answer': ', and deepfakes'}\n",
            "\n",
            "Model: RoBERTa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.007598629454150796, 'start': 0, 'end': 67, 'answer': 'Generative AI poses significant risks such as hallucinations, bias,'}\n",
            "\n",
            "Model: BART\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.014989475719630718, 'start': 11, 'end': 31, 'answer': 'AI poses significant'}\n"
          ]
        }
      ]
    }
  ]
}