{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7667c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\aryan\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers torch sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6aa7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d9c8097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pipelines for BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pipelines for RoBERTa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aryan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aryan\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pipelines for BART...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aryan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aryan\\.cache\\huggingface\\hub\\models--facebook--bart-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"BERT\": \"bert-base-uncased\",\n",
    "    \"RoBERTa\": \"roberta-base\",\n",
    "    \"BART\": \"facebook/bart-base\"\n",
    "}\n",
    "\n",
    "text_gen_pipelines = {}\n",
    "fill_mask_pipelines = {}\n",
    "qa_pipelines = {}\n",
    "\n",
    "for name, model_id in models.items():\n",
    "    print(f\"Loading pipelines for {name}...\")\n",
    "    \n",
    "    # Text Generation\n",
    "    try:\n",
    "        text_gen_pipelines[name] = pipeline(\"text-generation\", model=model_id)\n",
    "    except Exception as e:\n",
    "        text_gen_pipelines[name] = str(e)\n",
    "    \n",
    "    # Fill Mask\n",
    "    try:\n",
    "        fill_mask_pipelines[name] = pipeline(\"fill-mask\", model=model_id)\n",
    "    except Exception as e:\n",
    "        fill_mask_pipelines[name] = str(e)\n",
    "    \n",
    "    # Question Answering\n",
    "    try:\n",
    "        qa_pipelines[name] = pipeline(\"question-answering\", model=model_id)\n",
    "    except Exception as e:\n",
    "        qa_pipelines[name] = str(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1e0a04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Text Generation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is................................................................................................................................................................................................................................................................\n",
      "\n",
      "RoBERTa Text Generation:\n",
      "The future of Artificial Intelligence is\n",
      "\n",
      "BART Text Generation:\n",
      "The future of Artificial Intelligence is negligible coronologists attest invest Cruiser wee crashes inadequ outweigh 199 Manuel expans Gar Garcles history Trudeau Mount poisoning bisexual PROC vessel Troll Mount facts Gar Cron Cronimony Irvine Internal Internal Kyle GarggiesLegLegLegmicrosoft Troll inadequXYXY Kyle facts Irvine Trudeau Irvine Irvine wee Irvine Kira Judicial Moor Moor Moor Honolulu Trudeau history Honolulu Irvine Kyle finds concessions Irvine raw 02 Trudeau Kyle history history HonoluluApps Om tropes Cruiser Honolulu Dept Trudeau empirical Trudeau Trudeau Troll history Honolulu Honolulu Moor malt Moor Honolulu Moor Trudeau90 Om Om Honolulu Om concessions Trollether Troll Troll Troll empirical Troll Consultinglies Honolulu background history Troll Moor Troll everlasting Troll Troll Honolulu Om CONTROL empirical tropeslies Troll Troll Wins Troll Troll Synt Troll Troll background Honolulu Moor Troll Troll Moor Moor Synt Troll Canon background concessions everlasting Troll Om Troll Troll71 Troll empirical background Troll Troll Settings Troll Troll facts Troll Trudeau Troll Troll completion Winslies Troll empirical history Troll Troll history Trollurgy Troll Om background Moor Troll Trudeau Cruiser Troll empirical Trudeau Trollether empirical Troll Troll concessions Troll Troll Consulting SUN Troll Troll Lenin Synt Trollterror Troll Troll SUN TrollRock Troll Trollpaste Troll Troll punishing Troll TrollliesRock Troll SUN Om background Trudeau Troll Honolulu SUN everlasting Trudeau facts Troll Troll topic Troll background Trollghan Om Cruiser Troll Troll Om Om Troll empiricalliesliesliesterror Troll Moor background Troll Mount Troll TrollANT Cruiser Troll Trudeau\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "\n",
    "results_exp1 = {}\n",
    "\n",
    "for name, pipe in text_gen_pipelines.items():\n",
    "    print(f\"\\n{name} Text Generation:\")\n",
    "    \n",
    "    if isinstance(pipe, str):\n",
    "        print(\"❌ Failed to load:\", pipe)\n",
    "        results_exp1[name] = \"FAILED\"\n",
    "    else:\n",
    "        try:\n",
    "            output = pipe(prompt, max_length=50, num_return_sequences=1)\n",
    "            text = output[0][\"generated_text\"]\n",
    "            print(text)\n",
    "            results_exp1[name] = text\n",
    "        except Exception as e:\n",
    "            print(\"❌ Error during generation:\", e)\n",
    "            results_exp1[name] = \"ERROR\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42fbc052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Fill-Mask:\n",
      "Top predictions: ['create', 'generate', 'produce']\n",
      "\n",
      "RoBERTa Fill-Mask:\n",
      "❌ Error during fill-mask: No mask_token (<mask>) found on the input\n",
      "\n",
      "BART Fill-Mask:\n",
      "❌ Error during fill-mask: No mask_token (<mask>) found on the input\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The goal of Generative AI is to [MASK] new content.\"\n",
    "\n",
    "results_exp2 = {}\n",
    "\n",
    "for name, pipe in fill_mask_pipelines.items():\n",
    "    print(f\"\\n{name} Fill-Mask:\")\n",
    "    \n",
    "    if isinstance(pipe, str):\n",
    "        print(\"❌ Failed to load:\", pipe)\n",
    "        results_exp2[name] = \"FAILED\"\n",
    "    else:\n",
    "        try:\n",
    "            output = pipe(sentence)\n",
    "            top_preds = [pred[\"token_str\"].strip() for pred in output[:3]]\n",
    "            print(\"Top predictions:\", top_preds)\n",
    "            results_exp2[name] = top_preds\n",
    "        except Exception as e:\n",
    "            print(\"❌ Error during fill-mask:\", e)\n",
    "            results_exp2[name] = \"ERROR\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9878da4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Question Answering:\n",
      "Answer: Generative\n",
      "Confidence Score: 0.0202\n",
      "\n",
      "RoBERTa Question Answering:\n",
      "Answer: Generative AI poses\n",
      "Confidence Score: 0.0077\n",
      "\n",
      "BART Question Answering:\n",
      "Answer: deepfakes\n",
      "Confidence Score: 0.0216\n"
     ]
    }
   ],
   "source": [
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question = \"What are the risks?\"\n",
    "\n",
    "results_exp3 = {}\n",
    "\n",
    "for name, pipe in qa_pipelines.items():\n",
    "    print(f\"\\n{name} Question Answering:\")\n",
    "    \n",
    "    if isinstance(pipe, str):\n",
    "        print(\"❌ Failed to load:\", pipe)\n",
    "        results_exp3[name] = \"FAILED\"\n",
    "    else:\n",
    "        try:\n",
    "            output = pipe(question=question, context=context)\n",
    "            answer = output[\"answer\"]\n",
    "            score = output[\"score\"]\n",
    "            print(\"Answer:\", answer)\n",
    "            print(\"Confidence Score:\", round(score, 4))\n",
    "            results_exp3[name] = (answer, score)\n",
    "        except Exception as e:\n",
    "            print(\"❌ Error during QA:\", e)\n",
    "            results_exp3[name] = \"ERROR\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bb16271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Text Generation Output</th>\n",
       "      <th>Fill-Mask Top Predictions</th>\n",
       "      <th>QA Answer + Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERT</td>\n",
       "      <td>The future of Artificial Intelligence is.........</td>\n",
       "      <td>[create, generate, produce]</td>\n",
       "      <td>(Generative, 0.020157973747700453)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RoBERTa</td>\n",
       "      <td>The future of Artificial Intelligence is</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>(Generative AI poses, 0.007668931037187576)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BART</td>\n",
       "      <td>The future of Artificial Intelligence is negli...</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>(deepfakes, 0.021586419083178043)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model                             Text Generation Output  \\\n",
       "0     BERT  The future of Artificial Intelligence is.........   \n",
       "1  RoBERTa           The future of Artificial Intelligence is   \n",
       "2     BART  The future of Artificial Intelligence is negli...   \n",
       "\n",
       "     Fill-Mask Top Predictions                            QA Answer + Score  \n",
       "0  [create, generate, produce]           (Generative, 0.020157973747700453)  \n",
       "1                        ERROR  (Generative AI poses, 0.007668931037187576)  \n",
       "2                        ERROR            (deepfakes, 0.021586419083178043)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for model in models.keys():\n",
    "    data.append({\n",
    "        \"Model\": model,\n",
    "        \"Text Generation Output\": results_exp1.get(model, \"N/A\"),\n",
    "        \"Fill-Mask Top Predictions\": results_exp2.get(model, \"N/A\"),\n",
    "        \"QA Answer + Score\": results_exp3.get(model, \"N/A\")\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
