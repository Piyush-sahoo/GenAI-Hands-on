{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9043ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/absarkar/Developer/GenAI-Hands-on/unit 1/h1/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85cdfbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"BERT\": \"bert-base-uncased\",\n",
    "    \"RoBERTa\": \"roberta-base\",\n",
    "    \"BART\": \"facebook/bart-base\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4199bb95",
   "metadata": {},
   "source": [
    "# Experiment 1: Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6db3ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use mps:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n",
      "\n",
      " RoBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use mps:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is'}]\n",
      "\n",
      " BART\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"The future of Artificial Intelligence isartist rub Interstellar Fr mailConnell Interstellar sparkling Doctorsomach CE InterstellarJusticeJusticeither mail mail undertaking ColtsJusticeJustice anguish verticallyburst TicketSTEMoard Interstellar mailither Interstellar Interstellar Interstellar mail datingall CE veterallallMotorall Lieberman StillMotoritherburstall CEall Interstellarpodpod 244allall dominanceolithic女allapeakeJusticeJustice Interstellarall vertically dominance Life vertically ran Life violationsallallall pigeallallirlingallalltrackingallall harming headsets Ticket Interstellarolithic Interstellarpodallall ah Lieberman GH Ticket Life Life Life ranallall GH Gram LifeMotorallParametersallall ill Life Life Interstellarallall 747 LiebermanSTEM Jerome CarsonallSTEM.''ed 747anche Liebermanburstall violationsall Ticket Gram Lieberman Liebermanazine.''all Interstellar pige Life Liebermanazineall 747STEM Carson Lieberman Lieberman Lieberman Factorallall Gram Lifeolithicancoanco Gram passengers Lieberman Lieberman Interstellar Lieberman Liebermanタanco Life Interstellar Lieberman.''ctor Lieberman.'' Interstellarittall Lieberman Lifeolithic Lifehigh Interstellar Life Life Bodallallerennthirstanco dur Sok JeromeSTEM Liebermananco dur dur InterstellarSTEM Lifeitt Life harming Jerome Lifectorctor Lieberman Jeromeall Lieberman LiebermanSTEM 1906all Life Life Lieberman Life Life 1906 Interstellar Liebermanolithic Lieberman Liebermanerenn 1906 JeromeSTEMhigh.'' Lieberman Jerome 1906all 1906 Jerome Life Life Communismalloften Lieberman Factor Lifeallallensity 747 pige\"}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "\n",
    "for name, model_id in models.items():\n",
    "    print(f\"\\n {name}\")\n",
    "    try:\n",
    "        generator = pipeline(\"text-generation\", model=model_id)\n",
    "        output = generator(prompt, max_length=30, truncation=True)\n",
    "        print(output)\n",
    "    except Exception as e:\n",
    "        print(\"Failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0baf34",
   "metadata": {},
   "source": [
    "## Observations\n",
    "**BERT**\n",
    "\n",
    "We get repeating dots after the prompt text as output from BERT. This has happened primarily because it is an encoder-only model and is not autoregressive so it can't predict the next token step by step. When forced into generation, it falls back to generating high-probability junk tokens like \".\".\n",
    "\n",
    "**RoBERTa**\n",
    "\n",
    "The model stopped immediately after printing the prompt out. This model is also encoder-only. When forced into generation, it either outputs nothing or terminates immediately due to token probability collapse.\n",
    "\n",
    "**BART**\n",
    "\n",
    "This model kept generating tokens which have no semantic meaning at all. BART is an encoder-decoder model so it can generate tokens but bart-base has no causal language modeling like RoBERTa and some decoder weights are randomly initialized due to which we get fluent token flow but no coherent semantic structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad2d66e",
   "metadata": {},
   "source": [
    "# Experiment 2: Masked Language Modeling (Missing Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa83bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create -> 0.54\n",
      "generate -> 0.156\n",
      "produce -> 0.054\n",
      "\n",
      " RoBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " generate -> 0.371\n",
      " create -> 0.368\n",
      " discover -> 0.084\n",
      "\n",
      " BART\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create -> 0.075\n",
      " help -> 0.066\n",
      " provide -> 0.061\n"
     ]
    }
   ],
   "source": [
    "sentences = {\n",
    "    \"BERT\": \"The goal of Generative AI is to [MASK] new content.\",\n",
    "    \"RoBERTa\": \"The goal of Generative AI is to <mask> new content.\",\n",
    "    \"BART\": \"The goal of Generative AI is to <mask> new content.\"\n",
    "}\n",
    "\n",
    "for name, model_id in models.items():\n",
    "    print(f\"\\n {name}\")\n",
    "    try:\n",
    "        fill_mask = pipeline(\"fill-mask\", model=model_id)\n",
    "        results = fill_mask(sentences[name])\n",
    "        for r in results[:3]:\n",
    "            print(r[\"token_str\"], \"->\", round(r[\"score\"], 3))\n",
    "    except Exception as e:\n",
    "        print(\"Failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc46fcb5",
   "metadata": {},
   "source": [
    "## Observations\n",
    "**BERT**\n",
    "\n",
    "It predicts highly relevant verbs like create, generate, etc. It also predicts with high confidence (~54%). The predictions it made are also grammatically correct and syntactically precise. This happened because BERT is trained using Masked Language Modeling itself, and this task matches its pretraining objective of predicting a missing token using bidirectional context.\n",
    "\n",
    "**RoBERTa**\n",
    "\n",
    "It's top predictions are almost evenly split and it is slightly less overconfident than BERT. It is also MLM-trained but is trained on more data than BERT with dynamic masking, which leads to better generalization and less probability collapse on one token.\n",
    "\n",
    "**BART**\n",
    "\n",
    "The predictions made by it are ok but they have very low confidence and has no clear dominant answer as well. BART is not primarily MLM trained but is trained on denoising autoencoding and on larger text-spans and not single-token masks. So for this task it works syntactically well but does not align well with its training objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e1eaef",
   "metadata": {},
   "source": [
    "# Experiment 3: Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92ac8e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.004337908700108528, 'start': 32, 'end': 82, 'answer': 'risks such as hallucinations, bias, and deepfakes.'}\n",
      "\n",
      " RoBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.007780194049701095, 'start': 0, 'end': 67, 'answer': 'Generative AI poses significant risks such as hallucinations, bias,'}\n",
      "\n",
      " BART\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.061262412928044796, 'start': 38, 'end': 81, 'answer': 'such as hallucinations, bias, and deepfakes'}\n"
     ]
    }
   ],
   "source": [
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question = \"What are the risks?\"\n",
    "\n",
    "for name, model_id in models.items():\n",
    "    print(f\"\\n {name}\")\n",
    "    try:\n",
    "        qa = pipeline(\"question-answering\", model=model_id)\n",
    "        answer = qa(question=question, context=context)\n",
    "        print(answer)\n",
    "    except Exception as e:\n",
    "        print(\"Failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990350a",
   "metadata": {},
   "source": [
    "## Observations\n",
    "**BERT**\n",
    "\n",
    "The answer is fully correct but it has very low confidence score. qa_outputs head is randomly initialized for this and BERT is not fine-tuned on QA data. The encoder representations are strong enough to align question and context and guess the correct span but without QA training the start/end classifiers are poorly calibrated.\n",
    "\n",
    "**RoBERTa**\n",
    "\n",
    "The answer is incomplete and incorrect as well. It also has very low confidence score. Here also the QA head is randomly initialized because the model has no QA-specific alignment training. So it falls back to the high-attention tokens near the start.\n",
    "\n",
    "**BART**\n",
    "\n",
    "The answer is correct and well-scoped and confidence score is higher than BERT and RoBERTa. The encoder-decoder attention in BART helps align question to relevant context region, so we get a decent output even without QA fine-tuning. However, the QA head is still untrained itself and the score is still not fully reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3674ffd7",
   "metadata": {},
   "source": [
    "# Observations Table\n",
    "\n",
    "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Generation** | BERT | *Failure* | *Generated a long sequence of dots only.* | *BERT is an encoder-only model and isn't trained for next-token generation.* |\n",
    "| | RoBERTa | *Failure* | *It just returned the prompt itself.* | *RoBERTa is also encoder-only and lacks a causal language modeling decoder.* |\n",
    "| | BART | *Failure* | *Generated long but incoherent and meaningless text.* | *BART can generate test, but bart-base is not trained for causal language modeling and the decoder weights are partially untrained.* |\n",
    "| **Fill-Mask** | BERT | *Success* | *Predicted 'create', 'generate', etc. with high confidence.* | *BERT is trained on Masked Language Modeling (MLM), which matches the task.* |\n",
    "| | RoBERTa | *Success* | *Produced accurate predictions like 'create' and 'generate' with balanced scores.* | *It is also MLM trained and has dynamic masking + larger pretraining data.* |\n",
    "| | BART | *Failure* | *Returned ok predictions with very low confidence and no dominant answer.* | *BART is trained as a denoising autoencoder and not well trained for single-token MLM prediction.* |\n",
    "| **QA** | BERT | *Partial Failure* | *Extracted the correct answer span but with very low confidence.* | *Encoder representations are strong, but the QA head is randomly initialized as there is no QA fine-tuning.* |\n",
    "| | RoBERTa | *Failure* | *Returned a span from the start of the context, which was incomplete.* | *Mostly due to no QA fine-tuning* |\n",
    "| | BART | *Partial Success* | *Extracted correct answer span with slightly higher confidence compared to other models.* | *Encoder-decoder attention helps alignment of question to context, but QA head is still untrained itself.* |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
