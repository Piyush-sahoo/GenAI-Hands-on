{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34fa0682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "789d1512",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"BERT\": \"bert-base-uncased\",\n",
    "    \"RoBERTa\": \"roberta-base\",\n",
    "    \"BART\": \"facebook/bart-base\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14222217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BERT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8fdf47c1e5f4ddba0bd9e3d38700fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c738080614d44000b57edd78e00c5fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6bc961bbf0c4a62ac031aef4b828284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df2b662129d4013b13eb0fc7793ed04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54218509be34da5b4e1e5dd2d62f252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n",
      "\n",
      " RoBERTa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01e4bf7ac4949b283555e50c791d3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c47bcdc7fd4c13a01bb61b539532a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5feea75016a043149bd9b37ef383c7af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55b04a7d2c04947a7a6acf199af7653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06bc7060d494b1cb8cf5e01af25f8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1756b96b3f4096875375fa64200be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is'}]\n",
      "\n",
      " BART\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d29a8e14cca4652964ae3ac9c155bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaee66e592094ccc846a8770b62317b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8e27f991104fb7ab651a070b7cdeed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34f157270b3470dbd28ed14325aab08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698310b5a23b4ae2bef2b4973d37b643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence isookyXX Kiringin Planetary Planetaryooluddy antagonist oppressutinggingingin inspection popup inspectionMikeookyooky Metroid celebratedutingginmyra Info molooky Milan Milanmyra crackooky learners ● aimed Milan aimed Ox Malone learners learnersgin0000YNookyookyooky learners securelyooky chosen chosengin asserts Malone MaloneookyestroREDRepublicankilling Fighter kale laud laud learners Respectooky chosen learnersRED honestly crack chosen chosen chosensurface Fighter honestly Anon chosen Malone Maloneheader chosen honestly Ok chosenll chosen reproductiveansky chosenolia Tags chosen chosenestro crack chosenestroRED FighterRepublicanheaderestroestroheaderansky endorsingll wideansky settlesll chosen Fighter chosenRepublicanestro Fighter chosenheaderestro honestlyheader Tags chosenheader reproductiveheader Fighterookyheaderheader Fighterheaderheader � Fighteroliaestro endorsing Maloneanskyheaderestroheaderheader SpearsheaderheaderRepublicanRepublicanestroheaderestro TagsRepublicanheaderheaderheader crack chosenheaderLairRepublicanRepublicanRepublican wide FighteranskyheaderRED formerlyheaderheader chosen MaloneheaderheaderLairestroRepublican doublingheader Fighterozheaderheaderastrousheaderheader regularlyestroheaderRepublicanheader Tagsheader chosenheaderheader PROCheaderheader wideheaderheader endorsingheader �RepublicanRepublican score MaloneheaderRepublican crackRepublicanheaderRepublican Fighterheaderllheader SpearsoliaheaderRepublicanLairheaderRepublican settlesRepublican wide playoffsheaderheader MaloneRepublicanheader crack Fighterheader crack crack Quantityheaderheader AnonRepublicanRepublican'}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "\n",
    "for name, model_id in models.items():\n",
    "    print(f\"\\n {name}\")\n",
    "    try:\n",
    "        generator = pipeline(\"text-generation\", model=model_id)\n",
    "        output = generator(prompt, max_length=30, truncation=True)\n",
    "        print(output)\n",
    "    except Exception as e:\n",
    "        print(\"Failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd790a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create -> 0.54\n",
      "generate -> 0.156\n",
      "produce -> 0.054\n",
      "\n",
      " RoBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " generate -> 0.371\n",
      " create -> 0.368\n",
      " discover -> 0.084\n",
      "\n",
      " BART\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create -> 0.075\n",
      " help -> 0.066\n",
      " provide -> 0.061\n"
     ]
    }
   ],
   "source": [
    "sentences = {\n",
    "    \"BERT\": \"The goal of Generative AI is to [MASK] new content.\",\n",
    "    \"RoBERTa\": \"The goal of Generative AI is to <mask> new content.\",\n",
    "    \"BART\": \"The goal of Generative AI is to <mask> new content.\"\n",
    "}\n",
    "\n",
    "for name, model_id in models.items():\n",
    "    print(f\"\\n {name}\")\n",
    "    try:\n",
    "        fill_mask = pipeline(\"fill-mask\", model=model_id)\n",
    "        results = fill_mask(sentences[name])\n",
    "        for r in results[:3]:\n",
    "            print(r[\"token_str\"], \"->\", round(r[\"score\"], 3))\n",
    "    except Exception as e:\n",
    "        print(\"Failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b16b7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.010130776558071375, 'start': 32, 'end': 81, 'answer': 'risks such as hallucinations, bias, and deepfakes'}\n",
      "\n",
      " RoBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.007283675251528621, 'start': 72, 'end': 82, 'answer': 'deepfakes.'}\n",
      "\n",
      " BART\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.03296162933111191, 'start': 14, 'end': 37, 'answer': 'poses significant risks'}\n"
     ]
    }
   ],
   "source": [
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question = \"What are the risks?\"\n",
    "\n",
    "for name, model_id in models.items():\n",
    "    print(f\"\\n {name}\")\n",
    "    try:\n",
    "        qa = pipeline(\"question-answering\", model=model_id)\n",
    "        answer = qa(question=question, context=context)\n",
    "        print(answer)\n",
    "    except Exception as e:\n",
    "        print(\"Failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986c2f20",
   "metadata": {},
   "source": [
    "## Observation Table\n",
    "\n",
    "Based on the experimental results, here's the completed observation table:\n",
    "\n",
    "| **Task** | **Model** | **Classification (Success/Failure)** | **Observation (What actually happened?)** | **Why did this happen? (Architectural Reason)** |\n",
    "|----------|-----------|--------------------------------------|-------------------------------------------|------------------------------------------------|\n",
    "| **Generation** | BERT | Failure | Generated nonsense - repeated dots (periods) | BERT is an Encoder-only model; it isn't trained to predict the next word. It's trained for MLM (Masked Language Modeling), not autoregressive generation. |\n",
    "| | RoBERTa | Failure | Returned the same prompt without generating new text | RoBERTa is also an Encoder-only model like BERT, designed for understanding tasks, not generation. |\n",
    "| | BART | Failure | Generated completely random/gibberish tokens (Fighter, Republican, Malone, Tags, etc.) | BART wasn't fine-tuned for causal generation. The base model requires task-specific fine-tuning; without it, decoder produces nonsensical outputs. |\n",
    "| **Fill-Mask** | BERT | Success | Predicted: 'create' (0.54), 'generate' (0.156), 'produce' (0.054) | BERT is trained on Masked Language Modeling (MLM). This is its core training objective, so it excels at this task. |\n",
    "| | RoBERTa | Success | Predicted: 'generate' (0.371), 'create' (0.368), 'discover' (0.084) | RoBERTa is an optimized BERT variant, also trained on MLM. It performs well on fill-mask tasks. |\n",
    "| | BART | Success | Predicted: 'create' (0.075), 'help' (0.066), 'provide' (0.061) | BART uses denoising autoencoding during pre-training, which includes masked token prediction. However, lower confidence scores than BERT/RoBERTa. |\n",
    "| **QA** | BERT | Partial Success | Answer: \"risks such as hallucinations, bias, and deepfakes\" (score: 0.010) | BERT can extract spans but wasn't fine-tuned on SQuAD. Very low confidence score indicates poor performance without task-specific training. |\n",
    "| | RoBERTa | Partial Success | Answer: \"deepfakes.\" (score: 0.007) | Similar to BERT - can extract text spans but gives incomplete answer with very low confidence. Not fine-tuned for QA. |\n",
    "| | BART | Partial Success | Answer: \"poses significant risks\" (score: 0.033) | BART has encoder-decoder architecture suitable for QA, but base model isn't fine-tuned. Extracts partial relevant text with low confidence. |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Encoder-only models (BERT, RoBERTa)**: Excel at understanding tasks (MLM, classification) but fail at generation tasks\n",
    "2. **Encoder-decoder models (BART)**: Designed for generation but require fine-tuning for specific tasks\n",
    "3. **Fine-tuning matters**: All base models show poor performance on tasks they weren't specifically trained for\n",
    "4. **Task alignment**: Best performance occurs when model architecture matches the task (e.g., BERT on fill-mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cf5778",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
