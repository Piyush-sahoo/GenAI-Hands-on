{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UL3bB9kuAPzH"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk"
      ],
      "metadata": {
        "id": "lLndj3aNAan5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the models and prompt\n",
        "models_to_test = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}"
      ],
      "metadata": {
        "id": "pzcz-RsnAiTZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task -1 : Text Generation"
      ],
      "metadata": {
        "id": "luVB9sHlBf8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "prompt = \"The future of Artificial Intelligence is\"\n"
      ],
      "metadata": {
        "id": "DbgooQ-cBfGY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model_path in models_to_test.items():\n",
        "    print(f\"--- Running Experiment with {name} ---\")\n",
        "    try:\n",
        "        # Task: Text Generation\n",
        "        generator = pipeline('text-generation', model=model_path)\n",
        "\n",
        "        # Setting max_new_tokens for a fair comparison\n",
        "        output = generator(prompt, max_new_tokens=100, num_return_sequences=1)\n",
        "        results[name] = output[0]['generated_text']\n",
        "        print(f\"Output: {results[name]}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        results[name] = f\"Error: {str(e)}\"\n",
        "        print(f\"Result: {name} failed as expected or produced an error.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pY3uNXBByMW",
        "outputId": "4da71033-26ca-435e-bfe8-8fe54c546fff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Experiment with BERT ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: The future of Artificial Intelligence is....................................................................................................\n",
            "\n",
            "--- Running Experiment with RoBERTa ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: The future of Artificial Intelligence is\n",
            "\n",
            "--- Running Experiment with BART ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: The future of Artificial Intelligence isersonerson Brend different different differentraine synchronized perceive Chat synchronized salvation hazeshall salvation synchronized Cache vaccination different vaccination thresholdsFactor paradox Tennis vacuum thresholds thresholds Theseushing thresholds thresholds thresholds Laws thresholds thresholdsode thresholds thresholds Hospital thresholds paradox thresholds revoked Laws thresholdsshall Cache Ah unsureorthy Ah Ah Laws Laws thresholdsDun Laws vaccination vaccinationochemistry Elkwat Ah Ah Ahï¿½ thresholds Ah Laws videot Laws Lawsshall thresholdsochemistryochemistry thresholds thresholds Due Ah Ahshall thresholds mosquitoesshallshallersonerson sliced Ah Nothing thresholds Ah temper Ahochemistryerson heroic thresholds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame.from_dict(results, orient='index', columns=['Generated Text'])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzf9-sSDBykz",
        "outputId": "a831b36d-ff29-4ec0-82f4-c034bb5077dc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Generated Text\n",
            "BERT     The future of Artificial Intelligence is.........\n",
            "RoBERTa           The future of Artificial Intelligence is\n",
            "BART     The future of Artificial Intelligence isersone...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**  \n",
        "As BERT and RoBERTA are encoder only models, they excell at understanding and reading the prompt. However the part of text generation in a transformer is the role of decoder. As such, we can see that they didn't generate any meaningful sentence.   \n",
        "  \n",
        "On the other hand, BART is an encoder-decoder model and is capable of text generation. However we can observe from above cells that although it did generate some text, it wasn't as coherent or meaningful as our normal GPT models."
      ],
      "metadata": {
        "id": "FlFXm_VeCg5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task - 2 : Masked language modelling"
      ],
      "metadata": {
        "id": "DCc81Hk_DzHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_prompt_bert = \"The goal of Generative AI is to [MASK] new content.\"\n",
        "mask_prompt_others = \"The goal of Generative AI is to <mask> new content.\""
      ],
      "metadata": {
        "id": "rCREvNhmCNVZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_mask = {}\n",
        "for name, model_path in models_to_test.items():\n",
        "    print(f\"--- Running Fill-Mask with {name} ---\")\n",
        "    try:\n",
        "        # Initialize fill-mask pipeline\n",
        "        mask_filler = pipeline('fill-mask', model=model_path)\n",
        "\n",
        "        # Select the correct prompt format\n",
        "        current_prompt = mask_prompt_bert if name == \"BERT\" else mask_prompt_others\n",
        "\n",
        "        # Get top prediction\n",
        "        output = mask_filler(current_prompt)\n",
        "\n",
        "        # Store the top 1 result\n",
        "        top_prediction = output[0]['token_str']\n",
        "        score = round(output[0]['score'], 4)\n",
        "\n",
        "        results_mask[name] = f\"{top_prediction} (Confidence: {score})\"\n",
        "        print(f\"Top Prediction: {top_prediction}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        results_mask[name] = f\"Error: {str(e)}\"\n",
        "        print(f\"Result: {name} failed.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-f9khlUoECvo",
        "outputId": "35e74735-f3b3-43cb-df8a-e10fc785a66b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Fill-Mask with BERT ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Prediction: create\n",
            "\n",
            "--- Running Fill-Mask with RoBERTa ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Prediction:  generate\n",
            "\n",
            "--- Running Fill-Mask with BART ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Prediction:  create\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_df = pd.DataFrame.from_dict(results_mask, orient='index', columns=['Predicted Word'])\n",
        "print(mask_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D1NjdDgEFHF",
        "outputId": "5a2f6304-a31a-428c-b3a4-759cde374bf1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         Predicted Word\n",
            "BERT        create (Confidence: 0.5397)\n",
            "RoBERTa   generate (Confidence: 0.3711)\n",
            "BART        create (Confidence: 0.0746)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**  \n",
        "BERT and RoBERTA were specifically trained for such masked language modelling tasks. Since they use bidirectional encoding, they are able to predict the correct word with good confidence.\n",
        "  \n",
        "BART is also able to predict the missing word, although it can do alot more such as text generation. It wasn't specifically trained for this task and as such we can see that its confidence is lower."
      ],
      "metadata": {
        "id": "NczY8efGEkVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task - 3: Question Answering"
      ],
      "metadata": {
        "id": "OwCfsDeKGonr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question = \"What are the risks?\""
      ],
      "metadata": {
        "id": "kjz3KxE5EPQT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_qa = {}\n",
        "\n",
        "for name, model_path in models_to_test.items():\n",
        "    print(f\"--- Running Question Answering with {name} ---\")\n",
        "    try:\n",
        "        # Task: Question Answering\n",
        "        qa_pipeline = pipeline('question-answering', model=model_path)\n",
        "\n",
        "        # Get answer\n",
        "        output = qa_pipeline(question=question, context=context)\n",
        "\n",
        "        answer = output['answer']\n",
        "        score = round(output['score'], 4)\n",
        "        results_qa[name] = f\"'{answer}' (Score: {score})\"\n",
        "        print(f\"Answer: {answer}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Base models often lack the 'QA head' weights needed for this pipeline\n",
        "        results_qa[name] = \"Failed/No QA Head\"\n",
        "        print(f\"Result: {name} does not have a fine-tuned QA head.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBsn3zk7GrCw",
        "outputId": "e87c7fe8-2590-48c4-b3e1-90920a726ffa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Question Answering with BERT ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: , and deepfakes\n",
            "\n",
            "--- Running Question Answering with RoBERTa ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Generative AI poses significant risks such as hallucinations, bias, and deepfakes\n",
            "\n",
            "--- Running Question Answering with BART ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: as hallucinations, bias, and\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_df = pd.DataFrame.from_dict(results_qa, orient='index', columns=['Extracted Answer'])\n",
        "print(qa_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA44l4PTGthS",
        "outputId": "4cd15b3d-dd64-4aa6-e13e-a88bb2acab51"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          Extracted Answer\n",
            "BERT                     ', and deepfakes' (Score: 0.0156)\n",
            "RoBERTa  'Generative AI poses significant risks such as...\n",
            "BART         'as hallucinations, bias, and' (Score: 0.028)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**  \n",
        "RoBERTA gave the best result, while BERT and BART were relatively fine with their answers"
      ],
      "metadata": {
        "id": "40sYvvHwHFRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Task           | Model   | Classification (Success/Failure) | Observation (What actually happened?)              | Why did this happen? (Architectural Reason)                                                                                |\n",
        "| :------------- | :------ | :------------------------------- | :------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Generation** | BERT    | *Failure*                        | Generated ... until max tokens                     | *BERT is an Encoder; it isn't trained to predict the next word.*                                                           |\n",
        "|                | RoBERTa | Failure                          | Did not generate any content                       | RoBERTa was trained on top of BERT and has a similar encoder only architecture                                             |\n",
        "|                | BART    | Partial Success                  | Generated texts, however were not fully meaningful | As BART is encoder and decoder, it is capable of text generation. However it is not as good as GPT models.                 |\n",
        "| **Fill-Mask**  | BERT    | *Success*                        | *Predicted 'create', 'generate'.*                  | *BERT is trained on Masked Language Modeling (MLM).*                                                                       |\n",
        "|                | RoBERTa | Success                          | Predicted generate                                 | Like BERT, RoBERTA is also trained on Masked Language Modelling, but with a bigger dataset                                 |\n",
        "|                | BART    | Success                          | Predicted create                                   | BART can do a lot more than masked language modelling. As such it did predict accurately, although with a lower confidence |\n",
        "| **QA**         | BERT    | Partial Success                  | Answer was not grammatically correct               | They don't have task specific heads and lack fine tuned question answering capabilities                                    |\n",
        "|                | RoBERTa | Success                          | Answered relatively well                           | They don't have task specific heads and lack fine tuned question answering capabilities                                    |\n",
        "|                | BART    | Partial Success                  | Slightly better than BERT                          | As BART also has a decoder, it can technically generate text. Although lack of task specific head, make it hard.           |\n",
        "\n"
      ],
      "metadata": {
        "id": "XB-mYG4aHbTD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O61XSJvaGyht"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}