{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4bT9P1HtIX4",
        "outputId": "eb718203-feb6-47d6-9ee7-bf5e25aeeb3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n"
          ]
        }
      ],
      "source": [
        "# Install transformers if not already installed\n",
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from transformers import pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLTUSdNQtT_F",
        "outputId": "ab9b6e77-3c18-40fa-a20a-7f18869babe3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 1: Text Generation**"
      ],
      "metadata": {
        "id": "VMRHLNfhtk1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup for Text Generation"
      ],
      "metadata": {
        "id": "c6tWnJWStrVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXPERIMENT 1: TEXT GENERATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nPrompt: '{prompt}'\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX_VMcfntX_x",
        "outputId": "0516cdec-090d-4e28-a849-e94712358339"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "EXPERIMENT 1: TEXT GENERATION\n",
            "================================================================================\n",
            "\n",
            "Prompt: 'The future of Artificial Intelligence is'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test BERT for Text Generation"
      ],
      "metadata": {
        "id": "iEDMHUjztyhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing BERT (bert-base-uncased)...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "try:\n",
        "    generator_bert = pipeline('text-generation', model='bert-base-uncased')\n",
        "    result_bert = generator_bert(prompt, max_length=20)\n",
        "    print(f\"Result: {result_bert}\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: {type(e).__name__}\")\n",
        "    print(f\"Message: {str(e)[:200]}\")\n",
        "\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjDQ-OEGtvTn",
        "outputId": "c97e347f-422d-40fd-b309-94c911ca146b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing BERT (bert-base-uncased)...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: [{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test RoBERTa for Text Generation"
      ],
      "metadata": {
        "id": "nFv3vfYCuF-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing RoBERTa (roberta-base)...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "try:\n",
        "    generator_roberta = pipeline('text-generation', model='roberta-base')\n",
        "    result_roberta = generator_roberta(prompt, max_length=20)\n",
        "    print(f\"Result: {result_roberta}\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: {type(e).__name__}\")\n",
        "    print(f\"Message: {str(e)[:200]}\")\n",
        "\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUWViVxWt0uO",
        "outputId": "a5b2f414-11a8-4fb5-aaa2-f6c8b1a58e13"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing RoBERTa (roberta-base)...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: [{'generated_text': 'The future of Artificial Intelligence is'}]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test BART for Text Generation"
      ],
      "metadata": {
        "id": "EYngraDMuMx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing BART (facebook/bart-base)...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "try:\n",
        "    generator_bart = pipeline('text-generation', model='facebook/bart-base')\n",
        "    result_bart = generator_bart(prompt, max_length=20)\n",
        "    print(f\"Result: {result_bart}\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: {type(e).__name__}\")\n",
        "    print(f\"Message: {str(e)[:200]}\")\n",
        "\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AebNK_ItuHgB",
        "outputId": "0c7bc374-a67a-4058-b458-e2ea462f2901"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing BART (facebook/bart-base)...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: [{'generated_text': 'The future of Artificial Intelligence isURA patrons patrons Controller Ole Ole OleDON Ole Nost SY Ole subsection Ole Guests Guests Nostrahim Nost patrons patronsIPS flush patrons patrons Ole Ole Pe Nost Nost patrons Nost Might patronslette patronsorative flush flush flush Guests futures Nost Ole Ole PCR nurture Pe flush flush sack ornabove flush disorderagements deepen Become Nost Nost nurture kind patrons propagate propagate NostRat Nost Sevenwas ravumo demonstrate Guests \"\\'Elizabethposeセ Leopard Nost Nost demonstrate candles Seven appre demonstrate demonstrate Nostintersinters drawbacknell demonstrate Seven nurture nurture demonstrate nurture demonstratelyn nurture propagate rav Mand grosswas nurture corrections rav rav mammals rav Nost nurtureusher Nost Nost candles candles demonstrate Pe Nost gross candleswas demonstrate propagate rav rav rav wandering Nost NostCharge drawback Nost Nost Array drawback demonstrate refresh hotsldom propagate Seven exhibit propagate rav propagate gross ravwas \"\\' Nost nurture behavesshell Pe Pe assassination Nost behaves candles demonstratelynlyn Hebrew Echo nurture nurture Investigative behaveswas tr Echo Nost Gators nurture demonstratechan adapting Meadow hotsldom candles nurture nurture nurture Hebrewlyn hots Mand Gators gross Ryder drawback drawback exhibit tr vigorously drawback specialized interoper Nostocial drawback whale hots hots adapting adapting candles candles� vigorously Nost tr tr drawback Nostlyn exhibit drawback hots hots drawback530 Nost pixels drawback drawback drawback hots Nost drawbackldom nose Array buzz hots vigorously hots gross grosswas Nost drawback grossudd vigorously Nost hots drawback vigorously nose'}]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 2: Fill-Mask**"
      ],
      "metadata": {
        "id": "QROX59AxuUYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup for Fill-Mask"
      ],
      "metadata": {
        "id": "l0PkrR9zuZQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: Each model uses different mask tokens\n",
        "text_bert = \"The goal of Generative AI is to [MASK] new content.\"\n",
        "text_roberta = \"The goal of Generative AI is to <mask> new content.\"\n",
        "text_bart = \"The goal of Generative AI is to <mask> new content.\"\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXPERIMENT 2: MASKED LANGUAGE MODELING (FILL-MASK)\")\n",
        "print(\"=\"*80)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRZriuaquXn6",
        "outputId": "83387131-6a98-4580-b6b6-849c7abf6baa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "EXPERIMENT 2: MASKED LANGUAGE MODELING (FILL-MASK)\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test BERT for Fill-Mask"
      ],
      "metadata": {
        "id": "TouG2QoXudWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing BERT (bert-base-uncased)...\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Input: '{text_bert}'\\n\")\n",
        "\n",
        "try:\n",
        "    fill_mask_bert = pipeline('fill-mask', model='bert-base-uncased')\n",
        "    result_bert = fill_mask_bert(text_bert)\n",
        "    print(\"Top 3 predictions:\")\n",
        "    for i, pred in enumerate(result_bert[:3], 1):\n",
        "        print(f\"{i}. '{pred['token_str']}' (score: {pred['score']:.4f})\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: {str(e)[:200]}\")\n",
        "\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFP23-uHubf1",
        "outputId": "108f8c8e-702d-4916-fd3b-81fc869a35bc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing BERT (bert-base-uncased)...\n",
            "--------------------------------------------------\n",
            "Input: 'The goal of Generative AI is to [MASK] new content.'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 predictions:\n",
            "1. 'create' (score: 0.5397)\n",
            "2. 'generate' (score: 0.1558)\n",
            "3. 'produce' (score: 0.0541)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test RoBERTa for Fill-Mask"
      ],
      "metadata": {
        "id": "pTBOKtJSvGiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing RoBERTa (roberta-base)...\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Input: '{text_roberta}'\\n\")\n",
        "\n",
        "try:\n",
        "    fill_mask_roberta = pipeline('fill-mask', model='roberta-base')\n",
        "    result_roberta = fill_mask_roberta(text_roberta)\n",
        "    print(\"Top 3 predictions:\")\n",
        "    for i, pred in enumerate(result_roberta[:3], 1):\n",
        "        print(f\"{i}. '{pred['token_str']}' (score: {pred['score']:.4f})\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: {str(e)[:200]}\")\n",
        "\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzNzh_5ivfFr",
        "outputId": "8d0f19c8-28f2-4b52-fc06-465d7064a725"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing RoBERTa (roberta-base)...\n",
            "--------------------------------------------------\n",
            "Input: 'The goal of Generative AI is to <mask> new content.'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 predictions:\n",
            "1. ' generate' (score: 0.3711)\n",
            "2. ' create' (score: 0.3677)\n",
            "3. ' discover' (score: 0.0835)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test BART for Fill-Mask"
      ],
      "metadata": {
        "id": "D-fHQdsQvIGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing BART (facebook/bart-base)...\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Input: '{text_bart}'\\n\")\n",
        "\n",
        "try:\n",
        "    fill_mask_bart = pipeline('fill-mask', model='facebook/bart-base')\n",
        "    result_bart = fill_mask_bart(text_bart)\n",
        "    print(\"Top 3 predictions:\")\n",
        "    for i, pred in enumerate(result_bart[:3], 1):\n",
        "        print(f\"{i}. '{pred['token_str']}' (score: {pred['score']:.4f})\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: {str(e)[:200]}\")\n",
        "\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEKdxJ20vgG4",
        "outputId": "9decb5b7-5b23-49ab-e8f4-332eda5fe2bc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing BART (facebook/bart-base)...\n",
            "--------------------------------------------------\n",
            "Input: 'The goal of Generative AI is to <mask> new content.'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 predictions:\n",
            "1. ' create' (score: 0.0746)\n",
            "2. ' help' (score: 0.0657)\n",
            "3. ' provide' (score: 0.0609)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 3: Question Answering**"
      ],
      "metadata": {
        "id": "8VxW1aS4vK44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Setup for QA"
      ],
      "metadata": {
        "id": "Rx_T8sy1vNdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question = \"What are the risks?\"\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXPERIMENT 3: QUESTION ANSWERING\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nContext: '{context}'\")\n",
        "print(f\"Question: '{question}'\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF1XP8QNviSk",
        "outputId": "591c6cfc-4ad3-4136-db6f-1f9c7b7ccfa3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "EXPERIMENT 3: QUESTION ANSWERING\n",
            "================================================================================\n",
            "\n",
            "Context: 'Generative AI poses significant risks such as hallucinations, bias, and deepfakes.'\n",
            "Question: 'What are the risks?'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test BERT for QA"
      ],
      "metadata": {
        "id": "bfG1WG8kvPFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing BERT (bert-base-uncased)...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "try:\n",
        "    qa_bert = pipeline('question-answering', model='bert-base-uncased')\n",
        "    result_bert = qa_bert(question=question, context=context)\n",
        "    print(f\"Answer: '{result_bert['answer']}'\")\n",
        "    print(f\"Confidence Score: {result_bert['score']:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: {str(e)[:200]}\")\n",
        "\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5I2KFMLvkNz",
        "outputId": "6e870bb5-7e73-4e74-fd40-326175700f04"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing BERT (bert-base-uncased)...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: ', and deepfakes'\n",
            "Confidence Score: 0.0073\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test RoBERTa for QA"
      ],
      "metadata": {
        "id": "J3g7penbvRGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing RoBERTa (roberta-base)...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "try:\n",
        "    qa_roberta = pipeline('question-answering', model='roberta-base')\n",
        "    result_roberta = qa_roberta(question=question, context=context)\n",
        "    print(f\"Answer: '{result_roberta['answer']}'\")\n",
        "    print(f\"Confidence Score: {result_roberta['score']:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: {str(e)[:200]}\")\n",
        "\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9PFLOdpvm9i",
        "outputId": "781ce37d-648a-4cdb-e82e-4341b9a2391b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing RoBERTa (roberta-base)...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 'Generative AI poses significant risks such as hallucinations, bias,'\n",
            "Confidence Score: 0.0089\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test BART for QA"
      ],
      "metadata": {
        "id": "wVzvnXVmvSyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing BART (facebook/bart-base)...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "try:\n",
        "    qa_bart = pipeline('question-answering', model='facebook/bart-base')\n",
        "    result_bart = qa_bart(question=question, context=context)\n",
        "    print(f\"Answer: '{result_bart['answer']}'\")\n",
        "    print(f\"Confidence Score: {result_bart['score']:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: {str(e)[:200]}\")\n",
        "\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYNCU62qvoky",
        "outputId": "ce47e454-c787-4d06-e099-b2aa638b4909"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing BART (facebook/bart-base)...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 'deepfakes.'\n",
            "Confidence Score: 0.0526\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deliverable: Observation Table\n",
        "\n",
        "Based on the experiments conducted, here are the observations:\n",
        "\n",
        "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "|------|-------|----------------------------------|---------------------------------------|----------------------------------------------|\n",
        "| **Generation** | BERT | Failure | Generated only periods/dots: '........' (nonsensical repetitive symbols) | BERT is an Encoder-only model; it lacks a language modeling head and wasn't trained to predict the next word autoregressively |\n",
        "| | RoBERTa | Failure | Simply repeated the input prompt with no new text generated | RoBERTa is also Encoder-only; like BERT, it's designed for understanding context, not generating new sequences |\n",
        "| | BART | Failure | Generated random, incoherent words: 'patrons', 'Ole', 'flush', 'demonstrate', 'nurture', etc. (complete gibberish) | BART has Encoder-Decoder architecture suitable for generation, BUT the base model wasn't trained as a causal language model - it needs task-specific fine-tuning |\n",
        "| **Fill-Mask** | BERT | Success | Predicted 'create' (0.5397), 'generate' (0.1558), 'produce' (0.0541) - all semantically correct | BERT is trained on Masked Language Modeling (MLM) as its core pre-training objective; this is exactly what it was designed for |\n",
        "| | RoBERTa | Success | Predicted 'generate' (0.3711), 'create' (0.3677), 'discover' (0.0835) - semantically accurate predictions | RoBERTa uses optimized MLM training with more data, dynamic masking, and longer training; performs excellently on its native task |\n",
        "| | BART | Success | Predicted 'create' (0.0746), 'help' (0.0657), 'provide' (0.0609) - reasonable but lower confidence scores | BART uses a denoising autoencoder approach with text infilling capabilities, but MLM isn't its primary training objective, hence lower confidence |\n",
        "| **QA** | BERT | Failure | Answered ', and deepfakes' with very low confidence (0.0073) - extracted wrong span | Base BERT has suitable architecture for span extraction but isn't fine-tuned on SQuAD or QA datasets; picks arbitrary spans |\n",
        "| | RoBERTa | Failure | Answered with nearly the entire context sentence (0.0089 confidence) - failed to identify the specific answer span | Similar to BERT - encoder architecture is suitable but lacks QA fine-tuning; extremely low confidence indicates poor performance |\n",
        "| | BART | Failure | Answered 'deepfakes.' with low confidence (0.0526) - partially correct but incomplete | BART's Encoder-Decoder architecture is designed for generation tasks, not extractive QA; it's not fine-tuned for span extraction tasks |"
      ],
      "metadata": {
        "id": "q3PCeG_12jfG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3dSao3v5ue-M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}