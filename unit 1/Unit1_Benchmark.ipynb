{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3e559fc",
   "metadata": {},
   "source": [
    "# Unit 1 – Model Benchmark Challenge\n",
    "\n",
    "**Objective:**  \n",
    "To compare BERT, RoBERTa, and BART by forcing them to perform tasks they are not architecturally designed for, and observing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e14d1c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb898df8",
   "metadata": {},
   "source": [
    "## Experiment 1: Text Generation\n",
    "\n",
    "**Prompt:**  \n",
    "\"The future of Artificial Intelligence is\"\n",
    "\n",
    "**Hypothesis:**  \n",
    "Encoder-only models like BERT and RoBERTa will fail because they are not designed to generate the next token.  \n",
    "BART, being an encoder-decoder model, should perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4006948a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is................................................................................................................................................................................................................................................................\n",
      "\n",
      "RoBERTa Output:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8761f2eb461344db80c1277348770c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\advai\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\advai\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e407a8e2fad428ab72f77bc78162831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112d9877ad2046778914b418cefdb28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9341f21613a46c89c44ddfacbe18b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad63b41ebe94c62abf28bdcbcdbef3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee8a2c1a2644fec83478350133f23e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is\n",
      "\n",
      "BART Output:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7142a757624329a4dfe530f6045509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\advai\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\advai\\.cache\\huggingface\\hub\\models--facebook--bart-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d0406d42034906b51ceb8e6ca3933e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425fd1530a064544a4509fa9755b8511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8069caa5f71d4dcfbbb77dc6ad788053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6853a94941fa4f92951d00480d59a146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence isonial WaltonUFF Damcorn stolenCheckRGB grad lava lava Walton Walton WaltonUFF stolen Waltonきankedcorn lavaedience grad Alert181 Walton lavaAd Guam reapp lava elsewhere Waltonlisedienceedience animosity lavaedience incor Againstjavaedience reapp reappedience piedienceedienceetts reappedienceediencejava 331edience fearing animosity inputsjavaediencejavaedience Againstedience hungry animosity Nero Neroedience animosity Against Against Box NeroedienceBottom std Short Againstjava NeroRGB inputs inputs Nero reappediencejava econom Againstき Nero tonguesedience inputs animosityjava tongues Against animosity serpent 331 econom Nero Vanderbiltjavajavareek Nero Against Against econom� Against tongues tongues Nero Nero Nero scoreboard Nero grad Nero]=]=edience Island econom 331 inputs serpent Nero Feeling Against Nero Against Nero tongues serpentsol Nero Feeling tongues Nero]= Nero econom Nero Against]= Nero Nero Against tonguesgil Nero Nero serpent std manipulated]= Nero demanded Nero reapp Nero tongues Nero Dresden serpent Although serpent Nero Nero econom serpent elsewheregil Nero]= Cha Nero Nero reconsider Nero serpent serpent serpent cult Nero Nerolang hots serpent Nero reconsider serpent Against serpent Nero elsewhere Nero incred Nero Nerogiljava serpent 331 Nero Feeling Nero Analytics contract Nero Nero declares Nero Nero 1962 Nero contract tongues Nero serpent incons Againstursion serpent Vanderbilt Neroombs Nero contract Analytics serpentingu serpent 331 Against serpent tongues serpent Nero serpent tonguesLoop serpent tutorials Nero stop Nerolang Nero\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "\n",
    "models = {\n",
    "    \"BERT\": \"bert-base-uncased\",\n",
    "    \"RoBERTa\": \"roberta-base\",\n",
    "    \"BART\": \"facebook/bart-base\"\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} Output:\")\n",
    "    try:\n",
    "        generator = pipeline(\"text-generation\", model=model)\n",
    "        output = generator(prompt, max_length=30)\n",
    "        print(output[0][\"generated_text\"])\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac509cf",
   "metadata": {},
   "source": [
    "## Experiment 2: Masked Language Modeling\n",
    "\n",
    "**Input Sentence:**  \n",
    "\"The goal of Generative AI is to [MASK] new content.\"\n",
    "\n",
    "**Hypothesis:**  \n",
    "BERT and RoBERTa should succeed because they are trained using Masked Language Modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd27633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create\n",
      "generate\n",
      "produce\n",
      "\n",
      "RoBERTa Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: No mask_token (<mask>) found on the input\n",
      "\n",
      "BART Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: No mask_token (<mask>) found on the input\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The goal of Generative AI is to [MASK] new content.\"\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} Output:\")\n",
    "    try:\n",
    "        fill = pipeline(\"fill-mask\", model=model)\n",
    "        results = fill(sentence)\n",
    "        for r in results[:3]:\n",
    "            print(r[\"token_str\"])\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee12390f",
   "metadata": {},
   "source": [
    "## Experiment 3: Question Answering\n",
    "\n",
    "**Context:**  \n",
    "\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "\n",
    "**Question:**  \n",
    "\"What are the risks?\"\n",
    "\n",
    "**Observation Note:**  \n",
    "Base models not fine-tuned on QA may give weak or random answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e842330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallucinations, bias, and deepfakes\n",
      "\n",
      "RoBERTa Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "risks such as hallucinations, bias, and deepfakes\n",
      "\n",
      "BART Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI poses\n"
     ]
    }
   ],
   "source": [
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question = \"What are the risks?\"\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} Output:\")\n",
    "    try:\n",
    "        qa = pipeline(\"question-answering\", model=model)\n",
    "        result = qa(question=question, context=context)\n",
    "        print(result[\"answer\"])\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276463d9",
   "metadata": {},
   "source": [
    "## Observation Table\n",
    "\n",
    "| Task | Model | Classification (Success/Failure) | Observation | Architectural Reason |\n",
    "|----|------|----------------------------------|-------------|----------------------|\n",
    "| Generation | BERT | Failure | Generated errors or random output | Encoder-only model; not trained for next-token generation |\n",
    "| Generation | RoBERTa | Failure | Could not generate coherent text | Encoder-only architecture |\n",
    "| Generation | BART | Partial Success | Generated short meaningful text | Encoder-Decoder supports generation |\n",
    "| Fill-Mask | BERT | Success | Predicted words like \"create\", \"generate\" | Trained using Masked Language Modeling |\n",
    "| Fill-Mask | RoBERTa | Success | Accurate word predictions | Optimized MLM training |\n",
    "| Fill-Mask | BART | Partial Success | Less accurate predictions | Not primarily trained for MLM |\n",
    "| QA | BERT | Partial Success | Returned short or vague answer | Not fine-tuned for QA |\n",
    "| QA | RoBERTa | Partial Success | Inconsistent answer | Base model without QA fine-tuning |\n",
    "| QA | BART | Partial Success | Answered but lacked precision | Needs QA fine-tuning |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
