{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OjUODFBneNt",
        "outputId": "2a1b679f-9900-4899-beb3-4dcd1c48caae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Define our three models\n",
        "models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- EXPERIMENT 1: TEXT GENERATION ---\")\n",
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "for name, model_path in models.items():\n",
        "    print(f\"\\nTesting {name}...\")\n",
        "    try:\n",
        "        # We use the text-generation pipeline\n",
        "        generator = pipeline(\"text-generation\", model=model_path)\n",
        "        result = generator(prompt, max_length=20, num_return_sequences=1)\n",
        "        print(f\"Result: {result[0]['generated_text']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"FAILURE: {name} cannot generate text easily. Error: {str(e)[:100]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcaJthFKnn63",
        "outputId": "cee28617-18a9-4cbc-d0f2-7987c02b476c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- EXPERIMENT 1: TEXT GENERATION ---\n",
            "\n",
            "Testing BERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: The future of Artificial Intelligence is................................................................................................................................................................................................................................................................\n",
            "\n",
            "Testing RoBERTa...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: The future of Artificial Intelligence is\n",
            "\n",
            "Testing BART...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: The future of Artificial Intelligence is audiences Overviewessor381essoressoressorCRECREessor381Ops Slaveessoressor38188essorOps381381Opsessoressor Janeiro Janeiro Episode rejuvenRPG childishessoratefulOps381essor AllaahessorCRE hurled Highly shrinkulo0101essorinea0101Ops hurledineainea01 bottle locally terrifying locallycue sched ToolsWOR0101 Slave rejuven Receasonable rejuveninea rejuvenRPG rejuven rejuveninea651 bizarre rejuven rejuvenasonable bottle rejuven rejuvenWild rejuven rejuven rejuven Fiber rejuven locally rejuven rejuvenovie rejuvenWOR rejuven bottle rejuven ignorance rejuven rejuven preferring sleeper rejuven rejuven locally Cologne rejuvenTY rejuvenWOR fullerWOR rejuven rejuvenerno rejuvenaires rejuven locally ignorance Thumbnails rejuven rejuven ignorance preferring rejuven rejuven Rights rejuven rejuven sleeper rejuvenHCR locally locally rejuven bottle locally rejuvenHCR rejuven rejuven Thumbnails rejuven locally weight rejuven locally Brigham rejuven rejuven schedaires locally rejuven461 bottle rejuven locallyaires rejuvenTY locally rejuvenWOR locally rejuvenRPG651 rejuven rejuven weight weightaires sleeper rejuvenWOR Brigham rejuven== locally locallytten rejuven Brigham rejuven locally locally locally01 rejuven locally fold preferring rejuven ignorance sched rejuven rejuven startups locally rejuven locally sleeper rejuven locallyproclaimed ignorance fuller Thumbnails Thumbnails \"# rejuven rejuvenShop rejuven ones rejuven tease rejuven locally amd Rights ignorance locally influenceBrain influence locally tease]) rejuven Thumbnails weight weight weight rejuven rejuvenigion rejuvenigionigion locallyaires locally  ̶ rejuven rejuven bottle weight weight sleeper weight weight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- EXPERIMENT 2: FILL-MASK ---\")\n",
        "\n",
        "for name, model_path in models.items():\n",
        "    # Fix the mask token based on the model's requirements\n",
        "    mask_token = \"[MASK]\" if name == \"BERT\" else \"<mask>\"\n",
        "    text = f\"The goal of Generative AI is to {mask_token} new content.\"\n",
        "\n",
        "    print(f\"\\nTesting {name}...\")\n",
        "    try:\n",
        "        filler = pipeline(\"fill-mask\", model=model_path)\n",
        "        result = filler(text)\n",
        "        # Show top 2 predictions\n",
        "        preds = [r['token_str'] for r in result[:2]]\n",
        "        print(f\"Top Predictions: {preds}\")\n",
        "    except Exception as e:\n",
        "        print(f\"FAILURE: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZHzeJrppDGR",
        "outputId": "cdcb093b-084e-4f30-97ee-b850798b5efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- EXPERIMENT 2: FILL-MASK ---\n",
            "\n",
            "Testing BERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Predictions: ['create', 'generate']\n",
            "\n",
            "Testing RoBERTa...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Predictions: [' generate', ' create']\n",
            "\n",
            "Testing BART...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Predictions: [' create', ' help']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- EXPERIMENT 3: QUESTION ANSWERING ---\")\n",
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question = \"What are the risks?\"\n",
        "\n",
        "for name, model_path in models.items():\n",
        "    print(f\"\\nTesting {name}...\")\n",
        "    try:\n",
        "        qa_bot = pipeline(\"question-answering\", model=model_path)\n",
        "        result = qa_bot(question=question, context=context)\n",
        "        print(f\"Answer: {result['answer']}\")\n",
        "        print(f\"Confidence Score: {round(result['score'], 4)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"FAILURE: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--CmJa5LpzR4",
        "outputId": "a3c8e20b-6a9d-45dd-8a6e-a1f4af12126b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- EXPERIMENT 3: QUESTION ANSWERING ---\n",
            "\n",
            "Testing BERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Generative AI poses significant risks such as hallucinations\n",
            "Confidence Score: 0.0085\n",
            "\n",
            "Testing RoBERTa...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: ,\n",
            "Confidence Score: 0.0051\n",
            "\n",
            "Testing BART...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: deepfakes\n",
            "Confidence Score: 0.0198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Task           | Model   | Classification (Success/Failure) | Observation (What actually happened?)                                     | Why did this happen? (Architectural Reason)                                                                   |\n",
        "| :------------- | :------ | :------------------------------- | :------------------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------ |\n",
        "| **Generation** | BERT    | Failure                          | It printed a long line of dots and periods instead of meaningful text.    | BERT is a pure **encoder** model and is not designed for text generation or next-token prediction.            |\n",
        "|                | RoBERTa | Failure                          | It did not output anything; the result was completely blank.              | RoBERTa is also an **encoder-only** model and cannot generate text without a decoder.                         |\n",
        "|                | BART    | Failure                          | It generated a large block of random, nonsensical words.                  | The **base BART model** can generate text, but without task-specific fine-tuning, its outputs are incoherent. |\n",
        "| **Fill-Mask**  | BERT    | Success                          | It correctly predicted words like “create” and “generate.”                | BERT is trained using **Masked Language Modeling (MLM)**, making it ideal for fill-mask tasks.                |\n",
        "|                | RoBERTa | Success                          | It accurately predicted masked words such as “generate” and “create.”     | RoBERTa improves on BERT’s MLM training with more data and no NSP, leading to better predictions.             |\n",
        "|                | BART    | Success                          | It successfully suggested words like “create” and “help.”                 | BART is trained by reconstructing corrupted text, which helps it infer missing tokens well.                   |\n",
        "| **QA**         | BERT    | Bad                              | It produced a very long, strange answer with a very low confidence score. | The model architecture supports QA, but the **base model lacks QA fine-tuning**.                              |\n",
        "|                | RoBERTa | Failure                          | It output only a single comma instead of an answer.                       | Without **extractive QA fine-tuning**, RoBERTa cannot properly locate answers in context.                     |\n",
        "|                | BART    | Bad                              | It picked a random word like “deepfakes” from the end of the passage.     | BART was not fine-tuned for QA, so it generates or selects tokens without true comprehension.                 |\n"
      ],
      "metadata": {
        "id": "9b-XcYXHsmzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I experimented with BERT, RoBERTa and BART in this lab. I discovered that BERT and RoBERTa are very good at understanding context (Fill-Mask), but not so skilled at generating text, still they can't generate any texts since Encoder-only GoT. BART is an Encoder-Decoder, so it is capable of producing text, but the base version requires additional training to be useful for Q&A et cetera. This experiment taught me that choosing the right architecture is just as crucial as the size of the model."
      ],
      "metadata": {
        "id": "PM68kwj_vRx3"
      }
    }
  ]
}