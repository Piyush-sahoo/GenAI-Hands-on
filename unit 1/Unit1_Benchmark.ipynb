{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liw2b-ScsU8E",
        "outputId": "d73cec23-7a13-4689-81e8-335e5d167508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 1: Text Generation\n",
        "Prompt : \"The future of Artificial Intelligence is\""
      ],
      "metadata": {
        "id": "LpjdPHVetevM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT\n",
        "gen_bert = pipeline(\"text-generation\", model=\"bert-base-uncased\")\n",
        "gen_bert(\"The future of Artificial Intelligence is\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRuzXtb2thtB",
        "outputId": "88d3e0de-22bd-4261-fc11-889f740d75d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RoBERTa\n",
        "gen_roberta = pipeline(\"text-generation\", model=\"roberta-base\")\n",
        "gen_roberta(\"The future of Artificial Intelligence is\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8cd9UsxuL7L",
        "outputId": "2753d81a-3ad2-402d-da50-7f21270d9c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'The future of Artificial Intelligence is'}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BART\n",
        "gen_bart = pipeline(\"text-generation\", model=\"facebook/bart-base\")\n",
        "gen_bart(\"The future of Artificial Intelligence is\", max_length=30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siH-haJ3uNfx",
        "outputId": "57a17708-6beb-4837-deb5-df25cf1b6855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'The future of Artificial Intelligence is duck goodicho Assessment Christine ton 1200 Assessment Assessment Assessment laughterMKTemperature Genetic Assessment dramas Assessment Langreddederationederation Transactions Assessment Christine total good Inn Inn Inncient Assessment Assessment Tunnel AssessmentCurrent enslcheat«cheat offended Christine Christine Christine total Christine bitcoin toncheati Assessment teenager took Assessment 1929 Assessmentaging« Transactions Inn duck 570 Assessment Assessment Christineexcluding prank dashed teenager AUTH GeneticMK Inn Genetic cooperative Genetic scissorsRedd teenager tonii Genetic AssessmentGirl Genetic toni hat Assessment Assessment 1929 ton teenager AUTH ton WITHOUT Assessment Inn Inn insists toni Assessment Inn teenager vaping ton Assessment teenager Assessmentvere Inn Assessment bitcoin Inn Assessment scarf insists ton teenager bedrocki Genetici Assessment== teenager bitcoin tonantha smear Assessment 1929iMK ton teenager teenager insistswar Genetic Assessmenti hatantha Genetic Assessment Inni hati Assessment Assessment teenager total Genetic socksi bitcoinii Assessment GamerGate Assessment teenager Innioxicityrogramoxicityantha Detection explanatory teenager insists insists setup teenager shoes Assessment teenager teenager scissorsQB scarf teenager -------------------------------- insists teenager teenager teenager teasing calc Assessment Assessment supplement tonantha bedrock insistsMKantha teenager sprinkleantha teenager insists shoes Assessment AssessmentCla teenager teenager scarf teenager teenager Genetic Genetic insists insistsanthaoxicity teenagerantha insists insistsMK teenageranthaantha Genetic sprinkle teenager vaping calc== teenager teenager spl teenagerantha GamerGateantha calcMK McKenna scarfanthaantha teenager teenager calc 2018antha calc'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "None of the tested models produced accurate free-form text generation, as their architectures and training objectives are not optimized for causal language modeling."
      ],
      "metadata": {
        "id": "dko9YXNTx8Gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 2: Fill-Mask (Masked Language Modeling)\n",
        "Sentence : \"The goal of Generative AI is to [MASK] new content.\""
      ],
      "metadata": {
        "id": "Qv8uG8irui5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT\n",
        "fill_bert = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "fill_bert(\"The goal of Generative AI is to [MASK] new content.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSL4UpjBumcF",
        "outputId": "59d23998-8d6f-49a1-9e56-b533b58fbc41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.5396932363510132,\n",
              "  'token': 3443,\n",
              "  'token_str': 'create',\n",
              "  'sequence': 'the goal of generative ai is to create new content.'},\n",
              " {'score': 0.15575720369815826,\n",
              "  'token': 9699,\n",
              "  'token_str': 'generate',\n",
              "  'sequence': 'the goal of generative ai is to generate new content.'},\n",
              " {'score': 0.05405500903725624,\n",
              "  'token': 3965,\n",
              "  'token_str': 'produce',\n",
              "  'sequence': 'the goal of generative ai is to produce new content.'},\n",
              " {'score': 0.04451530799269676,\n",
              "  'token': 4503,\n",
              "  'token_str': 'develop',\n",
              "  'sequence': 'the goal of generative ai is to develop new content.'},\n",
              " {'score': 0.01757744885981083,\n",
              "  'token': 5587,\n",
              "  'token_str': 'add',\n",
              "  'sequence': 'the goal of generative ai is to add new content.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RoBERTa\n",
        "fill_roberta = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
        "fill_roberta(\"The goal of Generative AI is to <mask> new content.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTcAOrL_upHG",
        "outputId": "464fc0b4-88ab-4928-d4f3-ccfd4ecc7dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.3711312413215637,\n",
              "  'token': 5368,\n",
              "  'token_str': ' generate',\n",
              "  'sequence': 'The goal of Generative AI is to generate new content.'},\n",
              " {'score': 0.3677145540714264,\n",
              "  'token': 1045,\n",
              "  'token_str': ' create',\n",
              "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
              " {'score': 0.08351420611143112,\n",
              "  'token': 8286,\n",
              "  'token_str': ' discover',\n",
              "  'sequence': 'The goal of Generative AI is to discover new content.'},\n",
              " {'score': 0.021335121244192123,\n",
              "  'token': 465,\n",
              "  'token_str': ' find',\n",
              "  'sequence': 'The goal of Generative AI is to find new content.'},\n",
              " {'score': 0.016521666198968887,\n",
              "  'token': 694,\n",
              "  'token_str': ' provide',\n",
              "  'sequence': 'The goal of Generative AI is to provide new content.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BART\n",
        "fill_bart = pipeline(\"fill-mask\", model=\"facebook/bart-base\")\n",
        "fill_bart(\"The goal of Generative AI is to <mask> new content.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED4mUHdpurwJ",
        "outputId": "28ac570f-0e96-4dc9-ec10-c109918bec27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.07461541891098022,\n",
              "  'token': 1045,\n",
              "  'token_str': ' create',\n",
              "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
              " {'score': 0.06571870297193527,\n",
              "  'token': 244,\n",
              "  'token_str': ' help',\n",
              "  'sequence': 'The goal of Generative AI is to help new content.'},\n",
              " {'score': 0.060880109667778015,\n",
              "  'token': 694,\n",
              "  'token_str': ' provide',\n",
              "  'sequence': 'The goal of Generative AI is to provide new content.'},\n",
              " {'score': 0.03593561053276062,\n",
              "  'token': 3155,\n",
              "  'token_str': ' enable',\n",
              "  'sequence': 'The goal of Generative AI is to enable new content.'},\n",
              " {'score': 0.03319477662444115,\n",
              "  'token': 1477,\n",
              "  'token_str': ' improve',\n",
              "  'sequence': 'The goal of Generative AI is to improve new content.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All three models performed similarly on the fill-mask task because they were trained to reconstruct missing tokens using contextual information."
      ],
      "metadata": {
        "id": "MqhsNQKDyKdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 3: Question Answering\n",
        "Question : \"What are the risks?\"\n",
        "\n",
        "Context : \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n"
      ],
      "metadata": {
        "id": "KfwJ-tXruzNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT\n",
        "qa_bert = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n",
        "\n",
        "qa_bert({\n",
        "    \"question\": \"What are the risks?\",\n",
        "    \"context\": \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rwtq5VdWu4HN",
        "outputId": "140b3dd0-2360-404c-d4b7-2d5d93d578b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.010208480060100555,\n",
              " 'start': 0,\n",
              " 'end': 71,\n",
              " 'answer': 'Generative AI poses significant risks such as hallucinations, bias, and'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RoBERTa\n",
        "qa_roberta = pipeline(\"question-answering\", model=\"roberta-base\")\n",
        "\n",
        "qa_roberta({\n",
        "    \"question\": \"What are the risks?\",\n",
        "    \"context\": \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLRCxyyivBNM",
        "outputId": "d4b5324c-a7ac-4512-ae67-3a6d3933f23b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.007568269269540906, 'start': 0, 'end': 10, 'answer': 'Generative'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BART\n",
        "qa_bart = pipeline(\"question-answering\", model=\"facebook/bart-base\")\n",
        "\n",
        "qa_bart({\n",
        "    \"question\": \"What are the risks?\",\n",
        "    \"context\": \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "514G9cyDvGD_",
        "outputId": "a826b16e-f373-4bcf-8daf-e7c7aa781594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.051301321014761925, 'start': 72, 'end': 82, 'answer': 'deepfakes.'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The QA pipeline performs extractive question answering, which aligns well with BERT’s encoder-only architecture. RoBERTa and BART were not fine-tuned or architecturally optimized for extractive QA in this configuration."
      ],
      "metadata": {
        "id": "-runeGRnyMAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deliverable: Observation Table\n",
        "\n",
        "| Task        | Model     | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "|------------|-----------|----------------------------------|---------------------------------------|---------------------------------------------|\n",
        "| Generation | BERT      | Failure                          | Generated random output with .....         | Encoder-only model so it cannot generate text autoregressively |\n",
        "| Generation | RoBERTa   | Failure                          | Just returened the same sentence as output            | Encoder-only model which lacks decoder for generation |\n",
        "| Generation | BART      | Success                          | The model produced incoherent and repetitive text containing unrelated tokens and no meaningful semantic structure    | Encoder–Decoder architecture enables generation |\n",
        "| Fill-Mask  | BERT      | Success                          | Correctly predicted missing words      | Trained using Masked Language Modeling (MLM) |\n",
        "| Fill-Mask  | RoBERTa   | Success                          | High-quality predictions               | Improved MLM training and more data |\n",
        "| Fill-Mask  | BART      | Success                          | Filled masked token correctly          | Denoising autoencoder objective |\n",
        "| QA         | BERT      | Partial Success                  | Extracted partial answer from the context              | Not fine-tuned on QA datasets |\n",
        "| QA         | RoBERTa   | Partial Success                  | One word answer from context only and low confidence              | Strong encoder but no QA fine-tuning |\n",
        "| QA         | BART      | Partial Success                  | One word answer  from context with extremely low confidence                  | Base model not optimized for extractive QA |\n"
      ],
      "metadata": {
        "id": "ekWmziijv1vq"
      }
    }
  ]
}