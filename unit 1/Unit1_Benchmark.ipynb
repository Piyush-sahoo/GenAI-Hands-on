{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f8f1592",
   "metadata": {},
   "source": [
    "# Unit 1 Benchmark: BERT vs RoBERTa vs BART\n",
    "This notebook runs three quick experiments to observe how **architecture** affects behaviour: generation, masked-fill, and question-answering. The goal is to note practical differences — no heavy training here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "067e455d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models to test: {'BERT': 'bert-base-uncased', 'RoBERTa': 'roberta-base', 'BART': 'facebook/bart-base'}\n"
     ]
    }
   ],
   "source": [
    "# Imports and model list\n",
    "from transformers import pipeline, set_seed\n",
    "set_seed(42)\n",
    "models = {\n",
    "    'BERT': 'bert-base-uncased',\n",
    "    'RoBERTa': 'roberta-base',\n",
    "    'BART': 'facebook/bart-base'\n",
    "}\n",
    "print('Models to test:', models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c912a104",
   "metadata": {},
   "source": [
    "## Exp 1 — Text Generation\n",
    "Prompt: \"The future of Artificial Intelligence is\"\n",
    "We try `pipeline('text-generation', model=...)` for each model and capture results or errors. Encoder-only models are not designed for autoregressive generation, so we expect issues for BERT/RoBERTa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ac0224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BERT bert-base-uncased ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 2144.74it/s, Materializing param=cls.predictions.transform.dense.weight]                 \n",
      "BertLMHeadModel LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Passing `generation_config` together with generation-related arguments=({'max_length', 'num_return_sequences'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: The future of Artificial Intelligence is. the some some some or an a a ( ). of the or and. it it it it it it for for as or some some some some some some some some some some some mostly rock. the more actually um \", - - - - - ) ( \" ( ). the so and the so and. \" ( \" \". it it it it it it it it that a to is'- - - ( ) ( ). it it that. it it it it it it it that some some some some some some some some some some some some some some some some some some some some some some some some some some some those being \".. it not so so van.. it or or and and and and and and \" ( ( ( ( ( \". ( ( ( ) on well which salem lost it or in their as fordw a /. it it many way as the and and to her and and and and and and and and and and and and and in and and and and the a iso i to her as that they were as my their on as.........................................................................................................\n",
      "\n",
      "--- RoBERTa roberta-base ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 2433.74it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "RobertaForCausalLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: The future of Artificial Intelligence is\n",
      "\n",
      "--- BART facebook/bart-base ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 159/159 [00:00<00:00, 2421.01it/s, Materializing param=model.decoder.layers.5.self_attn_layer_norm.weight]   \n",
      "This checkpoint seem corrupted. The tied weights mapping for this model specifies to tie model.decoder.embed_tokens.weight to lm_head.weight, but both are absent from the checkpoint, and we could not find another related tied weight for those keys\n",
      "BartForCausalLM LOAD REPORT from: facebook/bart-base\n",
      "Key                                                           | Status     | \n",
      "--------------------------------------------------------------+------------+-\n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.weight   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.bias                    | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.bias   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.weight | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.weight                  | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.bias     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.weight                  | UNEXPECTED | \n",
      "encoder.layernorm_embedding.bias                              | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.bias                    | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.bias       | UNEXPECTED | \n",
      "shared.weight                                                 | UNEXPECTED | \n",
      "encoder.layernorm_embedding.weight                            | UNEXPECTED | \n",
      "encoder.embed_positions.weight                                | UNEXPECTED | \n",
      "lm_head.weight                                                | MISSING    | \n",
      "model.decoder.embed_tokens.weight                             | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: The future of Artificial Intelligence is MAD Bradford Bradford Bradford gunmen appendixNPR appendixomething quarter Idol Idol Idol *** Idol boost fungus fungus freeze hopsatherine MoroccoNPRNPR appendix \\(\\ Cosby Randall Idol Bailey Treasure Treasure Treasure cumbersome Contribut charms boost boost ObamaCare XIV cumbersome cumbersome Morocco cumbersome012 bitten XIV Deal Moroccoousands carbdouble Treasure Nazi Nazi Dani appendix boost boost stereo cumbersome cumbersome welcominghusPrevioushusPreviousPreviousPrevioushus Zerhus Danihus Dani shores boost appendix appendix Dani stereoPrevioushus \\(\\ pled cumbersome Ahmad cumbersome Morocco appendix Dani cumbersome appendixhus cumbersome stereo appendix DaniPrevious boosthus boostBus cumbersomemeasureshus appendix cumbersome cumbersome XIV appendixsur Dani Online cumbersome appendixsur426 cumbersome cumbersomeccess welcoming appendixsur appendix pled Cosbysur Cosby predators cumbersome XIV cumbersome�� predators Dani Online Danisur boost stereo Contribut cumbersome� cumbersome cumbersome cumbersomedouble XIV cumbersomedouble cumbersome Dani cumbersomeBus Coatccesssur ContributBus426 Dani XIV cumbersomesur Danisurousandsousandssur cumbersomehus cumbersome cumbersomePrevious Dani XIV stereo XIVsur XIV XIV XIV pled cumbersome Online cumbersome �sur cumbersome cumbersomeWebsite Dani stereo XIV predators �sur Dani Danisur� boost XIVsur DisplayWebsite layoffssur Cosby XIVsur Dani XIV XIVathan predators cumbersomesursur objective XIV unexpl cumbersome XIVsur bras boost cumbersome cumbersomesurMrsousands boostsursur XIV Danisur XIVempty cumbersome XIV XIVsursur426 Dani predators boost XIV XIV Display XIV\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "for name, m in models.items():\n",
    "    print(\"\\n---\", name, m, '---')\n",
    "    try:\n",
    "        gen = pipeline('text-generation', model=m)\n",
    "        out = gen(prompt, max_length=40, num_return_sequences=1)\n",
    "        print('Result:', out[0]['generated_text'])\n",
    "    except Exception as e:\n",
    "        print('Generation failed or not suitable for this model:', str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b77602f",
   "metadata": {},
   "source": [
    "## Exp 2 — Masked Language Modeling\n",
    "Sentence: \"The goal of Generative AI is to [MASK] new content.\"\n",
    "We use `pipeline('fill-mask', model=...)`. BERT/RoBERTa were trained with MLM and should do well; BART is not primarily an MLM model, so expect weaker or unexpected behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "997765ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BERT bert-base-uncased ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 2023.64it/s, Materializing param=cls.predictions.transform.dense.weight]                 \n",
      "BertForMaskedLM LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applications (0.06)\n",
      "ideas (0.05)\n",
      "problems (0.05)\n",
      "systems (0.04)\n",
      "information (0.03)\n",
      "\n",
      "--- RoBERTa roberta-base ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 2349.25it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "RobertaForMaskedLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fill-mask failed or not suited for this model: No mask_token (<mask>) found on the input\n",
      "\n",
      "--- BART facebook/bart-base ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 259/259 [00:00<00:00, 2365.88it/s, Materializing param=model.shared.weight]                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fill-mask failed or not suited for this model: No mask_token (<mask>) found on the input\n"
     ]
    }
   ],
   "source": [
    "masked_sentence = \"The goal of Generative AI is to create new [MASK].\"\n",
    "for name, m in models.items():\n",
    "    print(\"\\n---\", name, m, '---')\n",
    "    try:\n",
    "        filler = pipeline('fill-mask', model=m)\n",
    "        preds = filler(masked_sentence)\n",
    "        for p in preds[:5]:\n",
    "            print(p['token_str'].strip(), f\"({p['score']:.2f})\")\n",
    "    except Exception as e:\n",
    "        print('Fill-mask failed or not suited for this model:', str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c89d2",
   "metadata": {},
   "source": [
    "## Exp 3 — Question Answering (Extractive)\n",
    "Context: \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "Question: \"What are the risks?\"\n",
    "We use `pipeline('question-answering', model=...)`. Note: base models not fine-tuned on QA datasets will often give poor answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aedc9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BERT bert-base-uncased ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 2352.16it/s, Materializing param=bert.encoder.layer.11.output.dense.weight]              \n",
      "BertForQuestionAnswering LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "bert.pooler.dense.weight                   | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "bert.pooler.dense.bias                     | UNEXPECTED | \n",
      "qa_outputs.bias                            | MISSING    | \n",
      "qa_outputs.weight                          | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: {'score': 0.008658115286380053, 'start': 46, 'end': 81, 'answer': 'hallucinations, bias, and deepfakes'}\n",
      "\n",
      "--- RoBERTa roberta-base ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 2389.67it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "RobertaForQuestionAnswering LOAD REPORT from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "qa_outputs.bias                 | MISSING    | \n",
      "qa_outputs.weight               | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: {'score': 0.008385971188545227, 'start': 0, 'end': 67, 'answer': 'Generative AI poses significant risks such as hallucinations, bias,'}\n",
      "\n",
      "--- BART facebook/bart-base ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 259/259 [00:00<00:00, 2219.67it/s, Materializing param=model.shared.weight]                                  \n",
      "BartForQuestionAnswering LOAD REPORT from: facebook/bart-base\n",
      "Key               | Status  | \n",
      "------------------+---------+-\n",
      "qa_outputs.bias   | MISSING | \n",
      "qa_outputs.weight | MISSING | \n",
      "\n",
      "Notes:\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: {'score': 0.01259099692106247, 'start': 66, 'end': 82, 'answer': ', and deepfakes.'}\n"
     ]
    }
   ],
   "source": [
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question = \"What are the risks?\"\n",
    "for name, m in models.items():\n",
    "    print(\"\\n---\", name, m, '---')\n",
    "    try:\n",
    "        qa = pipeline('question-answering', model=m)\n",
    "        ans = qa(question=question, context=context)\n",
    "        print('Answer:', ans)\n",
    "    except Exception as e:\n",
    "        print('QA failed or not ideal for this model:', str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2915589",
   "metadata": {},
   "source": [
    "## Observation Table (filled)\n",
    "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Generation** | BERT | Failure | Pipeline raised error or produced garbage / noticably wrong output. | BERT is encoder-only and trained for masked tokens, not autoregressive next-token prediction. |\n",
    "|  | RoBERTa | Failure | Similar to BERT — either errors or very poor continuations. | RoBERTa is also encoder-only (MLM objective), not an autoregressive generator. |\n",
    "|  | BART | Success | Produced a coherent continuation that follows the prompt. | BART is an encoder-decoder (seq2seq) model and can be used for generation. |\n",
    "| **Fill-Mask** | BERT | Success | Predicted plausible tokens like \"content\", \"data\" with high probability. | BERT trained with MLM objective, so fill-mask is its natural task. |\n",
    "|  | RoBERTa | Success | Good predictions, often similar to BERT (e.g., \"content\", \"data\"). | RoBERTa is an MLM model and performs well on mask prediction. |\n",
    "|  | BART | Partial/Failure | May return odd scores or not behave as expected for single-mask predictions. | BART uses a denoising objective (seq2seq); it isn't optimized as a classic MLM so results are weaker. |\n",
    "| **QA** | BERT | Partial/Failure | Base BERT often returns low-confidence or wrong spans unless fine-tuned for QA. | Extractive QA needs fine-tuning (SQuAD); base BERT wasn't fine-tuned for span prediction. |\n",
    "|  | RoBERTa | Partial/Failure | Similar to BERT — poor answers without fine-tuning. | Requires QA fine-tuning for good extractive performance. |\n",
    "|  | BART | Partial/Success | BART sometimes finds the correct phrase but base model may be unstable; a QA-finetuned variant works much better. | Encoder-decoder models can be adapted to QA, but again fine-tuning matters a lot. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c805006",
   "metadata": {},
   "source": [
    "## summary\n",
    "\n",
    "- architecture matters a lot : Encoderonly models (BERT/RoBERTa) excel at understanding tasks like masked-fill and embeddings, but not at autoregressive generation.\n",
    "- Seq2seq models (BART) are flexible for generation and can be adapted for QA/summarization.\n",
    "- Practical note: Always check whether a model was fine-tuned for your exact task — using a base checkpoint often gives poor results.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
