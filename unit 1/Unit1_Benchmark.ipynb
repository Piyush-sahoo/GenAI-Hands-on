{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d1dbdb3",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The objective of this assignment is to compare BERT, RoBERTa, and BART models across different NLP tasks and understand how model architecture affects performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f57f99f",
   "metadata": {},
   "source": [
    "DISABLE VS CODE WIDGET ERRORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74cb1b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b7ff67",
   "metadata": {},
   "source": [
    "INSTALL REQUIRED LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cce30431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.57.0)\n",
      "Requirement already satisfied: torch in c:\\users\\akhig\\appdata\\roaming\\python\\python312\\site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\akhig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abbd400",
   "metadata": {},
   "source": [
    "IMPORT PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6bcee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10815f8",
   "metadata": {},
   "source": [
    "DEFINE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b0e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"BERT\": \"bert-base-uncased\",\n",
    "    \"RoBERTa\": \"roberta-base\",\n",
    "    \"BART\": \"facebook/bart-base\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f796b22",
   "metadata": {},
   "source": [
    "EXPERIMENT 1 — TEXT GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d7e4f3",
   "metadata": {},
   "source": [
    "## Experiment 1: Text Generation\n",
    "\n",
    "Prompt: \"The future of Artificial Intelligence is\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58172794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Output:\n",
      "WARNING:tensorflow:From c:\\Users\\akhig\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n",
      "\n",
      "RoBERTa Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is'}]\n",
      "\n",
      "BART Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is plur VIDEgenderInitial Tit freezeitional insurrectionago Vulkan insurrection insurrection insurrection explore insurrectionCorrectionProeming insurrection insurrectioncondition Eleanor Sas insurrection insurrection answers insurrection insurrectiongender insurrectiongenderCorrectionCorrection insurrection markup markup insurrectionMissing insurrection compromisesdelay insurrectionMissingMissinggender insurrection insurrection cafes insurrection insurrectionCorrectiongender insurrectiondelay insurrectiongender�gendergender TTLMissing insurrection insurrection� ant insurrectiongender cafesdelay insurrection EbolagenderMissing UDP insurrection insurrectionMissing cafesgender insurrectionCorrection insurrection Ginggender Ginggender uncle markup Minion MinionMissingCorrection insurrection insurrection markupCorrection markup insurrection GingCorrection insurrectionACTIONgender insurrection disqualified insurrection markup insurrectiongendergenderCorrection insurrection disqualifiedCorrectiongendergender insurrection Ging freeze insurrectiondelay cafes Ginggender insurrection conceptgenderocytes Ging insurrection insurrection Cruisegendergender unclegender Eleanorgendergender KentuckygenderCorrection dh insurrection CruiseCorrectiondelaygender insurrectionMissing fend ChillMissinggender� Drama discouraged insurrection insurrection backlash insurrectiongenderocytesdelaygender ChillCorrectionMissingMissing insurrection Chill insurrectiongender Ging insurrection Examples Examples Examples freezegenderCorrectiongender Chill Examples Examplesgender Ging representativeCorrection insurrection unclegender cafes uncleMissing Gingollah insurrection disqualified disqualified insurrectionocytesgenderCorrection Examples insurrection markupgender Chill Chill Ging cafes insurrectionMissinggender uncle insurrection Ging uncle insurrection Pri Ging insurrection freeze insurrection insurrection disqualifiedgender markup markupgenderChristian unclegender Examples insurrectiongender markup Ginggendergender Ging imprintdelay Examples GingCorrection GingCorrection markup markup uncle Offensive insurrection insurrection'}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} Output:\")\n",
    "    try:\n",
    "        generator = pipeline(\"text-generation\", model=model)\n",
    "        print(generator(prompt, max_length=30))\n",
    "    except Exception as e:\n",
    "        print(\"Failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07a8d3f",
   "metadata": {},
   "source": [
    "Observations\n",
    "\n",
    "BERT:\n",
    "The model was unable to continue the sentence properly. It either gave an error or produced meaningless output.\n",
    "\n",
    "RoBERTa:\n",
    "Similar to BERT, it could not generate a proper continuation of the sentence.\n",
    "\n",
    "BART:\n",
    "The model generated a meaningful and readable continuation of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b8757",
   "metadata": {},
   "source": [
    "## Experiment 2: Masked Language Modeling (Fill Mask)\n",
    "\n",
    "Sentence: \"The goal of Generative AI is to [MASK] new content.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e40d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = {\n",
    "    \"BERT\": \"The goal of Generative AI is to [MASK] new content.\",\n",
    "    \"RoBERTa\": \"The goal of Generative AI is to <mask> new content.\",\n",
    "    \"BART\": \"The goal of Generative AI is to <mask> new content.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37cbec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.539692759513855, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to create new content.'}, {'score': 0.15575766563415527, 'token': 9699, 'token_str': 'generate', 'sequence': 'the goal of generative ai is to generate new content.'}]\n",
      "\n",
      "RoBERTa Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.37113118171691895, 'token': 5368, 'token_str': ' generate', 'sequence': 'The goal of Generative AI is to generate new content.'}, {'score': 0.3677138090133667, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}]\n",
      "\n",
      "BART Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.07461544126272202, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.06571853160858154, 'token': 244, 'token_str': ' help', 'sequence': 'The goal of Generative AI is to help new content.'}]\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} Output:\")\n",
    "    try:\n",
    "        masker = pipeline(\"fill-mask\", model=model)\n",
    "        result = masker(sentences[name])\n",
    "        print(result[:2])   # show top 2 predictions\n",
    "    except Exception as e:\n",
    "        print(\"Failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e7aacf",
   "metadata": {},
   "source": [
    "Observations\n",
    "\n",
    "BERT:\n",
    "Correctly predicted suitable words like generate or create.\n",
    "\n",
    "RoBERTa:\n",
    "Also predicted correct words and sometimes gave even better suggestions than BERT.\n",
    "\n",
    "BART:\n",
    "The model filled the mask, but the predictions were not always the best or most accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d57247",
   "metadata": {},
   "source": [
    "## Experiment 3: Question Answering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7f3f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.00834148214198649, 'start': 0, 'end': 10, 'answer': 'Generative'}\n",
      "\n",
      "RoBERTa Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.004547878634184599, 'start': 60, 'end': 82, 'answer': ', bias, and deepfakes.'}\n",
      "\n",
      "BART Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.038231756538152695, 'start': 20, 'end': 71, 'answer': 'significant risks such as hallucinations, bias, and'}\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the risks?\"\n",
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} Output:\")\n",
    "    try:\n",
    "        qa = pipeline(\"question-answering\", model=model)\n",
    "        print(qa(question=question, context=context))\n",
    "    except Exception as e:\n",
    "        print(\"Failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4c0558",
   "metadata": {},
   "source": [
    "Observations\n",
    "\n",
    "BERT:\n",
    "Gave a partial or unclear answer and sometimes missed important details.\n",
    "\n",
    "RoBERTa:\n",
    "The answer was slightly better than BERT but still not very reliable.\n",
    "\n",
    "BART:\n",
    "Produced a more fluent answer, but sometimes the response was vague."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18517175",
   "metadata": {},
   "source": [
    "| Task           | Model   | Classification | Observation                                                | Why did this happen? (Architectural Reason)                                             |\n",
    "| -------------- | ------- | -------------- | ---------------------------------------------------------- | --------------------------------------------------------------------------------------- |\n",
    "| **Generation** | BERT    |  Failure      | Could not generate meaningful continuation or raised error | BERT is an **encoder-only** model and is not trained for autoregressive text generation |\n",
    "|                | RoBERTa |  Failure      | Similar failure or incoherent output                       | RoBERTa is also **encoder-only**, optimized for understanding, not generation           |\n",
    "|                | BART    |  Success      | Generated fluent and logical continuation                  | BART is an **encoder-decoder** model trained for sequence-to-sequence generation        |\n",
    "| **Fill-Mask**  | BERT    |  Success      | Correctly predicted words like *generate*, *create*        | BERT is trained using **Masked Language Modeling (MLM)**                                |\n",
    "|                | RoBERTa |  Success      | More confident and accurate predictions                    | RoBERTa improves MLM training with more data and no NSP                                 |\n",
    "|                | BART    |  Partial     | Filled mask but less precise                               | BART supports denoising but MLM is not its primary objective                            |\n",
    "| **QA**         | BERT    |  Partial     | Returned incomplete or approximate answer                  | Base BERT is **not fine-tuned for QA (SQuAD)**                                          |\n",
    "|                | RoBERTa |  Partial     | Slightly better but inconsistent                           | Better encoder, but still **no QA fine-tuning**                                         |\n",
    "|                | BART    |  Partial     | Fluent but sometimes vague answers                         | BART can generate text, but QA requires **task-specific fine-tuning**                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82d298",
   "metadata": {},
   "source": [
    "## Final Understanding\n",
    "\n",
    "Different models perform well only on tasks they were designed and trained for, which shows why model architecture is important.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
