{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0V8Jz3XbH6O1"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch accelerate\n",
        "\n",
        "from transformers import pipeline\n",
        "import pandas as pd # To help display our Observation Table at the end"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}\n",
        "\n",
        "# Dictionary to store our results for the final table\n",
        "results = []"
      ],
      "metadata": {
        "id": "op_h5FxAKWGc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 1 - Text Generation"
      ],
      "metadata": {
        "id": "MVLUJKEHKpMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "print(\"--- Experiment 1: Text Generation Results ---\")\n",
        "for name, model_path in models.items():\n",
        "    print(f\"\\n[Testing Model: {name}]\")\n",
        "    try:\n",
        "        # Initializing the pipeline for text-generation\n",
        "        # Note: BERT/RoBERTa aren't natively 'causal' so they might warn you\n",
        "        generator = pipeline(\"text-generation\", model=model_path)\n",
        "\n",
        "        # Generating text\n",
        "        output = generator(prompt, max_new_tokens=15, truncation=True)\n",
        "        generated_text = output[0]['generated_text']\n",
        "\n",
        "        # Print the result immediately\n",
        "        print(f\"Result: {generated_text}\")\n",
        "        results.append({\"Model\": name, \"Task\": \"Text Gen\", \"Output\": generated_text})\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"FAILED (Architecture Mismatch: {str(e)[:50]}...)\"\n",
        "        print(f\"Result: {error_msg}\")\n",
        "        results.append({\"Model\": name, \"Task\": \"Text Gen\", \"Output\": \"ERROR\"})\n",
        "\n",
        "print(\"\\n--- Experiment 1 Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbqMUquSKqjk",
        "outputId": "29573172-eefe-401e-d709-b94f6ecdbb98"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Experiment 1: Text Generation Results ---\n",
            "\n",
            "[Testing Model: BERT]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: The future of Artificial Intelligence is...............\n",
            "\n",
            "[Testing Model: RoBERTa]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: The future of Artificial Intelligence is\n",
            "\n",
            "[Testing Model: BART]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: The future of Artificial Intelligence is salad salad hangs thorough thorough Bry cue thorough thorough thoroughificeificeryptionryption\n",
            "\n",
            "--- Experiment 1 Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 2: Masked Language Modeling (Missing Word)"
      ],
      "metadata": {
        "id": "xlHeXRtGLh_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 2: Masked Language Modeling (Fill-Mask)\n",
        "# Task: Predict the [MASK] token.\n",
        "print(\"Experiment 2: Masked Language Modeling\")\n",
        "prompt_mask = \"The goal of Generative AI is to [MASK] new content.\"\n",
        "\n",
        "for name, model_id in models.items():\n",
        "    print(f\"\\nTesting {name} ({model_id})...\")\n",
        "    try:\n",
        "        # BERT uses [MASK], RoBERTa/BART use <mask>, so we adjust automatically.\n",
        "        if \"roberta\" in model_id or \"bart\" in model_id:\n",
        "            current_prompt = prompt_mask.replace(\"[MASK]\", \"<mask>\")\n",
        "        else:\n",
        "            current_prompt = prompt_mask\n",
        "\n",
        "        mask_pipe = pipeline('fill-mask', model=model_id)\n",
        "        result = mask_pipe(current_prompt)\n",
        "        print(f\"Top prediction: {result[0]['token_str']} (Score: {result[0]['score']:.4f})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Result: FAILED. Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE7dEFrdLoul",
        "outputId": "a653e059-9a6e-4ff8-8bfa-49f96c22857e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment 2: Masked Language Modeling\n",
            "\n",
            "Testing BERT (bert-base-uncased)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top prediction: create (Score: 0.5397)\n",
            "\n",
            "Testing RoBERTa (roberta-base)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top prediction:  generate (Score: 0.3711)\n",
            "\n",
            "Testing BART (facebook/bart-base)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top prediction:  create (Score: 0.0746)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 3: Question Answering\n"
      ],
      "metadata": {
        "id": "pRSx3gKkM9Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 3: Question Answering\n",
        "# Task: Extract answer from context.\n",
        "print(\"Experiment 3: Question Answering\")\n",
        "qa_context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "qa_question = \"What are the risks?\"\n",
        "\n",
        "for name, model_id in models.items():\n",
        "    print(f\"\\nTesting {name} ({model_id})...\")\n",
        "    try:\n",
        "        qa_pipe = pipeline('question-answering', model=model_id)\n",
        "        result = qa_pipe(question=qa_question, context=qa_context)\n",
        "        print(f\"Answer: '{result['answer']}' (Score: {result['score']:.4f})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Result: FAILED or LOW CONFIDENCE. Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8gZTerEMS50",
        "outputId": "b3ffdbf8-46c0-4243-8373-0eaf037695bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment 3: Question Answering\n",
            "\n",
            "Testing BERT (bert-base-uncased)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: ', and deepfakes' (Score: 0.0120)\n",
            "\n",
            "Testing RoBERTa (roberta-base)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 'as hallucinations, bias, and deepfakes' (Score: 0.0080)\n",
            "\n",
            "Testing BART (facebook/bart-base)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 'Generative' (Score: 0.0196)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dJZKW6yDNBGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **Generation** | BERT | *Failure* | Generated a repetitive string of dots: `...............` | BERT is an Encoder; it isn't trained to predict the next word in a sequence (lacks causal masking). |\n",
        "| | RoBERTa | *Failure* | Returned an empty string/only the prompt. | RoBERTa is an Encoder optimized for understanding; it cannot autoregressively generate new tokens. |\n",
        "| | BART | *Partial Success* | Generated a \"word salad\": `salad salad hangs thorough...` | BART is an Encoder-Decoder designed for generation, but the \"base\" weights are un-tuned for coherence. |\n",
        "| **Fill-Mask** | BERT | *Success* | Predicted '**create**' (Score: 0.5397). | BERT is natively trained on Masked Language Modeling (MLM). |\n",
        "| | RoBERTa | *Success* | Predicted '**generate**' (Score: 0.3711). | RoBERTa is an optimized Encoder specifically built for high-performance MLM. |\n",
        "| | BART | *Success* | Predicted '**create**' (Score: 0.0746). | Its Seq2Seq objective includes denoising (filling gaps), though it is less specialized for this than pure Encoders. |\n",
        "| **QA** | BERT | *Failure* | Returned a fragment: `', and deepfakes'` (Score: 0.0120). | Encoder weights are newly initialized for QA; requires fine-tuning on a dataset like SQuAD to locate answer spans. |\n",
        "| | RoBERTa | *Failure* | Returned a partial phrase: `'as hallucinations, bias, and deepfakes'` (Score: 0.0080). | Lacks the specific fine-tuning required to map questions to exact context offsets despite its strong understanding. |\n",
        "| | BART | *Failure* | Returned a single irrelevant word: `'Generative'` (Score: 0.0196). | The Decoder-side weights for extraction are not trained in the base model, leading to random or poor extraction. |"
      ],
      "metadata": {
        "id": "v1saQaSiPGhd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ag394FFkNriI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}