{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9737aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5880c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6427d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"unit 1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2969ffc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    print(\"File loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{file_path}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3feffdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Preview ---\n",
      "Generative AI and Its Applications: A Foundational Briefing\n",
      "\n",
      "Executive Summary\n",
      "\n",
      "This document provides a comprehensive overview of Generative AI, synthesizing foundational concepts, technological underpinnings, and practical applications as outlined in the course materials from PES University. Generative AI represents a transformative subset of Artificial Intelligence focused on creating novel content, a capability primarily driven by the advent of Large Language Models (LLMs). The evolution of ...\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Data Preview ---\")\n",
    "print(text[:500] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed69dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c4dc02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Generative AI is a revolutionary technology that\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1af13f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI is a revolutionary technology that has gained a significant audience as a whole and has generated millions of people from a wide range of disciplines.\n",
      "\n",
      "\n",
      "\n",
      "The new AI is being deployed by a number of universities in the United States. A\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pipeline with the specific model\n",
    "fast_generator = pipeline('text-generation', model='distilgpt2')\n",
    "\n",
    "# Generate text\n",
    "output_fast = fast_generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(output_fast[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b19cc954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6859458ba54ce79f0794a8a9d96e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HP\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa7d3952de242db93aab1ecbf61b0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4473a89a2a40fd882e811707923744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb40c1293b04b9b974545960d906b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ada8d684fd4350b45e1511f41df65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80464b764907469cbe6e0459f5f062d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abf433af6114158ae6f565a4e533493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI is a revolutionary technology that advances artificial intelligence using machine learning.\n",
      "\n",
      "AI is the next best thing to artificial intelligence, and it's not even going anywhere.\n",
      "\n",
      "A lot of this is going to happen through new techniques – from\n"
     ]
    }
   ],
   "source": [
    "smart_generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "output_smart = smart_generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(output_smart[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61555076",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb37edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = \"Transformers revolutionized NL\\\n",
    "    P.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d27f5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Transform', 'ers', 'Ġrevolution', 'ized', 'ĠN', 'LP', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sample_sentence)\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29fa0918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [41762, 364, 5854, 1143, 399, 19930, 13]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"Token IDs: {token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebe6fbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0307ba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('Transformers', 'NNS'), ('revolutionized', 'VBD'), ('NLP', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags =nltk.pos_tag(nltk.word_tokenize(sample_sentence))\n",
    "print(f\"POS Tags: {pos_tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c511cf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b7216c7b2a489cb2ec9a25fef3f966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113565bc70a54f40b305a1ae73e1f889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19d6561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity               | Type       | Score\n",
      "---------------------------------------------\n",
      "AI                   | MISC       | 0.98\n",
      "PES University       | ORG        | 0.99\n",
      "AI                   | MISC       | 0.98\n",
      "Large Language Models | MISC       | 0.91\n",
      "LLMs                 | MISC       | 0.90\n",
      "Transformer          | MISC       | 0.99\n"
     ]
    }
   ],
   "source": [
    "snippet = text[:1000]\n",
    "entities = ner_pipeline(snippet)\n",
    "\n",
    "print(f\"{'Entity':<20} | {'Type':<10} | {'Score':<5}\")\n",
    "print(\"-\"*45)\n",
    "for entity in entities:\n",
    "    if entity['score'] > 0.90:\n",
    "        print(f\"{entity['word']:<20} | {entity['entity_group']:<10} | {entity['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "834e087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract a specific section for summarization\n",
    "transformer_section = \"\"\"\n",
    "The introduction of the Transformer architecture in the 2017 paper \"Attention is all you need\" was a watershed moment in AI. It provided a more effective and scalable way to handle sequential data like text, replacing older, less efficient methods like recurrence (RNNs) and convolutions.\n",
    "The fundamental innovation of the Transformer is the attention mechanism. This component allows the model to weigh the importance of different words (tokens) in the input sequence when making a prediction. In essence, for each word it processes, the model can \"pay attention\" to all other words in the input, helping it understand context, resolve ambiguity, and handle long-range dependencies. This is crucial for tasks like translation, summarization, and question answering.\n",
    "The Transformer architecture consists of an encoder stack (to process the input) and a decoder stack (to generate the output), both of which heavily utilize multi-head attention and feed-forward networks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4984c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_sum = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "res_fast = fast_sum(transformer_section, max_length=60, min_length=30, do_sample=False)\n",
    "print(res_fast[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_sum = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "res_smart = smart_sum(transformer_section, max_length=60, min_length=30, do_sample=False)\n",
    "print(res_smart[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b332973",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e6fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the fundamental innovation of the Transformer?\",\n",
    "    \"What are the risks of using Generative AI?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    res = qa_pipeline(question=q, context=text[:5000])\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {res['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1db5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e65b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sentence = \"The goal of Generative AI is to create new [MASK].\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
