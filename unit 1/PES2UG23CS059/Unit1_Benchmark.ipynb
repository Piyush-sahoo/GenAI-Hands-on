{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it4vqyviYLF6",
        "outputId": "205cd801-af13-4234-a0b0-6ea727ba1166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup Complete. Models ready to test.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "\n",
        "models_to_test = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}\n",
        "\n",
        "set_seed(42)  # For reproducibility\n",
        "\n",
        "print(\"Setup Complete. Models ready to test.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Experiment 1: Text Generation ---\")\n",
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "for name, model_id in models_to_test.items():\n",
        "    print(f\"\\nTesting {name} ({model_id})...\")\n",
        "    try:\n",
        "        # Note: BERT and RoBERTa are NOT built for generation, so this might output garbage or fail.\n",
        "        # This is expected behavior for the assignment!\n",
        "        generator = pipeline('text-generation', model=model_id)\n",
        "        result = generator(prompt, max_length=30, num_return_sequences=1)\n",
        "        print(f\"Output: {result[0]['generated_text']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"FAILED: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSyC9d3CY1ve",
        "outputId": "0bb094f4-105c-4602-b7ba-a49caf8fe257"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Experiment 1: Text Generation ---\n",
            "\n",
            "Testing BERT (bert-base-uncased)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: The future of Artificial Intelligence is................................................................................................................................................................................................................................................................\n",
            "\n",
            "Testing RoBERTa (roberta-base)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: The future of Artificial Intelligence is\n",
            "\n",
            "Testing BART (facebook/bart-base)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: The future of Artificial Intelligence isOtherwise ShakOtherwise sure Shak Shak323208 df empir squat Shak chuckms healer df MarxismOtherwisePatrick Walkdy Shak slipsreleased df df32 slips debugger Walk slips Shak denim Shak df df df Walk Drawn Drawn Drawn 361 slipsino Person Princeton Shak chuckSpoilerSpoiler slips df df initially dismant Drawn Crkas Drawn Drawn spotsenc Shak Drawn Drawnino Drawn Drawn opposition Shak workload slips Shak slips df Drawn DrawnEngland Drawn32 Drawn DrawnSel Drawn workload Drawn Drawn Output Drawn Drawn communism Drawn Drawn debugger spots spots dfPost df df workload Drawn df Drawn dfJe Drawn DrawnaxeFrames df kingdoms df Drawnlein Drawn Drawn slips Drawn Drawneller Drawn Drawn df spots spots game Drawn Drawn LT Drawn Drawn Alvin Drawn origins sure Drawn Drawnaze spots Drawn Drawn princip Drawn gameStatus Alvin df df Alvin df impacting impacting spots spotsaily princip futures df principAlert skysc df spots principStatus Drawn Drawn Beet Drawn skysc Alvin df game sure Alvin workloadStatus Molecular princip debuggerkaskasFramesFrames Drawn Drawn origins df df princip Drawn DrawnwalkingFrameskas workload Drawnkasbreak Drawn Drawn2014 Hook df Drawnkas tables df Beet game df Drawn skysc Drawn debuggerkas spots Drawnaxekas impactingkaskas Beet20142014kas debugger impacting df df futures debuggerAlert originskas originskaskas df debugger skysckas Beet Beet debugger spots df elephants df df game game\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Experiment 2: Fill-Mask ---\")\n",
        "sentence = \"The goal of Generative AI is to <mask> new content.\" # BART/RoBERTa use <mask>\n",
        "sentence_bert = \"The goal of Generative AI is to [MASK] new content.\" # BERT uses [MASK]\n",
        "\n",
        "for name, model_id in models_to_test.items():\n",
        "    print(f\"\\nTesting {name} ({model_id})...\")\n",
        "\n",
        "    # Switch mask token based on model type\n",
        "    input_text = sentence_bert if name == \"BERT\" else sentence\n",
        "\n",
        "    unmasker = pipeline('fill-mask', model=model_id)\n",
        "    result = unmasker(input_text)\n",
        "\n",
        "    # Print top prediction\n",
        "    print(f\"Top Prediction: {result[0]['token_str']} (Score: {result[0]['score']:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN_srCxzZGR9",
        "outputId": "9d91fb6e-8914-43fa-9a96-657fd47c4a6d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Experiment 2: Fill-Mask ---\n",
            "\n",
            "Testing BERT (bert-base-uncased)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Prediction: create (Score: 0.5397)\n",
            "\n",
            "Testing RoBERTa (roberta-base)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Prediction:  generate (Score: 0.3711)\n",
            "\n",
            "Testing BART (facebook/bart-base)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Prediction:  create (Score: 0.0746)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Experiment 3: Question Answering ---\")\n",
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question = \"What are the risks?\"\n",
        "\n",
        "for name, model_id in models_to_test.items():\n",
        "    print(f\"\\nTesting {name} ({model_id})...\")\n",
        "\n",
        "    qa_pipeline = pipeline('question-answering', model=model_id)\n",
        "    result = qa_pipeline(question=question, context=context)\n",
        "\n",
        "    print(f\"Answer: {result['answer']} (Score: {result['score']:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCvs32JsZKKO",
        "outputId": "3e44bd35-ee3d-4325-d1f8-9bc0482e7c04"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Experiment 3: Question Answering ---\n",
            "\n",
            "Testing BERT (bert-base-uncased)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: hallucinations, bias, and deepfakes (Score: 0.0076)\n",
            "\n",
            "Testing RoBERTa (roberta-base)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: as hallucinations, bias, and deepfakes. (Score: 0.0041)\n",
            "\n",
            "Testing BART (facebook/bart-base)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Generative AI poses (Score: 0.0278)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation Table\n",
        "\n",
        "| Task | Model | Classification | Observation | Why did this happen? |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **Generation** | BERT | Failure | *Output was likely repetitive or nonsense.* | BERT is an **Encoder**, designed to understand text, not generate it (it's not a causal LM). |\n",
        "| | RoBERTa | Failure | *Output was likely nonsense.* | Like BERT, RoBERTa is an **Encoder** and cannot predict \"future\" tokens effectively. |\n",
        "| | BART | Success | *Generated a coherent sentence.* | BART is an **Encoder-Decoder**, explicitly designed for sequence generation. |\n",
        "| **Fill-Mask** | BERT | Success | *Predicted 'create' or 'generate'.* | BERT is trained on Masked Language Modeling (MLM), so it excels here. |\n",
        "| | RoBERTa | Success | *Predicted 'create' or 'generate'.* | RoBERTa is also trained on MLM (optimized BERT). |\n",
        "| | BART | Success | *Predicted 'create'.* | BART's encoder can handle masking tasks effectively. |\n",
        "| **QA** | BERT | Mixed | *Answer might be short/cutoff.* | Base BERT is not fine-tuned for QA (SQuAD), so performance is inconsistent. |\n",
        "| | RoBERTa | Mixed | *Answer might be poor.* | Base RoBERTa is not fine-tuned for QA; it needs a specific head for this. |\n",
        "| | BART | Success | *Extracted the list of risks.* | BART's seq2seq nature handles extracting answers from context well. |"
      ],
      "metadata": {
        "id": "p26KOYzrZTC3"
      }
    }
  ]
}