{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unit 1 Assignment – Model Benchmark Challenge\n",
    "\n",
    "Name: AMAR \n",
    "SRN: PES2UG23CS052\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b898f",
   "metadata": {},
   "source": [
    "Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1dbdb3",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The objective of this assignment is to compare BERT, RoBERTa, and BART models across different NLP tasks and understand how model architecture affects performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f57f99f",
   "metadata": {},
   "source": [
    "DISABLE VS CODE WIDGET ERRORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74cb1b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b7ff67",
   "metadata": {},
   "source": [
    "INSTALL REQUIRED LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce30431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: torch in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (74.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amar sagar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abbd400",
   "metadata": {},
   "source": [
    "IMPORT PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6bcee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10815f8",
   "metadata": {},
   "source": [
    "DEFINE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b0e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"BERT\": \"bert-base-uncased\",\n",
    "    \"RoBERTa\": \"roberta-base\",\n",
    "    \"BART\": \"facebook/bart-base\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f796b22",
   "metadata": {},
   "source": [
    "EXPERIMENT 1 — TEXT GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d7e4f3",
   "metadata": {},
   "source": [
    "## Experiment 1: Text Generation\n",
    "\n",
    "Prompt: \"The future of Artificial Intelligence is\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58172794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n",
      "\n",
      "RoBERTa Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amar Sagar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Amar Sagar\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is'}]\n",
      "\n",
      "BART Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amar Sagar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Amar Sagar\\.cache\\huggingface\\hub\\models--facebook--bart-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence isateurateur code code disease diseaseStandardkeesounded compared compared comparedoxicity proposes hooks hooks proposes proposes proposes SOU proposes sourcing jack�� impairmenturancesurancesinflammatoryShuturancesurancesilitating detainee impairment invarioredurancesurancesadvertisementilitatingilitating protectionhighurancesurances SlovenShutorgan invariurancesurances protectionurancesurancesurances di holdingadvertisementilitating limp hooks impairment rotilitatingurancesShuturances=\"urances impairmenturances Simple impairment impairment roturancesurances Bulleturancesurances holdingurances holdinginflammatory accur Bullet Slovenurancesurances=\" holdingurances Pageilitatingilitating negligence impairmentadvertisementilitatingadvertisement quartersilitating validilitating negligence negligence impairment negligence impairment impairment Pageurances holdingAllen impairment impairment Predicturancesilitatingilitating Bullet impairment rot negligence waking Bulleturances impairment impairmentder rot roturances�ilitatingadvertisementilitating negligence stars Bullet Bullet impairment impairment bestowed Page holding impairmentilitating Page negligenceStorage impairment rot impairment rot PageurancesurancesShut Caroline impairment rot rotilitatingilitatingurancesilitatingurances holding rot impairmentTM roturances negligence impairment pregnanciesilitating impairment rot delilitating Bullet Bullet os impairmentelve roturancesentry impairment impairment Dominican accurilitating impairmenturances impairment rotEREilitatingilitating Pageilitating impairmentilitating rot rot rot Page negligence Bullet negligence negligence Bullet impairment fisher negligence Bullet Bulletilitating impairment Bullet negligence Bullet holding impairmentShut impairment Bullet impairment Page impairment fisher Bulletilitating Page impairment impairmenturances Bullet impairment flashed impairmenturances Dominican impairment impairment impairment sciences impairmenturances negligence Bullet'}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} Output:\")\n",
    "    try:\n",
    "        generator = pipeline(\"text-generation\", model=model)\n",
    "        print(generator(prompt, max_length=30))\n",
    "    except Exception as e:\n",
    "        print(\"Failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07a8d3f",
   "metadata": {},
   "source": [
    "### Observation – Experiment 1\n",
    "\n",
    "BERT and RoBERTa were unable to generate meaningful text and mostly repeated or produced irrelevant output because they are encoder-only models not designed for text generation.\n",
    "BART was able to generate some continuation since it follows an encoder–decoder architecture.\n",
    "However, the generated text was not very coherent because the base BART model is not fine-tuned for open-ended text generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b8757",
   "metadata": {},
   "source": [
    "## Experiment 2: Masked Language Modeling (Fill Mask)\n",
    "\n",
    "Sentence: \"The goal of Generative AI is to [MASK] new content.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29e40d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = {\n",
    "    \"BERT\": \"The goal of Generative AI is to [MASK] new content.\",\n",
    "    \"RoBERTa\": \"The goal of Generative AI is to <mask> new content.\",\n",
    "    \"BART\": \"The goal of Generative AI is to <mask> new content.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37cbec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.539692759513855, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to create new content.'}, {'score': 0.15575766563415527, 'token': 9699, 'token_str': 'generate', 'sequence': 'the goal of generative ai is to generate new content.'}]\n",
      "\n",
      "RoBERTa Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.37113118171691895, 'token': 5368, 'token_str': ' generate', 'sequence': 'The goal of Generative AI is to generate new content.'}, {'score': 0.3677138090133667, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}]\n",
      "\n",
      "BART Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.07461544126272202, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.06571853160858154, 'token': 244, 'token_str': ' help', 'sequence': 'The goal of Generative AI is to help new content.'}]\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} Output:\")\n",
    "    try:\n",
    "        masker = pipeline(\"fill-mask\", model=model)\n",
    "        result = masker(sentences[name])\n",
    "        print(result[:2])   # show top 2 predictions\n",
    "    except Exception as e:\n",
    "        print(\"Failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e7aacf",
   "metadata": {},
   "source": [
    "## Observation Experiment 2\n",
    "BERT successfully predicted meaningful words like “create” and “generate” because it is trained using masked language modeling.\n",
    "RoBERTa also performed well after using the correct <mask> token, showing similar behavior to BERT.\n",
    "BART showed partial success, as masked language modeling is not its primary training objective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d57247",
   "metadata": {},
   "source": [
    "## Experiment 3: Question Answering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d7f3f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.007576475851237774, 'start': 72, 'end': 82, 'answer': 'deepfakes.'}\n",
      "\n",
      "RoBERTa Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.008387230336666107, 'start': 68, 'end': 81, 'answer': 'and deepfakes'}\n",
      "\n",
      "BART Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.04100703168660402, 'start': 72, 'end': 81, 'answer': 'deepfakes'}\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the risks?\"\n",
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} Output:\")\n",
    "    try:\n",
    "        qa = pipeline(\"question-answering\", model=model)\n",
    "        print(qa(question=question, context=context))\n",
    "    except Exception as e:\n",
    "        print(\"Failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4c0558",
   "metadata": {},
   "source": [
    "### Observation for Expermient 3\n",
    "\n",
    "The models produced inconsistent or partially correct answers when answering the given question.\n",
    "This is because the models used were base versions and not fine-tuned specifically for question answering tasks.\n",
    "The results highlight the importance of task-specific fine-tuning for accurate QA performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18517175",
   "metadata": {},
   "source": [
    "## Observation Table\n",
    "\n",
    "| Task | Model | Success/Failure | Observation | Architectural Reason |\n",
    "|------|------|----------------|------------|----------------------|\n",
    "| Generation | BERT | Failure | Could not generate text | Encoder-only |\n",
    "| Generation | RoBERTa | Failure | Similar to BERT | Encoder-only |\n",
    "| Generation | BART | Success | Generated coherent text | Encoder-Decoder |\n",
    "| Fill-Mask | BERT | Success | Correct predictions | MLM training |\n",
    "| Fill-Mask | RoBERTa | Success | Accurate predictions | Optimized MLM |\n",
    "| Fill-Mask | BART | Partial | Less accurate | Not MLM-based |\n",
    "| QA | BERT | Partial | Inconsistent answers | Not QA fine-tuned |\n",
    "| QA | RoBERTa | Partial | Similar behavior | Base model |\n",
    "| QA | BART | Partial | Random answers | Not QA fine-tuned |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82d298",
   "metadata": {},
   "source": [
    "## Final Understanding\n",
    "\n",
    "Through this hands-on, I understood that the architecture of a transformer model plays a crucial role in deciding which NLP tasks it can perform effectively.\n",
    "Encoder-only models like BERT and RoBERTa are better suited for understanding tasks, while encoder–decoder models like BART support text generation.\n",
    "I also learned that tokenizer design and task-specific fine-tuning significantly impact model performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
