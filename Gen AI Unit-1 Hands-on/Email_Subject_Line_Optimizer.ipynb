{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4311c34",
   "metadata": {},
   "source": [
    "# Email Subject Line Optimizer(Unit 1 Project)\n",
    "\n",
    "Name: ARJUN SAI N\n",
    "USN: PES2UG23CS091\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b110c2",
   "metadata": {},
   "source": [
    "    ## Project Description\n",
    "\n",
    "    This project generates five optimized variations of an email subject line using a Generative AI text-generation model.\n",
    "    The goal is to help users quickly find the most appealing and effective subject line for marketing, business, or personal emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ddbaecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISABLE VS CODE WARNINGS\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "927f6a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.56.2)\n",
      "Requirement already satisfied: torch in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.35.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arjun's\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "#INSTALL LIBRARIES\n",
    "!pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1782cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc38123f",
   "metadata": {},
   "source": [
    "## Load Text-Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d619706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arjun's\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Arjun's\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Arjun's\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2\"   # small model suitable for text variations\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104b8515",
   "metadata": {},
   "source": [
    "## Input Subject Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38f88be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_line = \"Meeting reminder for tomorrow\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f4dbf",
   "metadata": {},
   "source": [
    "## Generate 5 Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77f445c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "variations = []\n",
    "\n",
    "for i in range(5):\n",
    "    output = generator(\n",
    "        subject_line,\n",
    "        max_length=25,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    variations.append(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a01420f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Subject Line Variations:\n",
      "\n",
      "1. Meeting reminder for tomorrow. I can be your greatest friend, no matter what,\" Yang said. \"I'll always be there for you. I'll always be there for you.\"\n",
      "\n",
      "\"I can tell you that!\" Yang said as she ran her hands over Yang's forehead, then her dress. Yang had never seen something so beautiful in someone's whole life…\n",
      "\n",
      "But instead of her skirt, Yang had a look of happiness and happiness in her face.\n",
      "\n",
      "She had given her best in every possible way. And she loved being with her girlfriend…\n",
      "\n",
      "\"No need to get up!\" Yang said as she ran her hands over the top of her skirt. She grabbed her dress, and pulled it down over her head. She took it off.\n",
      "\n",
      "\"I'm sorry about what happened,\" she said. \"I'm so sorry. I was really busy.\"\n",
      "\n",
      "Yang's smile was full of smiles, and she looked so much better now.\n",
      "\n",
      "Yang pulled her dress up and gave a large hug to Yang. \"I love you, Yang,\" she said.\n",
      "\n",
      "\"I'm sorry, I'm so sorry. I couldn't have been any happier.\" Yang looked up at Yang, who was staring at her. She smiled down, then waved\n",
      "\n",
      "2. Meeting reminder for tomorrow.\n",
      "\n",
      "Thank you.\n",
      "\n",
      "3. Meeting reminder for tomorrow morning (not tomorrow night).\n",
      "\n",
      "5:00AM to 8:00PM\n",
      "\n",
      "Free breakfast and lunch.\n",
      "\n",
      "Meeting reminder for tomorrow morning (not tomorrow night).\n",
      "\n",
      "9:00AM to 5:00PM\n",
      "\n",
      "Free breakfast and lunch.\n",
      "\n",
      "Meeting reminder for tomorrow morning (not tomorrow night).\n",
      "\n",
      "7:00AM to 5:00PM\n",
      "\n",
      "Free breakfast and lunch.\n",
      "\n",
      "Meeting reminder for tomorrow morning (not tomorrow night).\n",
      "\n",
      "12:00PM to 6:00PM\n",
      "\n",
      "Free breakfast and lunch.\n",
      "\n",
      "Meeting reminder for tomorrow morning (not tomorrow night).\n",
      "\n",
      "10:00AM to 5:00PM\n",
      "\n",
      "Free breakfast and lunch.\n",
      "\n",
      "Meeting reminder for tomorrow morning (not tomorrow night).\n",
      "\n",
      "9:00AM to 5:00PM\n",
      "\n",
      "Free breakfast and lunch.\n",
      "\n",
      "Meeting reminder for tomorrow morning (not tomorrow night).\n",
      "\n",
      "6:00AM to 6:00PM\n",
      "\n",
      "Free breakfast and lunch.\n",
      "\n",
      "Meeting reminder for tomorrow morning (not tomorrow night).\n",
      "\n",
      "9:00AM to 5:00PM\n",
      "\n",
      "Free breakfast and lunch.\n",
      "\n",
      "Meeting reminder for tomorrow morning (not tomorrow night).\n",
      "\n",
      "\n",
      "\n",
      "4. Meeting reminder for tomorrow with a friend who has worked in the industry for over 20 years.\"\n",
      "\n",
      "The company, which is one of the largest US pharmaceutical companies, has been dogged by antitrust suits and other allegations of anticompetitive conduct. The company was fined $1.5 billion in 2006 for selling generic drugs to the government for $5.6 billion in exchange for $90 million in stock.\n",
      "\n",
      "At the time, Bayer was the world's largest maker of pharmaceuticals, according to the New York Times.\n",
      "\n",
      "In an interview with CBS News last year, Crain's CEO William Frieden said the company doesn't need to \"crawl all over for a patent to come to market with new drugs.\"\n",
      "\n",
      "\"You can go buy a drug, you can buy new drugs from the company and continue to innovate,\" he told the New York Times. \"They can then create new drugs and sell them to other companies, which is a competitive advantage.\"\n",
      "\n",
      "5. Meeting reminder for tomorrow's meeting: We have a meeting to get the people to come for them tomorrow, to meet on time. We need to get the people to come. And we need to be a strong team.\n",
      "\n",
      "I'm asking all of you to come out and come out and speak to the people.\n",
      "\n",
      "If you don't feel good, we will make a call to you and we will make a call to them. You are going to be a strong team.\n",
      "\n",
      "We are going to make a call to those in the room. We are going to make a call to the people and we are going to come back and we are going to go to the office and we are going to come back and we are going to come back and we are going to go.\n",
      "\n",
      "We are doing a lot of things. We are doing a lot of things. We are saying good things. We are saying good things.\n",
      "\n",
      "We are talking to people who have been at the club for a few years and they are saying good things.\n",
      "\n",
      "They are saying good things.\n",
      "\n",
      "They are saying good things.\n",
      "\n",
      "They are saying good things.\n",
      "\n",
      "It's really good.\n",
      "\n",
      "For all the people who are coming out and the people who are\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated Subject Line Variations:\\n\")\n",
    "for i, v in enumerate(variations, start=1):\n",
    "    print(f\"{i}. {v}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20bc1b0",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "The model successfully generated multiple variations of the email subject line.\n",
    "Each version offers a slightly different tone - friendly, formal, urgent, or informative.\n",
    "This demonstrates how text-generation models can help in marketing, communication, and copy-writing tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee228b5",
   "metadata": {},
   "source": [
    "## Final Understanding\n",
    "\n",
    "Through this project, I learned how generative language models like GPT-2 can be used for creative text generation.\n",
    "I also understood how temperature, sampling, and max_length influence the output.\n",
    "This project shows how AI can assist users in improving clarity, creativity, and communication efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Fixed)",
   "language": "python",
   "name": "python311_fixed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
