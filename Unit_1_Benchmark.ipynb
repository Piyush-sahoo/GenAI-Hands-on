{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e7667c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch sentencepiece pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6aa7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d9c8097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pipelines for BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 939.16it/s, Materializing param=cls.predictions.transform.dense.weight]                 \n",
      "BertLMHeadModel LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 975.60it/s, Materializing param=cls.predictions.transform.dense.weight]                 \n",
      "BertForMaskedLM LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1012.08it/s, Materializing param=bert.encoder.layer.11.output.dense.weight]             \n",
      "BertForQuestionAnswering LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "bert.pooler.dense.weight                   | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "bert.pooler.dense.bias                     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "qa_outputs.weight                          | MISSING    | \n",
      "qa_outputs.bias                            | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pipelines for RoBERTa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 903.18it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "RobertaForCausalLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 986.92it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "RobertaForMaskedLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 964.71it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "RobertaForQuestionAnswering LOAD REPORT from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "qa_outputs.weight               | MISSING    | \n",
      "qa_outputs.bias                 | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pipelines for BART...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 159/159 [00:00<00:00, 993.51it/s, Materializing param=model.decoder.layers.5.self_attn_layer_norm.weight]   \n",
      "This checkpoint seem corrupted. The tied weights mapping for this model specifies to tie model.decoder.embed_tokens.weight to lm_head.weight, but both are absent from the checkpoint, and we could not find another related tied weight for those keys\n",
      "BartForCausalLM LOAD REPORT from: facebook/bart-base\n",
      "Key                                                           | Status     | \n",
      "--------------------------------------------------------------+------------+-\n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.bias     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.bias                    | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.bias                    | UNEXPECTED | \n",
      "encoder.embed_positions.weight                                | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.weight   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.weight | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.bias   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.weight     | UNEXPECTED | \n",
      "encoder.layernorm_embedding.bias                              | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.weight                  | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.weight                  | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.bias       | UNEXPECTED | \n",
      "encoder.layernorm_embedding.weight                            | UNEXPECTED | \n",
      "shared.weight                                                 | UNEXPECTED | \n",
      "lm_head.weight                                                | MISSING    | \n",
      "model.decoder.embed_tokens.weight                             | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n",
      "Loading weights: 100%|██████████| 259/259 [00:00<00:00, 965.29it/s, Materializing param=model.shared.weight]                                  \n",
      "Loading weights: 100%|██████████| 259/259 [00:00<00:00, 890.08it/s, Materializing param=model.shared.weight]                                  \n",
      "BartForQuestionAnswering LOAD REPORT from: facebook/bart-base\n",
      "Key               | Status  | \n",
      "------------------+---------+-\n",
      "qa_outputs.weight | MISSING | \n",
      "qa_outputs.bias   | MISSING | \n",
      "\n",
      "Notes:\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"BERT\": \"bert-base-uncased\",\n",
    "    \"RoBERTa\": \"roberta-base\",\n",
    "    \"BART\": \"facebook/bart-base\"\n",
    "}\n",
    "\n",
    "text_gen_pipelines = {}\n",
    "fill_mask_pipelines = {}\n",
    "qa_pipelines = {}\n",
    "\n",
    "for name, model_id in models.items():\n",
    "    print(f\"Loading pipelines for {name}...\")\n",
    "    \n",
    "    # Text Generation\n",
    "    try:\n",
    "        text_gen_pipelines[name] = pipeline(\"text-generation\", model=model_id)\n",
    "    except Exception as e:\n",
    "        text_gen_pipelines[name] = str(e)\n",
    "    \n",
    "    # Fill Mask\n",
    "    try:\n",
    "        fill_mask_pipelines[name] = pipeline(\"fill-mask\", model=model_id)\n",
    "    except Exception as e:\n",
    "        fill_mask_pipelines[name] = str(e)\n",
    "    \n",
    "    # Question Answering\n",
    "    try:\n",
    "        qa_pipelines[name] = pipeline(\"question-answering\", model=model_id)\n",
    "    except Exception as e:\n",
    "        qa_pipelines[name] = str(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1e0a04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Text Generation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... the or and -................................................................................................................................................... \"... the some some some some some some some some some some some some some some some some some\n",
      "\n",
      "RoBERTa Text Generation:\n",
      "The future of Artificial Intelligence is\n",
      "\n",
      "BART Text Generation:\n",
      "The future of Artificial Intelligence is prioritieshower moderationOXToy priorities laun Minute Mikhailphyphy Cumm circa laun laun laun Mikhail Mikhail Mikhail forum reservoirs reservoirs forumalien468 reservoirsalien Mikhail folded reservoirs 1909 1909 foldedal risky reservoirs deeper Mikhail forum Mikhail reservoirs reservoirssup scrolls commemor reservoirs deeper reservoirs reservoirs Attend reservoirs deeper 1909 Mikhail Mikhail reservoirs forumphy Accordingly Dal Dal Dal reservoirs reservoirsToy reservoirs scrolls scrolls Mikhail reservoirsphy reservoirs laun listening reservoirsuto reservoirs reservoirs reservoirs laun Elliot reservoirs Elliot scrolls reservoirs deeper Kiss reservoirs|||| reservoirs reservoirs Bros Body reservoirs risky reservoirs reservoirs Elliot reservoirs reservoirs Trinidad reservoirs reservoirsoldedYellowYellow Collections reservoirs Collections Collections reservoirs reservoirs scrolls reservoirsYellow Resurrection reservoirsadra reservoirs initiatives Mikhail reservoirs initiatives Bros reservoirs Resurrection Shanghai certific reservoirs Dal Apost reservoirs reservoirs Dal reservoirs HOME reservoirsYellow reservoirs reservoirsYellow Shanghai reservoirs reservoirs reservoirs initiatives Best Resurrection reservoirs reservoirs Collections initiatives reservoirs reservoirsalienalien reservoirsYellowYellow reservoirs Collections reservoirsenseenseense risky reservoirs jun reservoirs reservoirs activating reservoirs reservoirspartisan reservoirs reservoirs HOME Collections Collections Body Construct reservoirs Elliot reservoirs Collections certific reservoirs reservoirs PrayerToy reservoirs Resurrectionpartisaninders reservoirs reservoirs Resurrection initiatives sins reservoirs Collections meantime asleep reservoirs reservoirsitzer reservoirs reservoirs 1889 reservoirs reservoirs Shanghai reservoirsYellow ElliotYellowYellowYellow Resurrection briefings reservoirs Collectionsinders reservoirs Reviewinders reservoirs Cheese meantime reservoirs reservoirs meantime reservoirs Collections Construct Construct Collections Elliot CollectionsindersYellowYellow initiatives reservoirsitzer Collections reservoirsupt reservoirs reservoirsreporting\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "\n",
    "results_exp1 = {}\n",
    "\n",
    "for name, pipe in text_gen_pipelines.items():\n",
    "    print(f\"\\n{name} Text Generation:\")\n",
    "    \n",
    "    if isinstance(pipe, str):\n",
    "        print(\"Failed to load:\", pipe)\n",
    "        results_exp1[name] = \"FAILED\"\n",
    "    else:\n",
    "        try:\n",
    "            output = pipe(prompt, max_length=50, num_return_sequences=1)\n",
    "            text = output[0][\"generated_text\"]\n",
    "            print(text)\n",
    "            results_exp1[name] = text\n",
    "        except Exception as e:\n",
    "            print(\"Error during generation:\", e)\n",
    "            results_exp1[name] = \"ERROR\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42fbc052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Fill-Mask:\n",
      "Top predictions: ['create', 'generate', 'produce']\n",
      "\n",
      "RoBERTa Fill-Mask:\n",
      "Error during fill-mask: No mask_token (<mask>) found on the input\n",
      "\n",
      "BART Fill-Mask:\n",
      "Error during fill-mask: No mask_token (<mask>) found on the input\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The goal of Generative AI is to [MASK] new content.\"\n",
    "\n",
    "results_exp2 = {}\n",
    "\n",
    "for name, pipe in fill_mask_pipelines.items():\n",
    "    print(f\"\\n{name} Fill-Mask:\")\n",
    "    \n",
    "    if isinstance(pipe, str):\n",
    "        print(\"Failed to load:\", pipe)\n",
    "        results_exp2[name] = \"FAILED\"\n",
    "    else:\n",
    "        try:\n",
    "            output = pipe(sentence)\n",
    "            top_preds = [pred[\"token_str\"].strip() for pred in output[:3]]\n",
    "            print(\"Top predictions:\", top_preds)\n",
    "            results_exp2[name] = top_preds\n",
    "        except Exception as e:\n",
    "            print(\"Error during fill-mask:\", e)\n",
    "            results_exp2[name] = \"ERROR\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9878da4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Question Answering:\n",
      "Answer: deepfakes\n",
      "Confidence Score: 0.0218\n",
      "\n",
      "RoBERTa Question Answering:\n",
      "Answer: deepfakes.\n",
      "Confidence Score: 0.0091\n",
      "\n",
      "BART Question Answering:\n",
      "Answer: , and\n",
      "Confidence Score: 0.016\n"
     ]
    }
   ],
   "source": [
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question = \"What are the risks?\"\n",
    "\n",
    "results_exp3 = {}\n",
    "\n",
    "for name, pipe in qa_pipelines.items():\n",
    "    print(f\"\\n{name} Question Answering:\")\n",
    "    \n",
    "    if isinstance(pipe, str):\n",
    "        print(\"Failed to load:\", pipe)\n",
    "        results_exp3[name] = \"FAILED\"\n",
    "    else:\n",
    "        try:\n",
    "            output = pipe(question=question, context=context)\n",
    "            answer = output[\"answer\"]\n",
    "            score = output[\"score\"]\n",
    "            print(\"Answer:\", answer)\n",
    "            print(\"Confidence Score:\", round(score, 4))\n",
    "            results_exp3[name] = (answer, score)\n",
    "        except Exception as e:\n",
    "            print(\"Error during QA:\", e)\n",
    "            results_exp3[name] = \"ERROR\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bb16271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Text Generation Output</th>\n",
       "      <th>Fill-Mask Top Predictions</th>\n",
       "      <th>QA Answer + Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERT</td>\n",
       "      <td>The future of Artificial Intelligence is.........</td>\n",
       "      <td>[create, generate, produce]</td>\n",
       "      <td>(deepfakes, 0.021799192763864994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RoBERTa</td>\n",
       "      <td>The future of Artificial Intelligence is</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>(deepfakes., 0.009068168699741364)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BART</td>\n",
       "      <td>The future of Artificial Intelligence is prior...</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>(, and, 0.01596701890230179)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model                             Text Generation Output  \\\n",
       "0     BERT  The future of Artificial Intelligence is.........   \n",
       "1  RoBERTa           The future of Artificial Intelligence is   \n",
       "2     BART  The future of Artificial Intelligence is prior...   \n",
       "\n",
       "     Fill-Mask Top Predictions                   QA Answer + Score  \n",
       "0  [create, generate, produce]   (deepfakes, 0.021799192763864994)  \n",
       "1                        ERROR  (deepfakes., 0.009068168699741364)  \n",
       "2                        ERROR        (, and, 0.01596701890230179)  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for model in models.keys():\n",
    "    data.append({\n",
    "        \"Model\": model,\n",
    "        \"Text Generation Output\": results_exp1.get(model, \"N/A\"),\n",
    "        \"Fill-Mask Top Predictions\": results_exp2.get(model, \"N/A\"),\n",
    "        \"QA Answer + Score\": results_exp3.get(model, \"N/A\")\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
