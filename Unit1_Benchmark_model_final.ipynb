{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc2362e9",
   "metadata": {},
   "source": [
    "NAME: ANANYAA SREE SP\n",
    "\n",
    "SRN: PES2UG23CS066\n",
    "\n",
    "SECTION: A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fac1f",
   "metadata": {},
   "source": [
    "# Unit 1 Assignment: Model Benchmark Challenge\n",
    "\n",
    "This notebook evaluates architectural differences between BERT, RoBERTa, and BART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63066979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\program files\\python312\\lib\\site-packages (4.57.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: torch in c:\\program files\\python312\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python312\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\program files\\python312\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\program files\\python312\\lib\\site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\program files\\python312\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\program files\\python312\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\program files\\python312\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\program files\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\program files\\python312\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\program files\\python312\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\program files\\python312\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\program files\\python312\\lib\\site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from torch) (74.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\program files\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9380b899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79297ff2",
   "metadata": {},
   "source": [
    "## Experiment 1: Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ab660ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n",
      "--------------------------------------------------\n",
      "Model: roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is'}]\n",
      "--------------------------------------------------\n",
      "Model: facebook/bart-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is Style demonstrates Style Style Style convictionsthemeatively UTC Jobsativelyalks GN Styleativelythememouth Jobsiddlesiddlesiddles900900 deals Style Styleiddlesiddles struiddles presidents camel rebuilding rebuildingiddlesiddlesmouth Style fo Styleκ demonstrates ordained conveyed replicaitting Forumitting Style StyleHost StyleProfileProfile demonstratesiddles HM HM GUN GUN GUN HMjaniddlesiddlesittingiddles Forum GUNitting footholdiddles replicaiddlesiddlesProfileiddles GUNiddles GeneticHostiddles55Host GUNiddlesiddles GUN assailiddlesiddles foolishiddlesiddles Styleiddles GUN replica GUN fo GUN HM rebuilding GUN GUNiddles footholdiddles GUN Style GUN GUNHost Periodiddlesiddles55iddles presidents HMProfileiddlesiddles Period presidentsiddlesiddles308 GUN Arriiddlesiddles Reliefiddles GUN GUN Style Recovery593Hostiddlesiddles reinvent GUN really reallyiddlesiddles ventilationiddlesiddlesPlPliddlesiddles593iddlesiddles Voldemortiddles BBiddles GUNNotes Tripoli Tripoliiddleswikiiddles GUN HM593 Tripoliiddlesiddles rebuildingiddles GUN Uk GUNiddles GUNurg GUN assail assail GUN GUN rebuilding really GUN GUN Voldemort grav demonstrates really Voldemortiddles GUN Voldemortiddles grav Inspectioniddles Voldemort55 GUN GUNleneck GUNiddles Clifford demonstrates GUN GUNjliddles GUN really593 demonstrates demonstrates grav Xavierjl GUN593 Voldemort Clifford GUN HM Clifford GUN Voldemort assail GUN593 GUN HM fluct GUN593Profileiddles demonstrates grav Inspection593 Inspection demonstrates593i GUNProfileleneck'}]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_gen_models = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"roberta-base\",\n",
    "    \"facebook/bart-base\"\n",
    "]\n",
    "\n",
    "for model in text_gen_models:\n",
    "    print(f\"Model: {model}\")\n",
    "    try:\n",
    "        gen = pipeline(\"text-generation\", model=model)\n",
    "        print(gen(\"The future of Artificial Intelligence is\", max_length=30))\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7308084",
   "metadata": {},
   "source": [
    "## Experiment 2: Masked Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9925e74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.539692759513855, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to create new content.'}, {'score': 0.15575766563415527, 'token': 9699, 'token_str': 'generate', 'sequence': 'the goal of generative ai is to generate new content.'}, {'score': 0.05405496060848236, 'token': 3965, 'token_str': 'produce', 'sequence': 'the goal of generative ai is to produce new content.'}, {'score': 0.044515229761600494, 'token': 4503, 'token_str': 'develop', 'sequence': 'the goal of generative ai is to develop new content.'}, {'score': 0.017577484250068665, 'token': 5587, 'token_str': 'add', 'sequence': 'the goal of generative ai is to add new content.'}]\n",
      "--------------------------------------------------\n",
      "Model: roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.37113118171691895, 'token': 5368, 'token_str': ' generate', 'sequence': 'The goal of Generative AI is to generate new content.'}, {'score': 0.3677138090133667, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.08351466804742813, 'token': 8286, 'token_str': ' discover', 'sequence': 'The goal of Generative AI is to discover new content.'}, {'score': 0.02133519947528839, 'token': 465, 'token_str': ' find', 'sequence': 'The goal of Generative AI is to find new content.'}, {'score': 0.01652175933122635, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}]\n",
      "--------------------------------------------------\n",
      "Model: facebook/bart-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.07461544126272202, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.06571853160858154, 'token': 244, 'token_str': ' help', 'sequence': 'The goal of Generative AI is to help new content.'}, {'score': 0.060880184173583984, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}, {'score': 0.035935722291469574, 'token': 3155, 'token_str': ' enable', 'sequence': 'The goal of Generative AI is to enable new content.'}, {'score': 0.03319481760263443, 'token': 1477, 'token_str': ' improve', 'sequence': 'The goal of Generative AI is to improve new content.'}]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fill_mask_models = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"roberta-base\",\n",
    "    \"facebook/bart-base\"\n",
    "]\n",
    "\n",
    "sentences = {\n",
    "    \"bert-base-uncased\": \"The goal of Generative AI is to [MASK] new content.\",\n",
    "    \"roberta-base\": \"The goal of Generative AI is to <mask> new content.\",\n",
    "    \"facebook/bart-base\": \"The goal of Generative AI is to <mask> new content.\"\n",
    "}\n",
    "\n",
    "for model in fill_mask_models:\n",
    "    print(f\"Model: {model}\")\n",
    "    fm = pipeline(\"fill-mask\", model=model)\n",
    "    print(fm(sentences[model]))\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367b6892",
   "metadata": {},
   "source": [
    "## Experiment 3: Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acfbf6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.013356235111132264, 'start': 46, 'end': 60, 'answer': 'hallucinations'}\n",
      "--------------------------------------------------\n",
      "Model: roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.00838578911498189, 'start': 68, 'end': 81, 'answer': 'and deepfakes'}\n",
      "--------------------------------------------------\n",
      "Model: facebook/bart-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.0733608528971672, 'start': 0, 'end': 81, 'answer': 'Generative AI poses significant risks such as hallucinations, bias, and deepfakes'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "qa_models = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"roberta-base\",\n",
    "    \"facebook/bart-base\"\n",
    "]\n",
    "\n",
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question = \"What are the risks?\"\n",
    "\n",
    "for model in qa_models:\n",
    "    print(f\"Model: {model}\")\n",
    "    qa = pipeline(\"question-answering\", model=model)\n",
    "    print(qa(question=question, context=context))\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b578c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "| Task       | Model   | Classification (Success/Failure) | Observation (What actually happened?)                   | Why did this happen? (Architectural Reason)       |\n",
    "| ---------- | ------- | -------------------------------- | ------------------------------------------------------- | --------------------------------|\n",
    "| Generation | BERT    | Failure                          | Generated repetitive dots instead of meaningful text.   | Encoder-only model; cannot                                                                                                                             generate next tokens.  \n",
    "| Generation | RoBERTa | Failure                          | Did not continue the prompt beyond the input sentence.  | Encoder-only architecture with                                                                                                                         no decoder.        \n",
    "| Generation | BART    | Success                          | Generated long text, but output was incoherent.         | Encoder–decoder architecture                                                                                                                           supports generation. \n",
    "| Fill-Mask  | BERT    | Success                          | Correctly predicted words like “create” and “generate”. | Trained using masked language                                                                                                                           modeling.           \n",
    "| Fill-Mask  | RoBERTa | Success                          | Predicted correct masked words with high confidence.    | Optimized encoder trained for                                                                                                                          MLM.                \n",
    "| Fill-Mask  | BART    | Partial Success                  | Predicted reasonable words but with lower confidence.   | Masking is not its primary                                                                                                                             training objective.    \n",
    "| QA         | BERT    | Partial Success                  | Extracted correct answer with very low confidence.      | QA head not fine-tuned for                                                                                                                             question answering.    \n",
    "| QA         | RoBERTa | Failure                          | Returned partial and incomplete answer.                 | Lacks task-specific QA fine-                                                                                                                           tuning.               \n",
    "| QA         | BART    | Failure                          | Produced incomplete answer ending mid-phrase.           | Requires fine-tuning for                                                                                                                               extractive QA.           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a33395d-1be5-487a-88d1-aa90edf3d89b",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "- Encoder-only models fail at text generation.\n",
    "- BERT and RoBERTa excel at masked language modeling.\n",
    "- All base models perform poorly at QA without fine-tuning.\n",
    "\n",
    "#### Experiment 1\n",
    "BERT and RoBERTa fail to generate meaningful text. Although they run without crashing, their outputs lack semantic continuation because encoder-only models are not designed for autoregressive token generation. BART is able to generate long sequences due to its encoder–decoder architecture, demonstrating that generation is architecturally possible, even though the output quality is poor without task-specific training.\n",
    "\n",
    "#### Experiment 2\n",
    "\n",
    "BERT and RoBERTa perform exceptionally well at masked language modeling, correctly predicting words such as “create” and “generate” with high confidence. This aligns with their training objective, which explicitly involves predicting masked tokens using bidirectional context. BART performs worse in comparison, as masked word prediction is not its primary training task.\n",
    "\n",
    "#### Experiment 3\n",
    "\n",
    "All three base models show inconsistent performance in question answering. Although BERT correctly extracts the list of risks, its confidence score is extremely low. RoBERTa and BART return partial or incomplete answers. This behavior is expected because the models are not fine-tuned on a question answering dataset such as SQuAD, demonstrating that task-specific fine-tuning is critical for reliable QA performance.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
