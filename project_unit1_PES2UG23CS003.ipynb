{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Unit 1 Project: Chat Log Summarizer (33)\n",
        "**Student Name**: A Shri Karthik  \n",
        "**Category**: Business & Education Summarization\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Sd0-Dr8Euhk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. setup and imports\n",
        "\n"
      ],
      "metadata": {
        "id": "F8bIvQp6reyO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7vc0CuGOrMb-"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, logging\n",
        "import textwrap\n",
        "\n",
        "def print_project_header(title):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{title.center(80)}\")\n",
        "    print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Initialize the Specialized Chat summarizer and simulated input chat log"
      ],
      "metadata": {
        "id": "mt9jKL3Qrz6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"philschmid/bart-large-cnn-samsum\"\n",
        "chat_summarizer = pipeline(\"summarization\", model=model_name, device=-1)\n",
        "\n",
        "chat_log = \"\"\"\n",
        "Abhishek: bro did you do the assignment\n",
        "\n",
        "Karthik: which one\n",
        "\n",
        "Abhishek: the Generative AI one\n",
        "\n",
        "Karthik: there are FOUR assignments with AI in the name\n",
        "\n",
        "Abhishek: ok fair. the one due today\n",
        "\n",
        "Karthik: today as in… today today?\n",
        "\n",
        "Abhishek: yes. midnight. scary midnight.\n",
        "\n",
        "Karthik: im just starting it.\n",
        "\n",
        "Abhishek: lol. same.\n",
        "\n",
        "Karthik: what even is it about\n",
        "\n",
        "Abhishek: something about encoders, decoders, and why transformers ruined my sleep schedule\n",
        "\n",
        "Karthik: oh that. i watched half the lecture at 2x speed\n",
        "\n",
        "Abhishek: and understood?\n",
        "\n",
        "Karthik: emotionally, no. academically, also no.\n",
        "\n",
        "Abhishek: do we have notes?\n",
        "\n",
        "Karthik: Aditya said he’ll upload them “soon”\n",
        "\n",
        "Abhishek: define soon\n",
        "\n",
        "Karthik: soon like “after dinner” but dinner never ends\n",
        "\n",
        "Abhishek: classic.\n",
        "\n",
        "Karthik: should we just submit vibes\n",
        "\n",
        "Abhishek: i think that’s what the model would want\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5Cacf1or7Xh",
        "outputId": "c28d995c-e44b-42aa-9f2d-c9d0aa66b596"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Generate the Summary"
      ],
      "metadata": {
        "id": "yzMu1-T3sPQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_output = chat_summarizer(\n",
        "    chat_log,\n",
        "    max_length=50,\n",
        "    min_length=15,\n",
        "    do_sample=False\n",
        ")\n",
        "\n",
        "print_project_header(\"PROJECT #33: CHAT LOG SUMMARIZER\")\n",
        "print(f\"\\n[ORIGINAL CHAT LOG]:\\n{chat_log}\")\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(\"[GENERATED SUMMARY]:\")\n",
        "final_summary = summary_output[0]['summary_text']\n",
        "print(textwrap.fill(final_summary, width=80))\n",
        "print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1OurCIOsYr0",
        "outputId": "9c25df78-37c1-4e29-8f51-a74b15d7f55b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "                        PROJECT #33: CHAT LOG SUMMARIZER                        \n",
            "================================================================================\n",
            "\n",
            "[ORIGINAL CHAT LOG]:\n",
            "\n",
            "Abhishek: bro did you do the assignment\n",
            "\n",
            "Karthik: which one\n",
            "\n",
            "Abhishek: the Generative AI one\n",
            "\n",
            "Karthik: there are FOUR assignments with AI in the name\n",
            "\n",
            "Abhishek: ok fair. the one due today\n",
            "\n",
            "Karthik: today as in… today today?\n",
            "\n",
            "Abhishek: yes. midnight. scary midnight.\n",
            "\n",
            "Karthik: im just starting it.\n",
            "\n",
            "Abhishek: lol. same.\n",
            "\n",
            "Karthik: what even is it about\n",
            "\n",
            "Abhishek: something about encoders, decoders, and why transformers ruined my sleep schedule\n",
            "\n",
            "Karthik: oh that. i watched half the lecture at 2x speed\n",
            "\n",
            "Abhishek: and understood?\n",
            "\n",
            "Karthik: emotionally, no. academically, also no.\n",
            "\n",
            "Abhishek: do we have notes?\n",
            "\n",
            "Karthik: Aditya said he’ll upload them “soon”\n",
            "\n",
            "Abhishek: define soon\n",
            "\n",
            "Karthik: soon like “after dinner” but dinner never ends\n",
            "\n",
            "Abhishek: classic.\n",
            "\n",
            "Karthik: should we just submit vibes\n",
            "\n",
            "Abhishek: i think that’s what the model would want\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[GENERATED SUMMARY]:\n",
            "Karthik and Abhishek have a Generative AI assignment due today at midnight. They\n",
            "will submit vibes instead of taking notes.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 1. Project Overview\n",
        "This project addresses the challenge of summarizing informal, multi-turn dialogue. Chat logs are inherently \"noisy\"—they contain fragments, slang, and overlapping speakers. Our goal is to take these input sequences and generate a shorter, coherent output string that retains the core meaning.\n",
        "\n",
        "## 2. Methodology\n",
        "We implemented a **Sequence-to-Sequence (seq2seq)** pipeline using the Hugging Face `transformers` library.\n",
        "\n",
        "### Model Selection: `philschmid/bart-large-cnn-samsum`\n",
        "* **Architecture**: This is an **Encoder-Decoder** model. The Encoder processes the input bidirectional context, and the Decoder generates the summary one token at a time.\n",
        "* **Specialization**: Unlike the \"base\" BART used in our benchmark, this model is fine-tuned on the **SAMSum dataset**. This dataset consists of messenger-style dialogues, allowing the model to handle conversational semantics rather than producing \"rubbish\".\n",
        "\n",
        "\n",
        "## 3. Observations\n",
        "* **Semantic Understanding**: The model successfully differentiated between participants and identified the \"action items\" within the chat.\n",
        "* **Coherence**: The output is a grammatically correct narrative, which is very different from the incoherent outputs from the base models used in previous handson experiments.\n",
        "* **Efficiency**: The `pipeline` abstraction allows for high-speed inference even on a standard CPU."
      ],
      "metadata": {
        "id": "GpSwferSHVBQ"
      }
    }
  ]
}